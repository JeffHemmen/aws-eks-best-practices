{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Amazon EKS Best Practices Guide for Security \u00b6 This guide provides advice about protecting information, systems, and assets that are reliant on EKS while delivering business value through risk assessments and mitigation strategies. The guidance herein is part of a series of best practices guides that AWS is publishing to help customers implement EKS in accordance with best practices. Guides for Performance, Operational Excellence, Cost Optimization, and Reliability will be available in the coming months. How to use this guide \u00b6 This guide is meant for security practitioners who are responsible for implementing and monitoring the effectiveness of security controls for EKS clusters and the workloads they support. The guide is organized into different topic areas for easier consumption. Each topic starts with a brief overview, followed by a list of recommendations and best practices for securing your EKS clusters. The topics do not need to read in a particular order. Understanding the Shared Responsibility Model \u00b6 Security and compliance are considered shared responsibilities when using a managed service like EKS. Generally speaking, AWS is responsible for security \"of\" the cloud whereas you, the customer, are responsible for security \"in\" the cloud. With EKS, AWS is responsible for managing of the EKS managed Kubernetes control plane. This includes the Kubernetes masters, the ETCD database, and other infrastructure necessary for AWS to deliver a secure and reliable service. As a consumer of EKS, you are largely responsible for the topics in this guide, e.g. IAM, pod security, runtime security, network security, and so forth. When it comes to infrastructure security, AWS will assume additional responsibilities as you move from self-managed workers, to managed node groups, to Fargate. For example, with Fargate, AWS becomes responsible for securing the underlying instance/runtime used to run your Pods. AWS will also assume responsibility for securing and maintaining the underlying instance when you use Managed Node Groups (MNG). However, unlike Fargate, MNGs will not automatically scale your infrastructure/cluster. That can be handled by the cluster-autoscaler or other technologies such as native AWS autoscaling, SpotInst's Ocean , or Atlassian's Escalator . Before designing your system, it is important to know where the line of demarcation is between your responsibilities and the provider of the service (AWS). For additional information about the shared responsibility model, see https://aws.amazon.com/compliance/shared-responsibility-model/ Introduction \u00b6 There are several security best practice areas that are pertinent when using a managed Kubernetes service like EKS: Identity and Access Management Pod Security Runtime Security Network Security Multi-tenancy Detective Controls Infrastructure Security Data Encryption and Secrets Management Regulatory Compliance Incident Response and Forensics Image Security As part of designing any system, you need to think about its security implications and the practices that can affect your security posture. For example, you need to control who can perform actions against a set of resources. You also need the ability to quickly identify security incidents, protect your systems and services from unauthorized access, and maintain the confidentiality and integrity of data through data protection. Having a well-defined and rehearsed set of processes for responding to security incidents will improve your security posture too. These tools and techniques are important because they support objectives such as preventing financial loss or complying with regulatory obligations. AWS helps organizations achieve their security and compliance goals by offering a rich set of security services that have evolved based on feedback from a broad set of security conscious customers. By offering a highly secure foundation, customers can spend less time on \u201cundifferentiated heavy lifting\u201d and more time on achieving their business objectives. Feedback \u00b6 This guide is being released on GitHub so as to collect direct feedback and suggestions from the broader EKS/Kubernetes community. If you have a best practice that you feel we ought to include in the guide, please file an issue or submit a PR in the GitHub repository. Our intention is to update the guide periodically as new features are added to the service or when a new best practice evolves. Further Reading \u00b6 Kubernetes Security Whitepaper , sponsored by the Security Audit Working Group, this Whitepaper describes key aspects of the Kubernetes attack surface and security architecture with the aim of helping security practitioners make sound design and implementation decisions.","title":"Home"},{"location":"#amazon-eks-best-practices-guide-for-security","text":"This guide provides advice about protecting information, systems, and assets that are reliant on EKS while delivering business value through risk assessments and mitigation strategies. The guidance herein is part of a series of best practices guides that AWS is publishing to help customers implement EKS in accordance with best practices. Guides for Performance, Operational Excellence, Cost Optimization, and Reliability will be available in the coming months.","title":"Amazon EKS Best Practices Guide for Security"},{"location":"#how-to-use-this-guide","text":"This guide is meant for security practitioners who are responsible for implementing and monitoring the effectiveness of security controls for EKS clusters and the workloads they support. The guide is organized into different topic areas for easier consumption. Each topic starts with a brief overview, followed by a list of recommendations and best practices for securing your EKS clusters. The topics do not need to read in a particular order.","title":"How to use this guide"},{"location":"#understanding-the-shared-responsibility-model","text":"Security and compliance are considered shared responsibilities when using a managed service like EKS. Generally speaking, AWS is responsible for security \"of\" the cloud whereas you, the customer, are responsible for security \"in\" the cloud. With EKS, AWS is responsible for managing of the EKS managed Kubernetes control plane. This includes the Kubernetes masters, the ETCD database, and other infrastructure necessary for AWS to deliver a secure and reliable service. As a consumer of EKS, you are largely responsible for the topics in this guide, e.g. IAM, pod security, runtime security, network security, and so forth. When it comes to infrastructure security, AWS will assume additional responsibilities as you move from self-managed workers, to managed node groups, to Fargate. For example, with Fargate, AWS becomes responsible for securing the underlying instance/runtime used to run your Pods. AWS will also assume responsibility for securing and maintaining the underlying instance when you use Managed Node Groups (MNG). However, unlike Fargate, MNGs will not automatically scale your infrastructure/cluster. That can be handled by the cluster-autoscaler or other technologies such as native AWS autoscaling, SpotInst's Ocean , or Atlassian's Escalator . Before designing your system, it is important to know where the line of demarcation is between your responsibilities and the provider of the service (AWS). For additional information about the shared responsibility model, see https://aws.amazon.com/compliance/shared-responsibility-model/","title":"Understanding the Shared Responsibility Model"},{"location":"#introduction","text":"There are several security best practice areas that are pertinent when using a managed Kubernetes service like EKS: Identity and Access Management Pod Security Runtime Security Network Security Multi-tenancy Detective Controls Infrastructure Security Data Encryption and Secrets Management Regulatory Compliance Incident Response and Forensics Image Security As part of designing any system, you need to think about its security implications and the practices that can affect your security posture. For example, you need to control who can perform actions against a set of resources. You also need the ability to quickly identify security incidents, protect your systems and services from unauthorized access, and maintain the confidentiality and integrity of data through data protection. Having a well-defined and rehearsed set of processes for responding to security incidents will improve your security posture too. These tools and techniques are important because they support objectives such as preventing financial loss or complying with regulatory obligations. AWS helps organizations achieve their security and compliance goals by offering a rich set of security services that have evolved based on feedback from a broad set of security conscious customers. By offering a highly secure foundation, customers can spend less time on \u201cundifferentiated heavy lifting\u201d and more time on achieving their business objectives.","title":"Introduction"},{"location":"#feedback","text":"This guide is being released on GitHub so as to collect direct feedback and suggestions from the broader EKS/Kubernetes community. If you have a best practice that you feel we ought to include in the guide, please file an issue or submit a PR in the GitHub repository. Our intention is to update the guide periodically as new features are added to the service or when a new best practice evolves.","title":"Feedback"},{"location":"#further-reading","text":"Kubernetes Security Whitepaper , sponsored by the Security Audit Working Group, this Whitepaper describes key aspects of the Kubernetes attack surface and security architecture with the aim of helping security practitioners make sound design and implementation decisions.","title":"Further Reading"},{"location":"cluster-autoscaling/","text":"Kubernetes Cluster Autoscaler \u00b6 Overview \u00b6 The Kubernetes Cluster Autoscaler is a popular Cluster Autoscaling solution maintained by SIG Autoscaling . It is responsible for ensuring that your cluster has enough nodes to schedule your pods without wasting resources. It watches for pods that fail to schedule and for nodes that are underutilized. It then simulates the addition or removal of nodes before applying the change to your cluster. The AWS Cloud Provider implementation within Cluster Autoscaler controls the .DesiredReplicas field of your EC2 Auto Scaling Groups. This guide will provide a mental model for configuring the Cluster Autoscaler and choosing the best set of tradeoffs to meet your organization\u2019s requirements. While there is no single best configuration, there are a set of configuration options that enable you to trade off performance, scalability, cost, and availability. Additionally, this guide will provide tips and best practices for optimizing your configuration for AWS. Glossary \u00b6 The following terminology will be used frequently throughout this document. These terms can have broad meaning, but are limited to the definitions below for the purposes of this document. Scalability refers to how well the Cluster Autoscaler performs as your Kubernetes Cluster increases in number of pods and nodes. As scalability limits are reached, the Cluster Autoscaler\u2019s performance and functionality degrades. As the Cluster Autoscaler exceeds its scalability limits, it may no longer add or remove nodes in your cluster. Performance refers to how quickly the Cluster Autoscaler is able to make and execute scaling decisions. A perfectly performing Cluster Autoscaler would instantly make a decision and trigger a scaling action in response to stimuli, such as a pod becoming unschedulable. Availability means that pods can be scheduled quickly and without disruption. This includes when newly created pods need to be scheduled and when a scaled down node terminates any remaining pods scheduled to it. Cost is determined by the decision behind scale out and scale in events. Resources are wasted if an existing node is underutilized or a new node is added that is too large for incoming pods. Depending on the use case, there can be costs associated with prematurely terminating pods due to an aggressive scale down decision. Node Groups are an abstract Kubernetes concept for a group of nodes within a cluster. It is not a true Kubernetes resource, but exists as an abstraction in the Cluster Autoscaler, Cluster API, and other components. Nodes within a Node Group share properties like labels and taints, but may consist of multiple Availability Zones or Instance Types. EC2 Auto Scaling Groups can be used as an implementation of Node Groups on EC2. EC2 Auto Scaling Groups are configured to launch instances that automatically join their Kubernetes Clusters and apply labels and taints to their corresponding Node resource in the Kubernetes API. EC2 Managed Node Groups are another implementation of Node Groups on EC2. They abstract away the complexity manually configuring EC2 Autoscaling Scaling Groups and provide additional management features like node version upgrade and graceful node termination. Operating the Cluster Autoscaler \u00b6 The Cluster Autoscaler is typically installed as a Deployment in your cluster. It uses leader election to ensure high availability, but work is done by a single replica at a time. It is not horizontally scalable. For basic setups, the default it should work out of the box using the provided installation instructions , but there are a few things to keep in mind. Ensure that: The Cluster Autoscaler\u2019s version matches the Cluster\u2019s Version. Cross version compatibility is not tested or supported . Auto Discovery is enabled, unless you have specific advanced use cases that prevent use of this mode. Configuring your Node Groups \u00b6 Effective autoscaling starts with correctly configuring a set of Node Groups for your cluster. Selecting the right set of Node Groups is key to maximizing availability and reducing cost across your workloads. AWS implements Node Groups using EC2 Auto Scaling Groups, which are flexible to a large number of use cases. However, the Cluster Autoscaler makes some assumptions about your Node Groups. Keeping your EC2 Auto Scaling Group configurations consistent with these assumptions will minimize undesired behavior. Ensure that: Each Node in a Node Group has identical scheduling properties, such as Labels, Taints, and Resources. For MixedInstancePolicies, the Instance Types must be of the same shape for CPU, Memory, and GPU The first Instance Type specified in the policy will be used to simulate scheduling. If your policy has additional Instance Types with more resources, resources may be wasted after scale out. If your policy has additional Instance Types with less resources, pods may fail to schedule on the instances. Node Groups with many nodes are preferred over many Node Groups with fewer nodes. This will have the biggest impact on scalability. Wherever possible, prefer EC2 features when both systems provide support (e.g. Regions, MixedInstancePolicy) Note: If possible, we recommend using EKS Managed Node Groups . Managed Node Groups come with powerful management features, including features for Cluster Autoscaler like automatic EC2 Auto Scaling Group discovery and graceful node termination. Optimizing for Performance and Scalability \u00b6 Understanding the autoscaling algorithm\u2019s runtime complexity will help you tune the Cluster Autoscaler to continue operating smoothly in large clusters with greater than 1,000 nodes . The primary knobs for tuning scalability of the Cluster Autoscaler are the resources provided to the process, the scan interval of the algorithm, and the number of Node Groups in the cluster. There are other factors involved in the true runtime complexity of this algorithm, such as scheduling plugin complexity and number of pods. These are considered to be unconfigurable parameters as they are natural to the cluster\u2019s workload and cannot easily be tuned. The Cluster Autoscaler loads the entire cluster\u2019s state into memory, including Pods, Nodes, and Node Groups. On each scan interval, the algorithm identifies unschedulable pods and simulates scheduling for each Node Group. Tuning these factors come with different tradeoffs which should be carefully considered for your use case. Vertically Autoscaling the Cluster Autoscaler \u00b6 The simplest way to scale the Cluster Autoscaler to larger clusters is to increase the resource requests for its deployment. Both memory and CPU should be increased for large clusters, though this varies significantly with cluster size. The autoscaling algorithm stores all pods and nodes in memory, which can result in a memory footprint larger than a gigabyte in some cases. Increasing resources is typically done manually. If you find that constant resource tuning is creating an operational burden, consider using the Addon Resizer or Vertical Pod Autoscaler . Reducing the number of Node Groups \u00b6 Minimizing the number of node groups is one way to ensure that the Cluster Autoscaler will continue to perform well on large clusters. This may be challenging for some organizations who structure their node groups per team or per application. While this is fully supported by the Kubernetes API, this is considered to be a Cluster Autoscaler anti-pattern with repercussions for scalability. There are many reasons to use multiple node groups (e.g. Spot or GPUs), but in many cases there are alternative designs that achieve the same effect while using a small number of groups. Ensure that: Pod isolation is done using Namespaces rather than Node Groups. This may not be possible in low-trust multi-tenant clusters. Pod ResourceRequests and ResourceLimits are properly set to avoid resource contention. Larger instance types will result in more optimal bin packing and reduced system pod overhead. NodeTaints or NodeSelectors are used to schedule pods as the exception, not as the rule. Regional resources are defined as a single EC2 Auto Scaling Group with multiple Availability Zones. Reducing the Scan Interval \u00b6 A low scan interval (e.g. 10 seconds) will ensure that the Cluster Autoscaler responds as quickly as possible when pods become unschedulable. However, each scan results in many API calls to the Kubernetes API and EC2 Auto Scaling Group or EKS Managed Node Group APIs. These API calls can result in rate limiting or even service unavailability for your Kubernetes Control Plane. The default scan interval is 10 seconds, but on AWS, launching a node takes significantly longer to launch a new instance. This means that it\u2019s possible to increase the interval without significantly increasing overall scale up time. For example, if it takes 2 minutes to launch a node, changing the interval to 1 minute will result a tradeoff of 6x reduced API calls for 38% slower scale ups. Sharding Across Node Groups \u00b6 The Cluster Autoscaler can be configured to operate on a specific set of Node Groups. Using this functionality, it\u2019s possible to deploy multiple instances of the Cluster Autoscaler, each configured to operate on a different set of Node Groups. This strategy enables you use arbitrarily large numbers of Node Groups, trading cost for scalability. We only recommend using this as a last resort for improving performance. The Cluster Autoscaler was not originally designed for this configuration, so there are some side effects. Since the shards do not communicate, it\u2019s possible for multiple autoscalers to attempt to schedule an unschedulable pod. This can result in unnecessary scale out of multiple Node Groups. These extra nodes will scale back in after the scale-down-delay . metadata : name : cluster - autoscaler namespace : cluster - autoscaler - 1 ... -- nodes = 1 : 10 : k8s - worker - asg - 1 -- nodes = 1 : 10 : k8s - worker - asg - 2 --- metadata : name : cluster - autoscaler namespace : cluster - autoscaler - 2 ... -- nodes = 1 : 10 : k8s - worker - asg - 3 -- nodes = 1 : 10 : k8s - worker - asg - 4 Ensure that: Each shard is configured to point to a unique set of EC2 Auto Scaling Groups Each shard is deployed to a separate namespace to avoid leader election conflicts Optimizing for Cost and Availability \u00b6 Spot Instances \u00b6 You can use Spot Instances in your node groups and save up to 90% off the on-demand price, with the trade-off the Spot Instances can be interrupted at any time when EC2 needs the capacity back. Insufficient Capacity Errors will occur when your EC2 Auto Scaling group cannot scale up due to lack of available capacity. Maximizing diversity by selecting many instance families can increase your chance of achieving your desired scale by tapping into many Spot capacity pools, and decrease the impact of Spot Instance interruptions on your cluster availability. Mixed Instance Policies with Spot Instances are a great way to increase diversity without increasing the number of node groups. Keep in mind, if you need guaranteed resources, use On-Demand Instances instead of Spot Instances. It\u2019s critical that all Instance Types have similar resource capacity when configuring Mixed Instance Policies. The autoscaler\u2019s scheduling simulator uses the first InstanceType in the MixedInstancePolicy. If subsequent Instance Types are larger, resources may be wasted after a scale up. If smaller, your pods may fail to schedule on the new instances due to insufficient capacity. For example, M4, M5, M5a, and M5n instances all have similar amounts of CPU and Memory and are great candidates for a MixedInstancePolicy. The EC2 Instance Selector tool can help you identify similar instance types. It's recommended to isolate On-Demand and Spot capacity into separate EC2 Auto Scaling groups. This is preferred over using a base capacity strategy because the scheduling properties are fundamentally different. Since Spot Instances be interrupted at any time (when EC2 needs the capacity back), users will often taint their preemptable nodes, requiring an explicit pod toleration to the preemption behavior. These taints result in different scheduling properties for the nodes, so they should be separated into multiple EC2 Auto Scaling Groups. The Cluster Autoscaler has a concept of Expanders , which provide different strategies for selecting which Node Group to scale. The strategy --expander=least-waste is a good general purpose default, and if you're going to use multiple node groups for Spot Instance diversification (as described in the image above), it could help further cost-optimize the node groups by scaling the group which would be best utilized after the scaling activity. Prioritizing a node group / ASG \u00b6 You may also configure priority based autoscaling by using the Priority expander. --expander=priority enables your cluster to prioritize a node group / ASG, and if it is unable to scale for any reason, it will choose the next node group in the prioritized list. This is useful in situations where, for example, you want to use P3 instance types because their GPU provides optimal performance for your workload, but as a second option you can also use P2 instance types. apiVersion : v1 kind : ConfigMap metadata : name : cluster - autoscaler - priority - expander namespace : kube - system data : priority : |- 10 : - .* p2 - node - group .* 50 : - .* p3 - node - group .* Cluster Autoscaler will try to scale up the EC2 Auto Scaling group matching the name p2-node-group . If this operation does not succeed within --max-node-provision-time , it will attempt to scale an EC2 Auto Scaling group matching the name p3-node-group . This value defaults to 15 minutes and can be reduced for more responsive node group selection, though if the value is too low, it can cause unnecessary scale outs. Overprovisioning \u00b6 The Cluster Autoscaler minimizes costs by ensuring that nodes are only added to the cluster when needed and are removed when unused. This significantly impacts deployment latency because many pods will be forced to wait for a node scale up before they can be scheduled. Nodes can take multiple minutes to become available, which can increase pod scheduling latency by an order of magnitude. This can be mitigated using overprovisioning , which trades cost for scheduling latency. Overprovisioning is implemented using temporary pods with negative priority, which occupy space in the cluster. When newly created pods are unschedulable and have higher priority, the temporary pods will be preempted to make room. The temporary pods then become unschedulable, triggering the Cluster Autoscaler to scale out new overprovisioned nodes. There are other less obvious benefits to overprovisioning. Without overprovisioning, one of the side effects of a highly utilized cluster is that pods will make less optimal scheduling decisions using the preferredDuringSchedulingIgnoredDuringExecution rule of Pod or Node Affinity. A common use case for this is to separate pods for a highly available application across availability zones using AntiAffinity. Overprovisioning can significantly increase the chance that a node of the correct zone is available. The amount of overprovisioned capacity is a careful business decision for your organization. At its core, it\u2019s a tradeoff between performance and cost. One way to make this decision is to determine your average scale up frequency and divide it by the amount of time it takes to scale up a new node. For example, if on average you require a new node every 30 seconds and EC2 takes 30 seconds to provision a new node, a single node of overprovisioning will ensure that there\u2019s always an extra node available, reducing scheduling latency by 30 seconds at the cost of a single additional EC2 Instance. To improve zonal scheduling decisions, overprovision a number of nodes equal to the number of availability zones in your EC2 Auto Scaling Group to ensure that the scheduler can select the best zone for incoming pods. Prevent Scale Down Eviction \u00b6 Some workloads are expensive to evict. Big data analysis, machine learning tasks, and test runners will eventually complete, but must be restarted if interrupted. The Cluster Autoscaler will attempt to scale down any node under the scale-down-utilization-threshold, which will interrupt any remaining pods on the node. This can be prevented by ensuring that pods that are expensive to evict are protected by a label recognized by the Cluster Autoscaler. Ensure that: Expensive to evict pods have the label cluster-autoscaler.kubernetes.io/safe-to-evict=false Advanced Use Cases \u00b6 EBS Volumes \u00b6 Persistent storage is critical for building stateful applications, such as database or distributed caches. EBS Volumes enable this use case on Kubernetes, but are limited to a specific zone. These applications can be highly available if sharded across multiple AZs using a separate EBS Volume for each AZ. The Cluster Autoscaler can then balance the scaling of the EC2 Autoscaling Groups. Ensure that: Node group balancing is enabled by setting balance-similar-node-groups=true . Node Groups are configured with identical settings except for different availability zones and EBS Volumes. Co-Scheduling \u00b6 Machine learning distributed training jobs benefit significantly from the minimized latency of same-zone node configurations. These workloads deploy multiple pods to a specific zone. This can be achieved by setting Pod Affinity for all co-scheduled pods or Node Affinity using topologyKey: failure-domain.beta.kubernetes.io/zone . The Cluster Autoscaler will then scale out a specific zone to match demands. You may wish to allocate multiple EC2 Auto Scaling Groups, one per availability zone to enable failover for the entire co-scheduled workload. Ensure that: Node group balancing is enabled by setting balance-similar-node-groups=false Node Affinity and/or Pod Preemption is used when clusters include both Regional and Zonal Node Groups. Use Node Affinity to force or encourage regional pods to avoid zonal Node Groups, and vice versa. If zonal pods schedule onto regional node groups, this will result in imbalanced capacity for your regional pods. If your zonal workloads can tolerate disruption and relocation, configure Pod Preemption to enable regionally scaled pods to force preemption and rescheduling on a less contested zone. Accelerators \u00b6 Some clusters take advantage of specialized hardware accelerators such as GPU. When scaling out, the accelerator device plugin can take several minutes to advertise the resource to the cluster. The Cluster Autoscaler has simulated that this node will have the accelerator, but until the accelerator becomes ready and updates the node\u2019s available resources, pending pods can not be scheduled on the node. This can result in repeated unnecessary scale out . Additionally, nodes with accelerators and high CPU or Memory utilization will not be considered for scale down, even if the accelerator is unused. This behavior can be expensive due to the relative cost of accelerators. Instead, the Cluster Autoscaler can apply special rules to consider nodes for scale down if they have unoccupied accelerators. To ensure the correct behavior for these cases, you can configure the kubelet on your accelerator nodes to label the node before it joins the cluster. The Cluster Autoscaler will use this label selector to trigger the accelerator optimized behavior. Ensure that: The Kubelet for GPU nodes is configured with --node-labels k8s.amazonaws.com/accelerator=$ACCELERATOR_TYPE Nodes with Accelerators adhere to the identical scheduling properties rule noted above. Scaling from 0 \u00b6 Cluster Autoscaler is capable of scaling Node Groups to and from zero, which can yield significant cost savings. It detects the CPU, memory, and GPU resources of an Auto Scaling Group by inspecting the InstanceType specified in its LaunchConfiguration or LaunchTemplate. Some pods require additional resources like WindowsENI or PrivateIPv4Address or specific NodeSelectors or Taints which cannot be discovered from the LaunchConfiguration. The Cluster Autoscaler can account for these factors by discovering them from tags on the EC2 Auto Scaling Group. For example: Key : k8s . io /cluster-autoscaler/node-template/resources/ $RESOURCE_NAME Value : 5 Key : k8s . io /cluster-autoscaler/node-template/label/ $LABEL_KEY Value : $LABEL_VALUE Key : k8s . io /cluster-autoscaler/node-template/taint/ $TAINT_KEY Value : NoSchedule Note: Keep in mind, when scaling to zero your capacity is returned to EC2 and may be unavailable in the future. Additional Parameters \u00b6 There are many configuration options that can be used to tune the behavior and performance of the Cluster Autoscaler. A complete list of parameters is available on Github . Parameter Description Default scan-interval How often cluster is reevaluated for scale up or down 10 seconds max-empty-bulk-delete Maximum number of empty nodes that can be deleted at the same time. 10 scale-down-delay-after-add How long after scale up that scale down evaluation resumes 10 minutes scale-down-delay-after-delete How long after node deletion that scale down evaluation resumes, defaults to scan-interval scan-interval scale-down-delay-after-failure How long after scale down failure that scale down evaluation resumes 3 minutes scale-down-unneeded-time How long a node should be unneeded before it is eligible for scale down 10 minutes scale-down-unready-time How long an unready node should be unneeded before it is eligible for scale down 20 minutes scale-down-utilization-threshold Node utilization level, defined as sum of requested resources divided by capacity, below which a node can be considered for scale down 0.5 scale-down-non-empty-candidates-count Maximum number of non empty nodes considered in one iteration as candidates for scale down with drain. Lower value means better CA responsiveness but possible slower scale down latency. Higher value can affect CA performance with big clusters (hundreds of nodes). Set to non positive value to turn this heuristic off - CA will not limit the number of nodes it considers.\u201c 30 scale-down-candidates-pool-ratio A ratio of nodes that are considered as additional non empty candidates for scale down when some candidates from previous iteration are no longer valid. Lower value means better CA responsiveness but possible slower scale down latency. Higher value can affect CA performance with big clusters (hundreds of nodes). Set to 1.0 to turn this heuristics off - CA will take all nodes as additional candidates. 0.1 scale-down-candidates-pool-min-count Minimum number of nodes that are considered as additional non empty candidates for scale down when some candidates from previous iteration are no longer valid. When calculating the pool size for additional candidates we take max(#nodes * scale-down-candidates-pool-ratio, scale-down-candidates-pool-min-count) 50 Additional Resources \u00b6 This page contains a list of Cluster Autoscaler presentations and demos. If you'd like to add a presentation or demo here, please send a pull request. Presentation/Demo Presenters Autoscaling and Cost Optimization on Kubernetes: From 0 to 100 Guy Templeton, Skyscanner & Jiaxin Shan, Amazon References \u00b6 https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md https://github.com/aws/amazon-ec2-instance-selector https://github.com/aws/aws-node-termination-handler","title":"Home"},{"location":"cluster-autoscaling/#kubernetes-cluster-autoscaler","text":"","title":"Kubernetes Cluster Autoscaler"},{"location":"cluster-autoscaling/#overview","text":"The Kubernetes Cluster Autoscaler is a popular Cluster Autoscaling solution maintained by SIG Autoscaling . It is responsible for ensuring that your cluster has enough nodes to schedule your pods without wasting resources. It watches for pods that fail to schedule and for nodes that are underutilized. It then simulates the addition or removal of nodes before applying the change to your cluster. The AWS Cloud Provider implementation within Cluster Autoscaler controls the .DesiredReplicas field of your EC2 Auto Scaling Groups. This guide will provide a mental model for configuring the Cluster Autoscaler and choosing the best set of tradeoffs to meet your organization\u2019s requirements. While there is no single best configuration, there are a set of configuration options that enable you to trade off performance, scalability, cost, and availability. Additionally, this guide will provide tips and best practices for optimizing your configuration for AWS.","title":"Overview"},{"location":"cluster-autoscaling/#glossary","text":"The following terminology will be used frequently throughout this document. These terms can have broad meaning, but are limited to the definitions below for the purposes of this document. Scalability refers to how well the Cluster Autoscaler performs as your Kubernetes Cluster increases in number of pods and nodes. As scalability limits are reached, the Cluster Autoscaler\u2019s performance and functionality degrades. As the Cluster Autoscaler exceeds its scalability limits, it may no longer add or remove nodes in your cluster. Performance refers to how quickly the Cluster Autoscaler is able to make and execute scaling decisions. A perfectly performing Cluster Autoscaler would instantly make a decision and trigger a scaling action in response to stimuli, such as a pod becoming unschedulable. Availability means that pods can be scheduled quickly and without disruption. This includes when newly created pods need to be scheduled and when a scaled down node terminates any remaining pods scheduled to it. Cost is determined by the decision behind scale out and scale in events. Resources are wasted if an existing node is underutilized or a new node is added that is too large for incoming pods. Depending on the use case, there can be costs associated with prematurely terminating pods due to an aggressive scale down decision. Node Groups are an abstract Kubernetes concept for a group of nodes within a cluster. It is not a true Kubernetes resource, but exists as an abstraction in the Cluster Autoscaler, Cluster API, and other components. Nodes within a Node Group share properties like labels and taints, but may consist of multiple Availability Zones or Instance Types. EC2 Auto Scaling Groups can be used as an implementation of Node Groups on EC2. EC2 Auto Scaling Groups are configured to launch instances that automatically join their Kubernetes Clusters and apply labels and taints to their corresponding Node resource in the Kubernetes API. EC2 Managed Node Groups are another implementation of Node Groups on EC2. They abstract away the complexity manually configuring EC2 Autoscaling Scaling Groups and provide additional management features like node version upgrade and graceful node termination.","title":"Glossary"},{"location":"cluster-autoscaling/#operating-the-cluster-autoscaler","text":"The Cluster Autoscaler is typically installed as a Deployment in your cluster. It uses leader election to ensure high availability, but work is done by a single replica at a time. It is not horizontally scalable. For basic setups, the default it should work out of the box using the provided installation instructions , but there are a few things to keep in mind. Ensure that: The Cluster Autoscaler\u2019s version matches the Cluster\u2019s Version. Cross version compatibility is not tested or supported . Auto Discovery is enabled, unless you have specific advanced use cases that prevent use of this mode.","title":"Operating the Cluster Autoscaler"},{"location":"cluster-autoscaling/#configuring-your-node-groups","text":"Effective autoscaling starts with correctly configuring a set of Node Groups for your cluster. Selecting the right set of Node Groups is key to maximizing availability and reducing cost across your workloads. AWS implements Node Groups using EC2 Auto Scaling Groups, which are flexible to a large number of use cases. However, the Cluster Autoscaler makes some assumptions about your Node Groups. Keeping your EC2 Auto Scaling Group configurations consistent with these assumptions will minimize undesired behavior. Ensure that: Each Node in a Node Group has identical scheduling properties, such as Labels, Taints, and Resources. For MixedInstancePolicies, the Instance Types must be of the same shape for CPU, Memory, and GPU The first Instance Type specified in the policy will be used to simulate scheduling. If your policy has additional Instance Types with more resources, resources may be wasted after scale out. If your policy has additional Instance Types with less resources, pods may fail to schedule on the instances. Node Groups with many nodes are preferred over many Node Groups with fewer nodes. This will have the biggest impact on scalability. Wherever possible, prefer EC2 features when both systems provide support (e.g. Regions, MixedInstancePolicy) Note: If possible, we recommend using EKS Managed Node Groups . Managed Node Groups come with powerful management features, including features for Cluster Autoscaler like automatic EC2 Auto Scaling Group discovery and graceful node termination.","title":"Configuring your Node Groups"},{"location":"cluster-autoscaling/#optimizing-for-performance-and-scalability","text":"Understanding the autoscaling algorithm\u2019s runtime complexity will help you tune the Cluster Autoscaler to continue operating smoothly in large clusters with greater than 1,000 nodes . The primary knobs for tuning scalability of the Cluster Autoscaler are the resources provided to the process, the scan interval of the algorithm, and the number of Node Groups in the cluster. There are other factors involved in the true runtime complexity of this algorithm, such as scheduling plugin complexity and number of pods. These are considered to be unconfigurable parameters as they are natural to the cluster\u2019s workload and cannot easily be tuned. The Cluster Autoscaler loads the entire cluster\u2019s state into memory, including Pods, Nodes, and Node Groups. On each scan interval, the algorithm identifies unschedulable pods and simulates scheduling for each Node Group. Tuning these factors come with different tradeoffs which should be carefully considered for your use case.","title":"Optimizing for Performance and Scalability"},{"location":"cluster-autoscaling/#vertically-autoscaling-the-cluster-autoscaler","text":"The simplest way to scale the Cluster Autoscaler to larger clusters is to increase the resource requests for its deployment. Both memory and CPU should be increased for large clusters, though this varies significantly with cluster size. The autoscaling algorithm stores all pods and nodes in memory, which can result in a memory footprint larger than a gigabyte in some cases. Increasing resources is typically done manually. If you find that constant resource tuning is creating an operational burden, consider using the Addon Resizer or Vertical Pod Autoscaler .","title":"Vertically Autoscaling the Cluster Autoscaler"},{"location":"cluster-autoscaling/#reducing-the-number-of-node-groups","text":"Minimizing the number of node groups is one way to ensure that the Cluster Autoscaler will continue to perform well on large clusters. This may be challenging for some organizations who structure their node groups per team or per application. While this is fully supported by the Kubernetes API, this is considered to be a Cluster Autoscaler anti-pattern with repercussions for scalability. There are many reasons to use multiple node groups (e.g. Spot or GPUs), but in many cases there are alternative designs that achieve the same effect while using a small number of groups. Ensure that: Pod isolation is done using Namespaces rather than Node Groups. This may not be possible in low-trust multi-tenant clusters. Pod ResourceRequests and ResourceLimits are properly set to avoid resource contention. Larger instance types will result in more optimal bin packing and reduced system pod overhead. NodeTaints or NodeSelectors are used to schedule pods as the exception, not as the rule. Regional resources are defined as a single EC2 Auto Scaling Group with multiple Availability Zones.","title":"Reducing the number of Node Groups"},{"location":"cluster-autoscaling/#reducing-the-scan-interval","text":"A low scan interval (e.g. 10 seconds) will ensure that the Cluster Autoscaler responds as quickly as possible when pods become unschedulable. However, each scan results in many API calls to the Kubernetes API and EC2 Auto Scaling Group or EKS Managed Node Group APIs. These API calls can result in rate limiting or even service unavailability for your Kubernetes Control Plane. The default scan interval is 10 seconds, but on AWS, launching a node takes significantly longer to launch a new instance. This means that it\u2019s possible to increase the interval without significantly increasing overall scale up time. For example, if it takes 2 minutes to launch a node, changing the interval to 1 minute will result a tradeoff of 6x reduced API calls for 38% slower scale ups.","title":"Reducing the Scan Interval"},{"location":"cluster-autoscaling/#sharding-across-node-groups","text":"The Cluster Autoscaler can be configured to operate on a specific set of Node Groups. Using this functionality, it\u2019s possible to deploy multiple instances of the Cluster Autoscaler, each configured to operate on a different set of Node Groups. This strategy enables you use arbitrarily large numbers of Node Groups, trading cost for scalability. We only recommend using this as a last resort for improving performance. The Cluster Autoscaler was not originally designed for this configuration, so there are some side effects. Since the shards do not communicate, it\u2019s possible for multiple autoscalers to attempt to schedule an unschedulable pod. This can result in unnecessary scale out of multiple Node Groups. These extra nodes will scale back in after the scale-down-delay . metadata : name : cluster - autoscaler namespace : cluster - autoscaler - 1 ... -- nodes = 1 : 10 : k8s - worker - asg - 1 -- nodes = 1 : 10 : k8s - worker - asg - 2 --- metadata : name : cluster - autoscaler namespace : cluster - autoscaler - 2 ... -- nodes = 1 : 10 : k8s - worker - asg - 3 -- nodes = 1 : 10 : k8s - worker - asg - 4 Ensure that: Each shard is configured to point to a unique set of EC2 Auto Scaling Groups Each shard is deployed to a separate namespace to avoid leader election conflicts","title":"Sharding Across Node Groups"},{"location":"cluster-autoscaling/#optimizing-for-cost-and-availability","text":"","title":"Optimizing for Cost and Availability"},{"location":"cluster-autoscaling/#spot-instances","text":"You can use Spot Instances in your node groups and save up to 90% off the on-demand price, with the trade-off the Spot Instances can be interrupted at any time when EC2 needs the capacity back. Insufficient Capacity Errors will occur when your EC2 Auto Scaling group cannot scale up due to lack of available capacity. Maximizing diversity by selecting many instance families can increase your chance of achieving your desired scale by tapping into many Spot capacity pools, and decrease the impact of Spot Instance interruptions on your cluster availability. Mixed Instance Policies with Spot Instances are a great way to increase diversity without increasing the number of node groups. Keep in mind, if you need guaranteed resources, use On-Demand Instances instead of Spot Instances. It\u2019s critical that all Instance Types have similar resource capacity when configuring Mixed Instance Policies. The autoscaler\u2019s scheduling simulator uses the first InstanceType in the MixedInstancePolicy. If subsequent Instance Types are larger, resources may be wasted after a scale up. If smaller, your pods may fail to schedule on the new instances due to insufficient capacity. For example, M4, M5, M5a, and M5n instances all have similar amounts of CPU and Memory and are great candidates for a MixedInstancePolicy. The EC2 Instance Selector tool can help you identify similar instance types. It's recommended to isolate On-Demand and Spot capacity into separate EC2 Auto Scaling groups. This is preferred over using a base capacity strategy because the scheduling properties are fundamentally different. Since Spot Instances be interrupted at any time (when EC2 needs the capacity back), users will often taint their preemptable nodes, requiring an explicit pod toleration to the preemption behavior. These taints result in different scheduling properties for the nodes, so they should be separated into multiple EC2 Auto Scaling Groups. The Cluster Autoscaler has a concept of Expanders , which provide different strategies for selecting which Node Group to scale. The strategy --expander=least-waste is a good general purpose default, and if you're going to use multiple node groups for Spot Instance diversification (as described in the image above), it could help further cost-optimize the node groups by scaling the group which would be best utilized after the scaling activity.","title":"Spot Instances"},{"location":"cluster-autoscaling/#prioritizing-a-node-group-asg","text":"You may also configure priority based autoscaling by using the Priority expander. --expander=priority enables your cluster to prioritize a node group / ASG, and if it is unable to scale for any reason, it will choose the next node group in the prioritized list. This is useful in situations where, for example, you want to use P3 instance types because their GPU provides optimal performance for your workload, but as a second option you can also use P2 instance types. apiVersion : v1 kind : ConfigMap metadata : name : cluster - autoscaler - priority - expander namespace : kube - system data : priority : |- 10 : - .* p2 - node - group .* 50 : - .* p3 - node - group .* Cluster Autoscaler will try to scale up the EC2 Auto Scaling group matching the name p2-node-group . If this operation does not succeed within --max-node-provision-time , it will attempt to scale an EC2 Auto Scaling group matching the name p3-node-group . This value defaults to 15 minutes and can be reduced for more responsive node group selection, though if the value is too low, it can cause unnecessary scale outs.","title":"Prioritizing a node group / ASG"},{"location":"cluster-autoscaling/#overprovisioning","text":"The Cluster Autoscaler minimizes costs by ensuring that nodes are only added to the cluster when needed and are removed when unused. This significantly impacts deployment latency because many pods will be forced to wait for a node scale up before they can be scheduled. Nodes can take multiple minutes to become available, which can increase pod scheduling latency by an order of magnitude. This can be mitigated using overprovisioning , which trades cost for scheduling latency. Overprovisioning is implemented using temporary pods with negative priority, which occupy space in the cluster. When newly created pods are unschedulable and have higher priority, the temporary pods will be preempted to make room. The temporary pods then become unschedulable, triggering the Cluster Autoscaler to scale out new overprovisioned nodes. There are other less obvious benefits to overprovisioning. Without overprovisioning, one of the side effects of a highly utilized cluster is that pods will make less optimal scheduling decisions using the preferredDuringSchedulingIgnoredDuringExecution rule of Pod or Node Affinity. A common use case for this is to separate pods for a highly available application across availability zones using AntiAffinity. Overprovisioning can significantly increase the chance that a node of the correct zone is available. The amount of overprovisioned capacity is a careful business decision for your organization. At its core, it\u2019s a tradeoff between performance and cost. One way to make this decision is to determine your average scale up frequency and divide it by the amount of time it takes to scale up a new node. For example, if on average you require a new node every 30 seconds and EC2 takes 30 seconds to provision a new node, a single node of overprovisioning will ensure that there\u2019s always an extra node available, reducing scheduling latency by 30 seconds at the cost of a single additional EC2 Instance. To improve zonal scheduling decisions, overprovision a number of nodes equal to the number of availability zones in your EC2 Auto Scaling Group to ensure that the scheduler can select the best zone for incoming pods.","title":"Overprovisioning"},{"location":"cluster-autoscaling/#prevent-scale-down-eviction","text":"Some workloads are expensive to evict. Big data analysis, machine learning tasks, and test runners will eventually complete, but must be restarted if interrupted. The Cluster Autoscaler will attempt to scale down any node under the scale-down-utilization-threshold, which will interrupt any remaining pods on the node. This can be prevented by ensuring that pods that are expensive to evict are protected by a label recognized by the Cluster Autoscaler. Ensure that: Expensive to evict pods have the label cluster-autoscaler.kubernetes.io/safe-to-evict=false","title":"Prevent Scale Down Eviction"},{"location":"cluster-autoscaling/#advanced-use-cases","text":"","title":"Advanced Use Cases"},{"location":"cluster-autoscaling/#ebs-volumes","text":"Persistent storage is critical for building stateful applications, such as database or distributed caches. EBS Volumes enable this use case on Kubernetes, but are limited to a specific zone. These applications can be highly available if sharded across multiple AZs using a separate EBS Volume for each AZ. The Cluster Autoscaler can then balance the scaling of the EC2 Autoscaling Groups. Ensure that: Node group balancing is enabled by setting balance-similar-node-groups=true . Node Groups are configured with identical settings except for different availability zones and EBS Volumes.","title":"EBS Volumes"},{"location":"cluster-autoscaling/#co-scheduling","text":"Machine learning distributed training jobs benefit significantly from the minimized latency of same-zone node configurations. These workloads deploy multiple pods to a specific zone. This can be achieved by setting Pod Affinity for all co-scheduled pods or Node Affinity using topologyKey: failure-domain.beta.kubernetes.io/zone . The Cluster Autoscaler will then scale out a specific zone to match demands. You may wish to allocate multiple EC2 Auto Scaling Groups, one per availability zone to enable failover for the entire co-scheduled workload. Ensure that: Node group balancing is enabled by setting balance-similar-node-groups=false Node Affinity and/or Pod Preemption is used when clusters include both Regional and Zonal Node Groups. Use Node Affinity to force or encourage regional pods to avoid zonal Node Groups, and vice versa. If zonal pods schedule onto regional node groups, this will result in imbalanced capacity for your regional pods. If your zonal workloads can tolerate disruption and relocation, configure Pod Preemption to enable regionally scaled pods to force preemption and rescheduling on a less contested zone.","title":"Co-Scheduling"},{"location":"cluster-autoscaling/#accelerators","text":"Some clusters take advantage of specialized hardware accelerators such as GPU. When scaling out, the accelerator device plugin can take several minutes to advertise the resource to the cluster. The Cluster Autoscaler has simulated that this node will have the accelerator, but until the accelerator becomes ready and updates the node\u2019s available resources, pending pods can not be scheduled on the node. This can result in repeated unnecessary scale out . Additionally, nodes with accelerators and high CPU or Memory utilization will not be considered for scale down, even if the accelerator is unused. This behavior can be expensive due to the relative cost of accelerators. Instead, the Cluster Autoscaler can apply special rules to consider nodes for scale down if they have unoccupied accelerators. To ensure the correct behavior for these cases, you can configure the kubelet on your accelerator nodes to label the node before it joins the cluster. The Cluster Autoscaler will use this label selector to trigger the accelerator optimized behavior. Ensure that: The Kubelet for GPU nodes is configured with --node-labels k8s.amazonaws.com/accelerator=$ACCELERATOR_TYPE Nodes with Accelerators adhere to the identical scheduling properties rule noted above.","title":"Accelerators"},{"location":"cluster-autoscaling/#scaling-from-0","text":"Cluster Autoscaler is capable of scaling Node Groups to and from zero, which can yield significant cost savings. It detects the CPU, memory, and GPU resources of an Auto Scaling Group by inspecting the InstanceType specified in its LaunchConfiguration or LaunchTemplate. Some pods require additional resources like WindowsENI or PrivateIPv4Address or specific NodeSelectors or Taints which cannot be discovered from the LaunchConfiguration. The Cluster Autoscaler can account for these factors by discovering them from tags on the EC2 Auto Scaling Group. For example: Key : k8s . io /cluster-autoscaler/node-template/resources/ $RESOURCE_NAME Value : 5 Key : k8s . io /cluster-autoscaler/node-template/label/ $LABEL_KEY Value : $LABEL_VALUE Key : k8s . io /cluster-autoscaler/node-template/taint/ $TAINT_KEY Value : NoSchedule Note: Keep in mind, when scaling to zero your capacity is returned to EC2 and may be unavailable in the future.","title":"Scaling from 0"},{"location":"cluster-autoscaling/#additional-parameters","text":"There are many configuration options that can be used to tune the behavior and performance of the Cluster Autoscaler. A complete list of parameters is available on Github . Parameter Description Default scan-interval How often cluster is reevaluated for scale up or down 10 seconds max-empty-bulk-delete Maximum number of empty nodes that can be deleted at the same time. 10 scale-down-delay-after-add How long after scale up that scale down evaluation resumes 10 minutes scale-down-delay-after-delete How long after node deletion that scale down evaluation resumes, defaults to scan-interval scan-interval scale-down-delay-after-failure How long after scale down failure that scale down evaluation resumes 3 minutes scale-down-unneeded-time How long a node should be unneeded before it is eligible for scale down 10 minutes scale-down-unready-time How long an unready node should be unneeded before it is eligible for scale down 20 minutes scale-down-utilization-threshold Node utilization level, defined as sum of requested resources divided by capacity, below which a node can be considered for scale down 0.5 scale-down-non-empty-candidates-count Maximum number of non empty nodes considered in one iteration as candidates for scale down with drain. Lower value means better CA responsiveness but possible slower scale down latency. Higher value can affect CA performance with big clusters (hundreds of nodes). Set to non positive value to turn this heuristic off - CA will not limit the number of nodes it considers.\u201c 30 scale-down-candidates-pool-ratio A ratio of nodes that are considered as additional non empty candidates for scale down when some candidates from previous iteration are no longer valid. Lower value means better CA responsiveness but possible slower scale down latency. Higher value can affect CA performance with big clusters (hundreds of nodes). Set to 1.0 to turn this heuristics off - CA will take all nodes as additional candidates. 0.1 scale-down-candidates-pool-min-count Minimum number of nodes that are considered as additional non empty candidates for scale down when some candidates from previous iteration are no longer valid. When calculating the pool size for additional candidates we take max(#nodes * scale-down-candidates-pool-ratio, scale-down-candidates-pool-min-count) 50","title":"Additional Parameters"},{"location":"cluster-autoscaling/#additional-resources","text":"This page contains a list of Cluster Autoscaler presentations and demos. If you'd like to add a presentation or demo here, please send a pull request. Presentation/Demo Presenters Autoscaling and Cost Optimization on Kubernetes: From 0 to 100 Guy Templeton, Skyscanner & Jiaxin Shan, Amazon","title":"Additional Resources"},{"location":"cluster-autoscaling/#references","text":"https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md https://github.com/aws/amazon-ec2-instance-selector https://github.com/aws/aws-node-termination-handler","title":"References"},{"location":"compliance/","text":"Compliance \u00b6 Compliance is a shared responsibility between AWS and the consumers of its services. Generally speaking, AWS is responsible for \u201csecurity of the cloud\u201d whereas its users are responsible for \u201csecurity in the cloud.\u201d The line that delineates what AWS and its users are responsible for, will vary depending on the service. For example, with Fargate, AWS is responsible for managing the physical security of its data centers, the hardware, the virtual infrastructure (Amazon EC2), and the container runtime (Docker). Users of Fargate are responsible for securing the container image and their application. Knowing who is responsible for what is an important consideration when running workloads that must adhere to compliance standards. The following table shows the compliance programs with which the different container services conform. Compliance Program Amazon ECS Orchestrator Amazon EKS Orchestrator ECS Fargate Amazon ECR PCI DSS Level 1 1 1 1 1 HIPAA Eligible 1 1 1 1 SOC I 1 1 1 1 SOC II 1 1 1 1 SOC III 1 1 1 1 ISO 27001:2013 1 1 1 1 ISO 9001:2015 1 1 1 1 ISO 27017:2015 1 1 1 1 ISO 27018:2019 1 1 1 1 IRAP 1 0 1 1 FedRAMP Moderate (East/West) 1 JAB Review 0 1 FedRAMP High (GovCloud) 1 JAB Review 0 1 DOD CC SRG 1 JAB Review 0 1 HIPAA BAA 1 1 1 1 MTCS 1 1 0 1 C5 1 1 0 1 K-ISMS 1 1 0 1 ENS High 1 1 0 1 OSPAR 1 1 0 1 HITRUST CSF 1 1 1 1 Compliance status changes over time. For the latest status, always refer to https://aws.amazon.com/compliance/services-in-scope/ . For futher information about cloud accreditation models and best practices, see the AWS whitepaper, Acceditation Models for Secure Cloud Adoption Shifting Left \u00b6 The concept of shifting left involves catching policy violations and errors earlier in the software development lifecycle. From a security perspective, this can be very beneficial. A developer, for example, can fix issues with their configuration before their application is deployed to the cluster. Catching mistakes like this earlier will help prevent configurations that violate your policies from being deployed. Policy \u00b6 Policy can be thought of as a set of rules for governing behaviors, i.e. behaviors that are allowed or those that are prohibited. For example, you may have a policy that says that all Dockerfiles should include a USER directive that causes the container to run as a non-root user. As a document, a policy like this can be hard to discover and enforce. It may also become outdated as your requirements change. Recommendations \u00b6 Use Open Policy Agent (OPA) or Alcide's sKan to detect policy violations before deployment \u00b6 OPA is open source policy engine that's part of CNCF. It's used for making policy decisions and can be run a variety of different ways, e.g. as a language library or a service. OPA policies are written in a Domain Specific Language (DSL) called Rego. While it is often run as part of a Kubernetes Dynamic Admission Controller, OPA can also be incorporated into your CI/CD pipeline. This allows developers to get feedback about their configuration earlier in the release cycle which can subsequently help them resolve issues before they get to production. A collection of common OPA policies can be found in the GitHub repository for this project. Conftest is built on top of OPA and it provides a developer focused experience for testing Kubernetes configuration. sKan is powered by OPA and is \"tailor made\" to check whether their Kubernetes configuration files are compliant with security and operational best practices. Tools and resources \u00b6 kube-bench docker-bench-security actuary AWS Inspector Kubernetes Security Review A 3rd party security assessment of Kubernetes 1.13.4 (2019)","title":"Regulatory Compliance"},{"location":"compliance/#compliance","text":"Compliance is a shared responsibility between AWS and the consumers of its services. Generally speaking, AWS is responsible for \u201csecurity of the cloud\u201d whereas its users are responsible for \u201csecurity in the cloud.\u201d The line that delineates what AWS and its users are responsible for, will vary depending on the service. For example, with Fargate, AWS is responsible for managing the physical security of its data centers, the hardware, the virtual infrastructure (Amazon EC2), and the container runtime (Docker). Users of Fargate are responsible for securing the container image and their application. Knowing who is responsible for what is an important consideration when running workloads that must adhere to compliance standards. The following table shows the compliance programs with which the different container services conform. Compliance Program Amazon ECS Orchestrator Amazon EKS Orchestrator ECS Fargate Amazon ECR PCI DSS Level 1 1 1 1 1 HIPAA Eligible 1 1 1 1 SOC I 1 1 1 1 SOC II 1 1 1 1 SOC III 1 1 1 1 ISO 27001:2013 1 1 1 1 ISO 9001:2015 1 1 1 1 ISO 27017:2015 1 1 1 1 ISO 27018:2019 1 1 1 1 IRAP 1 0 1 1 FedRAMP Moderate (East/West) 1 JAB Review 0 1 FedRAMP High (GovCloud) 1 JAB Review 0 1 DOD CC SRG 1 JAB Review 0 1 HIPAA BAA 1 1 1 1 MTCS 1 1 0 1 C5 1 1 0 1 K-ISMS 1 1 0 1 ENS High 1 1 0 1 OSPAR 1 1 0 1 HITRUST CSF 1 1 1 1 Compliance status changes over time. For the latest status, always refer to https://aws.amazon.com/compliance/services-in-scope/ . For futher information about cloud accreditation models and best practices, see the AWS whitepaper, Acceditation Models for Secure Cloud Adoption","title":"Compliance"},{"location":"compliance/#shifting-left","text":"The concept of shifting left involves catching policy violations and errors earlier in the software development lifecycle. From a security perspective, this can be very beneficial. A developer, for example, can fix issues with their configuration before their application is deployed to the cluster. Catching mistakes like this earlier will help prevent configurations that violate your policies from being deployed.","title":"Shifting Left"},{"location":"compliance/#policy","text":"Policy can be thought of as a set of rules for governing behaviors, i.e. behaviors that are allowed or those that are prohibited. For example, you may have a policy that says that all Dockerfiles should include a USER directive that causes the container to run as a non-root user. As a document, a policy like this can be hard to discover and enforce. It may also become outdated as your requirements change.","title":"Policy"},{"location":"compliance/#recommendations","text":"","title":"Recommendations"},{"location":"compliance/#use-open-policy-agent-opa-or-alcides-skan-to-detect-policy-violations-before-deployment","text":"OPA is open source policy engine that's part of CNCF. It's used for making policy decisions and can be run a variety of different ways, e.g. as a language library or a service. OPA policies are written in a Domain Specific Language (DSL) called Rego. While it is often run as part of a Kubernetes Dynamic Admission Controller, OPA can also be incorporated into your CI/CD pipeline. This allows developers to get feedback about their configuration earlier in the release cycle which can subsequently help them resolve issues before they get to production. A collection of common OPA policies can be found in the GitHub repository for this project. Conftest is built on top of OPA and it provides a developer focused experience for testing Kubernetes configuration. sKan is powered by OPA and is \"tailor made\" to check whether their Kubernetes configuration files are compliant with security and operational best practices.","title":"Use Open Policy Agent (OPA) or Alcide's sKan to detect policy violations before deployment"},{"location":"compliance/#tools-and-resources","text":"kube-bench docker-bench-security actuary AWS Inspector Kubernetes Security Review A 3rd party security assessment of Kubernetes 1.13.4 (2019)","title":"Tools and resources"},{"location":"data/","text":"Encryption at rest \u00b6 There are three different AWS-native storage options you can use with Kubernetes: EBS , EFS , and FSx for Lustre . All three offer encryption at rest using a service managed key or a customer master key (CMK). For EBS you can use the in-tree storage driver or the EBS CSI driver . Both include parameters for encrypting volumes and supplying a CMK. For EFS, you can use the EFS CSI driver , however, unlike EBS, the EFS CSI driver does not support dynamic provisioning. If you want to use EFS with EKS, you will need to provision and configure at-rest encryption for the file system prior to creating a PV. For further information about EFS file encryption, please refer to Encrypting Data at Rest . Besides offering at-rest encryption, EFS and FSx for Lustre include an option for encrypting data in transit. FSx for Luster does this by default. For EFS, you can add transport encryption by adding the tls parameter to mountOptions in your PV as in this example: apiVersion : v1 kind : PersistentVolume metadata : name : efs-pv spec : capacity : storage : 5Gi volumeMode : Filesystem accessModes : - ReadWriteOnce persistentVolumeReclaimPolicy : Retain storageClassName : efs-sc mountOptions : - tls csi : driver : efs.csi.aws.com volumeHandle : <file_system_id> The FSx CSI driver supports dynamic provisioning of Lustre file systems. It encrypts data with a service managed key by default, although there is an option to provide you own CMK as in this example: kind : StorageClass apiVersion : storage.k8s.io/v1 metadata : name : fsx-sc provisioner : fsx.csi.aws.com parameters : subnetId : subnet-056da83524edbe641 securityGroupIds : sg-086f61ea73388fb6b deploymentType : PERSISTENT_1 kmsKeyId : <kms_arn> Attention As of May 28, 2020 all data written to the ephemeral volume in EKS Fargate pods is encrypted by default using an industry-standard AES-256 cryptographic algorithm. No modifications to your application are necessary as encryption and decryption are handled seamlessly by the service. Recommendations \u00b6 Encrypt data at rest \u00b6 Encrypting data at rest is considered a best practice. If you're unsure whether encryption is necessary, encrypt your data. Rotate your CMKs periodically \u00b6 Configure KMS to automatically rotate you CMKs. This will rotate your keys once a year while saving old keys indefinitely so that your data can still be decrypted. For additional information see Rotating customer master keys Use EFS access points to simplify access to shared datasets \u00b6 If you have shared datasets with different POSIX file permissions or want to restrict access to part of the shared file system by creating different mount points, consider using EFS access points. To learn more about working with access points, see https://docs.aws.amazon.com/efs/latest/ug/efs-access-points.html . Today, if you want to use an access point (AP) you'll need to reference the AP in the PV's volumeHandle parameter. Secrets management \u00b6 Kubernetes secrets are used to store sensitive information, such as user certificates, passwords, or API keys. They are persisted in etcd as base64 encoded strings. On EKS, the EBS volumes for etcd nodes are encypted with EBS encryption . A pod can retrieve a Kubernetes secrets objects by referencing the secret in the podSpec . These secrets can either be mapped to an environment variable or mounted as volume. For additional information on creating secrets, see https://kubernetes.io/docs/concepts/configuration/secret/ . Caution Secrets in a particular namespace can be referenced by all pods in the secret's namespace. Caution The node authorizer allows the Kubelet to read all of the secrets mounted to the node. Recommendations \u00b6 Use AWS KMS for envelope encryption of Kubernetes secrets \u00b6 This allows you to encrypt your secrets with a unique data encryption key (DEK). The DEK is then encypted using a key encryption key (KEK) from AWS KMS which can be automatically rotated on a recurring schedule. With the KMS plugin for Kubernetes, all Kubernetes secrets are stored in etcd in ciphertext instead of plain text and can only be decrypted by the Kubernetes API server. For additional details, see using EKS encryption provider support for defense in depth Audit the use of secrets \u00b6 On EKS, turn on audit logging and create a CloudWatch metrics filter and alarm to alert you when a secret is used (optional). The following is an example of a metrics filter for the Kubernetes audit log, {($.verb=\"get\") && ($.objectRef.resource=\"secret\")} . You can also use the following queries with CloudWatch Log Insights: fields @timestamp, @message | sort @timestamp desc | limit 100 | stats count(*) by objectRef.name as secret | filter verb=\"get\" and objectRef.resource=\"secrets\" The above query will display the number of times a secret has been accessed within a specific timeframe. fields @timestamp, @message | sort @timestamp desc | limit 100 | filter verb=\"get\" and objectRef.resource=\"secrets\" | display objectRef.namespace, objectRef.name, user.username, responseStatus.code This query will display the secret, along with the namespace and username of the user who attempted to access the secret and the response code. Rotate your secrets periodically \u00b6 Kubernetes doesn't automatically rotate secrets. If you have to rotate secrets, consider using an external secret store, e.g. Vault or AWS Secrets Manager. Use separate namespaces as a way to isolate secrets from different applications \u00b6 If you have secrets that cannot be shared between applications in a namespace, create a separate namespace for those applications. Use volume mounts instead of environment variables \u00b6 The values of environment variables can unintentionally appear in logs. Secrets mounted as volumes are instantiated as tmpfs volumes (a RAM backed file system) that are automatically removed from the node when the pod is deleted. Use an external secrets provider \u00b6 There are several viable alternatives to using Kubernetes secrets, include Bitnami's Sealed Secrets and Hashicorp's Vault . Unlike Kubernetes secrets which can be shared amongst all of the pods within a namespace, Vault gives you the ability to limit access to particular pods through the use of Kubernetes service accounts. It also has support for secret rotation. If Vault is not to your liking, you can use similar approach with AWS Secrets Manager, as in this example https://github.com/jicowan/secret-sidecar or you could try using a serverless mutating webhook instead.","title":"Data Encryption and Secrets Management"},{"location":"data/#encryption-at-rest","text":"There are three different AWS-native storage options you can use with Kubernetes: EBS , EFS , and FSx for Lustre . All three offer encryption at rest using a service managed key or a customer master key (CMK). For EBS you can use the in-tree storage driver or the EBS CSI driver . Both include parameters for encrypting volumes and supplying a CMK. For EFS, you can use the EFS CSI driver , however, unlike EBS, the EFS CSI driver does not support dynamic provisioning. If you want to use EFS with EKS, you will need to provision and configure at-rest encryption for the file system prior to creating a PV. For further information about EFS file encryption, please refer to Encrypting Data at Rest . Besides offering at-rest encryption, EFS and FSx for Lustre include an option for encrypting data in transit. FSx for Luster does this by default. For EFS, you can add transport encryption by adding the tls parameter to mountOptions in your PV as in this example: apiVersion : v1 kind : PersistentVolume metadata : name : efs-pv spec : capacity : storage : 5Gi volumeMode : Filesystem accessModes : - ReadWriteOnce persistentVolumeReclaimPolicy : Retain storageClassName : efs-sc mountOptions : - tls csi : driver : efs.csi.aws.com volumeHandle : <file_system_id> The FSx CSI driver supports dynamic provisioning of Lustre file systems. It encrypts data with a service managed key by default, although there is an option to provide you own CMK as in this example: kind : StorageClass apiVersion : storage.k8s.io/v1 metadata : name : fsx-sc provisioner : fsx.csi.aws.com parameters : subnetId : subnet-056da83524edbe641 securityGroupIds : sg-086f61ea73388fb6b deploymentType : PERSISTENT_1 kmsKeyId : <kms_arn> Attention As of May 28, 2020 all data written to the ephemeral volume in EKS Fargate pods is encrypted by default using an industry-standard AES-256 cryptographic algorithm. No modifications to your application are necessary as encryption and decryption are handled seamlessly by the service.","title":"Encryption at rest"},{"location":"data/#recommendations","text":"","title":"Recommendations"},{"location":"data/#encrypt-data-at-rest","text":"Encrypting data at rest is considered a best practice. If you're unsure whether encryption is necessary, encrypt your data.","title":"Encrypt data at rest"},{"location":"data/#rotate-your-cmks-periodically","text":"Configure KMS to automatically rotate you CMKs. This will rotate your keys once a year while saving old keys indefinitely so that your data can still be decrypted. For additional information see Rotating customer master keys","title":"Rotate your CMKs periodically"},{"location":"data/#use-efs-access-points-to-simplify-access-to-shared-datasets","text":"If you have shared datasets with different POSIX file permissions or want to restrict access to part of the shared file system by creating different mount points, consider using EFS access points. To learn more about working with access points, see https://docs.aws.amazon.com/efs/latest/ug/efs-access-points.html . Today, if you want to use an access point (AP) you'll need to reference the AP in the PV's volumeHandle parameter.","title":"Use EFS access points to simplify access to shared datasets"},{"location":"data/#secrets-management","text":"Kubernetes secrets are used to store sensitive information, such as user certificates, passwords, or API keys. They are persisted in etcd as base64 encoded strings. On EKS, the EBS volumes for etcd nodes are encypted with EBS encryption . A pod can retrieve a Kubernetes secrets objects by referencing the secret in the podSpec . These secrets can either be mapped to an environment variable or mounted as volume. For additional information on creating secrets, see https://kubernetes.io/docs/concepts/configuration/secret/ . Caution Secrets in a particular namespace can be referenced by all pods in the secret's namespace. Caution The node authorizer allows the Kubelet to read all of the secrets mounted to the node.","title":"Secrets management"},{"location":"data/#recommendations_1","text":"","title":"Recommendations"},{"location":"data/#use-aws-kms-for-envelope-encryption-of-kubernetes-secrets","text":"This allows you to encrypt your secrets with a unique data encryption key (DEK). The DEK is then encypted using a key encryption key (KEK) from AWS KMS which can be automatically rotated on a recurring schedule. With the KMS plugin for Kubernetes, all Kubernetes secrets are stored in etcd in ciphertext instead of plain text and can only be decrypted by the Kubernetes API server. For additional details, see using EKS encryption provider support for defense in depth","title":"Use AWS KMS for envelope encryption of Kubernetes secrets"},{"location":"data/#audit-the-use-of-secrets","text":"On EKS, turn on audit logging and create a CloudWatch metrics filter and alarm to alert you when a secret is used (optional). The following is an example of a metrics filter for the Kubernetes audit log, {($.verb=\"get\") && ($.objectRef.resource=\"secret\")} . You can also use the following queries with CloudWatch Log Insights: fields @timestamp, @message | sort @timestamp desc | limit 100 | stats count(*) by objectRef.name as secret | filter verb=\"get\" and objectRef.resource=\"secrets\" The above query will display the number of times a secret has been accessed within a specific timeframe. fields @timestamp, @message | sort @timestamp desc | limit 100 | filter verb=\"get\" and objectRef.resource=\"secrets\" | display objectRef.namespace, objectRef.name, user.username, responseStatus.code This query will display the secret, along with the namespace and username of the user who attempted to access the secret and the response code.","title":"Audit the use of secrets"},{"location":"data/#rotate-your-secrets-periodically","text":"Kubernetes doesn't automatically rotate secrets. If you have to rotate secrets, consider using an external secret store, e.g. Vault or AWS Secrets Manager.","title":"Rotate your secrets periodically"},{"location":"data/#use-separate-namespaces-as-a-way-to-isolate-secrets-from-different-applications","text":"If you have secrets that cannot be shared between applications in a namespace, create a separate namespace for those applications.","title":"Use separate namespaces as a way to isolate secrets from different applications"},{"location":"data/#use-volume-mounts-instead-of-environment-variables","text":"The values of environment variables can unintentionally appear in logs. Secrets mounted as volumes are instantiated as tmpfs volumes (a RAM backed file system) that are automatically removed from the node when the pod is deleted.","title":"Use volume mounts instead of environment variables"},{"location":"data/#use-an-external-secrets-provider","text":"There are several viable alternatives to using Kubernetes secrets, include Bitnami's Sealed Secrets and Hashicorp's Vault . Unlike Kubernetes secrets which can be shared amongst all of the pods within a namespace, Vault gives you the ability to limit access to particular pods through the use of Kubernetes service accounts. It also has support for secret rotation. If Vault is not to your liking, you can use similar approach with AWS Secrets Manager, as in this example https://github.com/jicowan/secret-sidecar or you could try using a serverless mutating webhook instead.","title":"Use an external secrets provider"},{"location":"detective/","text":"Auditing and logging \u00b6 Collecting and analyzing [audit] logs is useful for a variety of different reasons. Logs can help with root cause analysis and attribution, i.e. ascribing a change to a particular user. When enough logs have been collected, they can be used to detect anomalous behaviors too. On EKS, the audit logs are sent to Amazon Cloudwatch Logs. The audit policy for EKS currently augments the reference policy in the helper script with the following policy: - level : RequestResponse namespaces : [ \"kube-system\" ] verbs : [ \"update\" , \"patch\" , \"delete\" ] resources : - group : \"\" # core resources : [ \"configmaps\" ] resourceNames : [ \"aws-auth\" ] omitStages : - \"RequestReceived\" This logs changes to the aws-auth ConfigMap which is used to grant access to an EKS cluster. Recommendations \u00b6 Enable audit logs \u00b6 The audit logs are part of the EKS managed Kubernetes control plane logs that are managed by EKS. Instructions for enabling/disabling the control plane logs, which includes the logs for the Kubernetes API server, the controller manager, and the scheduler, along with the audit log, can be found here, https://docs.aws.amazon.com/eks/latest/userguide/control-plane-logs.html#enabling-control-plane-log-export . Info When you enable control plane logging, you will incur costs for storing the logs in CloudWatch. This raises a broader issue about the ongoing cost of security. Ultimately you will have to weigh those costs against the cost of a security breach, e.g. financial loss, damage to your reputation, etc. You may find that you can adequately secure your environment by implementing only some of the recommendations in this guide. Warning The maximum size for a CWL entry is 256KB whereas the maximum Kubernetes API request size is 1.5MiB. Utilize audit metadata \u00b6 Kubernetes audit logs include two annotations that indicate whether or not a request was authorized authorization.k8s.io/decision and the reason for the decision authorization.k8s.io/reason . Use these attributes to ascertain why a particular API call was allowed. Create alarms for suspicous events \u00b6 Create an alarm to automatically alert you where there is an increase in 403 Forbidden and 401 Unauthorized responses, and then use attributes like host , sourceIPs , and k8s_user.username to find out where those requests are coming from. Analyze logs with Log Insights \u00b6 Use CloudWatch Log Insights to monitor changes to RBAC objects, e.g. Roles, RoleBindings, ClusterRoles, and ClusterRoleBindings. A few sample queries appear below: Lists create, update, delete operations to Roles: fields @timestamp, @message | sort @timestamp desc | limit 100 | filter objectRef.resource=\"roles\" and verb in [\"create\", \"update\", \"patch\", \"delete\"] Lists create, update, delete operations to RoleBindings: fields @timestamp, @message | sort @timestamp desc | limit 100 | filter objectRef.resource=\"rolebindings\" and verb in [\"create\", \"update\", \"patch\", \"delete\"] Lists create, update, delete operations to ClusterRoles: fields @timestamp, @message | sort @timestamp desc | limit 100 | filter objectRef.resource=\"clusterroles\" and verb in [\"create\", \"update\", \"patch\", \"delete\"] Lists create, update, delete operations to ClusterRoleBindings: fields @timestamp, @message | sort @timestamp desc | limit 100 | filter objectRef.resource=\"clusterrolebindings\" and verb in [\"create\", \"update\", \"patch\", \"delete\"] Plots unauthorized read operations against Secrets: fields @timestamp, @message | sort @timestamp desc | limit 100 | filter objectRef.resource=\"secrets\" and verb in [\"get\", \"watch\", \"list\"] and responseStatus.code=\"401\" | count() by bin(1m) List of failed anonymous requests: fields @timestamp, @message, sourceIPs.0 | sort @timestamp desc | limit 100 | filter user.username=\"system:anonymous\" and responseStatus.code in [\"401\", \"403\"] Audit your CloudTrail logs \u00b6 AWS APIs called by pods that are utilizing IAM Roles for Service Accounts (IRSA) are automatically logged to CloudTrail along with the name of the service account. If the name of a service account that wasn't explicitly authorized to call an API appears in the log, it may be an indication that the IAM role's trust policy was misconfigured. Generally speaking, Cloudtrail is a great way to ascribe AWS API calls to specific IAM principals. Additional resources \u00b6 As the volume of logs increases, parsing and filtering them with Log Insights or another log analysis tool may become ineffective. As an alternative, you might want to consider running Sysdig Falco and ekscloudwatch . Falco analyzes audit logs and flags anomalies or abuse over an extended period of time. The ekscloudwatch project forwards audit log events from CloudWatch to Falco for analysis. Falco provides a set of default audit rules along with the ability to add your own. Yet another option might be to store the audit logs in S3 and use the SageMaker Random Cut Forest algorithm to anomalous behaviors that warrant further investigation. Tooling \u00b6 The following open source projects can be used to assess your cluster's alignment with established best practices: kubeaudit MKIT kube-scan Assigns a risk score to the workloads running in your cluster in accordance with the Kubernetes Common Configuration Scoring System framework amicontained Reveals which Capabilities are allowed and syscalls that are blocked by the container runtime kubesec.io polaris Starboard kAudit","title":"Detective Controls"},{"location":"detective/#auditing-and-logging","text":"Collecting and analyzing [audit] logs is useful for a variety of different reasons. Logs can help with root cause analysis and attribution, i.e. ascribing a change to a particular user. When enough logs have been collected, they can be used to detect anomalous behaviors too. On EKS, the audit logs are sent to Amazon Cloudwatch Logs. The audit policy for EKS currently augments the reference policy in the helper script with the following policy: - level : RequestResponse namespaces : [ \"kube-system\" ] verbs : [ \"update\" , \"patch\" , \"delete\" ] resources : - group : \"\" # core resources : [ \"configmaps\" ] resourceNames : [ \"aws-auth\" ] omitStages : - \"RequestReceived\" This logs changes to the aws-auth ConfigMap which is used to grant access to an EKS cluster.","title":"Auditing and logging"},{"location":"detective/#recommendations","text":"","title":"Recommendations"},{"location":"detective/#enable-audit-logs","text":"The audit logs are part of the EKS managed Kubernetes control plane logs that are managed by EKS. Instructions for enabling/disabling the control plane logs, which includes the logs for the Kubernetes API server, the controller manager, and the scheduler, along with the audit log, can be found here, https://docs.aws.amazon.com/eks/latest/userguide/control-plane-logs.html#enabling-control-plane-log-export . Info When you enable control plane logging, you will incur costs for storing the logs in CloudWatch. This raises a broader issue about the ongoing cost of security. Ultimately you will have to weigh those costs against the cost of a security breach, e.g. financial loss, damage to your reputation, etc. You may find that you can adequately secure your environment by implementing only some of the recommendations in this guide. Warning The maximum size for a CWL entry is 256KB whereas the maximum Kubernetes API request size is 1.5MiB.","title":"Enable audit logs"},{"location":"detective/#utilize-audit-metadata","text":"Kubernetes audit logs include two annotations that indicate whether or not a request was authorized authorization.k8s.io/decision and the reason for the decision authorization.k8s.io/reason . Use these attributes to ascertain why a particular API call was allowed.","title":"Utilize audit metadata"},{"location":"detective/#create-alarms-for-suspicous-events","text":"Create an alarm to automatically alert you where there is an increase in 403 Forbidden and 401 Unauthorized responses, and then use attributes like host , sourceIPs , and k8s_user.username to find out where those requests are coming from.","title":"Create alarms for suspicous events"},{"location":"detective/#analyze-logs-with-log-insights","text":"Use CloudWatch Log Insights to monitor changes to RBAC objects, e.g. Roles, RoleBindings, ClusterRoles, and ClusterRoleBindings. A few sample queries appear below: Lists create, update, delete operations to Roles: fields @timestamp, @message | sort @timestamp desc | limit 100 | filter objectRef.resource=\"roles\" and verb in [\"create\", \"update\", \"patch\", \"delete\"] Lists create, update, delete operations to RoleBindings: fields @timestamp, @message | sort @timestamp desc | limit 100 | filter objectRef.resource=\"rolebindings\" and verb in [\"create\", \"update\", \"patch\", \"delete\"] Lists create, update, delete operations to ClusterRoles: fields @timestamp, @message | sort @timestamp desc | limit 100 | filter objectRef.resource=\"clusterroles\" and verb in [\"create\", \"update\", \"patch\", \"delete\"] Lists create, update, delete operations to ClusterRoleBindings: fields @timestamp, @message | sort @timestamp desc | limit 100 | filter objectRef.resource=\"clusterrolebindings\" and verb in [\"create\", \"update\", \"patch\", \"delete\"] Plots unauthorized read operations against Secrets: fields @timestamp, @message | sort @timestamp desc | limit 100 | filter objectRef.resource=\"secrets\" and verb in [\"get\", \"watch\", \"list\"] and responseStatus.code=\"401\" | count() by bin(1m) List of failed anonymous requests: fields @timestamp, @message, sourceIPs.0 | sort @timestamp desc | limit 100 | filter user.username=\"system:anonymous\" and responseStatus.code in [\"401\", \"403\"]","title":"Analyze logs with Log Insights"},{"location":"detective/#audit-your-cloudtrail-logs","text":"AWS APIs called by pods that are utilizing IAM Roles for Service Accounts (IRSA) are automatically logged to CloudTrail along with the name of the service account. If the name of a service account that wasn't explicitly authorized to call an API appears in the log, it may be an indication that the IAM role's trust policy was misconfigured. Generally speaking, Cloudtrail is a great way to ascribe AWS API calls to specific IAM principals.","title":"Audit your CloudTrail logs"},{"location":"detective/#additional-resources","text":"As the volume of logs increases, parsing and filtering them with Log Insights or another log analysis tool may become ineffective. As an alternative, you might want to consider running Sysdig Falco and ekscloudwatch . Falco analyzes audit logs and flags anomalies or abuse over an extended period of time. The ekscloudwatch project forwards audit log events from CloudWatch to Falco for analysis. Falco provides a set of default audit rules along with the ability to add your own. Yet another option might be to store the audit logs in S3 and use the SageMaker Random Cut Forest algorithm to anomalous behaviors that warrant further investigation.","title":"Additional resources"},{"location":"detective/#tooling","text":"The following open source projects can be used to assess your cluster's alignment with established best practices: kubeaudit MKIT kube-scan Assigns a risk score to the workloads running in your cluster in accordance with the Kubernetes Common Configuration Scoring System framework amicontained Reveals which Capabilities are allowed and syscalls that are blocked by the container runtime kubesec.io polaris Starboard kAudit","title":"Tooling"},{"location":"hosts/","text":"Protecting the infrastructure (hosts) \u00b6 Inasmuch as it's important to secure your container images, it's equally important to safeguard the infrastructure that runs them. This section explores different ways to mitigate risks from attacks launched directly against the host. These guidelines should be used in conjunction with those outlined in the Runtime Security section. Recommendations \u00b6 Use an OS optimized for running containers \u00b6 Consider using Flatcar Linux, Project Atomic, RancherOS, and Bottlerocket , a special purpose OS from AWS designed for running Linux containers. It includes a reduced attack surface, a disk image that is verified on boot, and enforced permission boundaries using SELinux. Treat your infrastructure as immutable and automate the replacement of your worker nodes \u00b6 Rather than performing in-place upgrades, replace your workers when a new patch or update becomes available. This can be approached a couple of ways. You can either add instances to an existing autoscaling group using the latest AMI as you sequentially cordon and drain nodes until all of the nodes in the group have been replaced with the latest AMI. Alternatively, you can add instances to a new node group while you sequentially cordon and drain nodes from the old node group until all of the nodes have been replaced. EKS managed node groups uses the second approach and will present an option to upgrade workers when a new AMI becomes available. eksctl also has a mechanism for creating node groups with the latest AMI and for gracefully cordoning and draining pods from nodes groups before the instances are terminated. If you decide to use a different method for replacing your worker nodes, it is strongly recommended that you automate the process to minimize human oversight as you will likely need to replace workers regularly as new updates/patches are released and when the control plane is upgraded. With EKS Fargate, AWS will automatically update the underlying infrastructure as updates become available. Oftentimes this can be done seamlessly, but there may be times when an update will cause your pod to be rescheduled. Hence, we recommend that you create deployments with multiple replicas when running your application as a Fargate pod. Periodically run kube-bench to verify compliance with CIS benchmarks for Kubernetes \u00b6 When running kube-bench against an EKS cluster, follow these instructions from Aqua Security, https://github.com/aquasecurity/kube-bench#running-in-an-eks-cluster . Caution False positives may appear in the report because of the way the EKS optimized AMI configures the kubelet. The issue is currently being tracked on GitHub . Minimize access to worker nodes \u00b6 Instead of enabling SSH access, use SSM Session Manager when you need to remote into a host. Unlike SSH keys which can be lost, copied, or shared, Session Manager allows you to control access to EC2 instances using IAM. Moreover, it provides an audit trail and log of the commands that were run on the instance. As of August 19th, 2020 Managed Node Groups support custom AMIs and EC2 Launch Templates. This allows you to embed the SSM agent into the AMI or install it as the worker node is being bootstrapped. Deploy workers onto private subnets \u00b6 By deploying workers onto private subnets, you minimize their exposure to the Internet where attacks often originate. Beginning April 22, 2020, the assignment of public IP addresses to nodes in a managed node groups will be controlled by the subnet they are deployed onto. Prior to this, nodes in a Managed Node Group were automatically assigned a public IP. If you choose to deploy your worker nodes on to public subnets, implement restrictive AWS security group rules to limit their exposure. Run Amazon Inspector to assess hosts for exposure, vulnerabilities, and deviations from best practices \u00b6 Inspector requires the deployment of an agent that continually monitors activity on the instance while using a set of rules to assess alignment with best practices. Attention Inspector cannot be run on the infrastructure used to run Fargate pods. Alternatives \u00b6 Run SELinux \u00b6 Info Available on Red Hat Enterprise Linux (RHEL), CentOS, and CoreOS SELinux provides an additional layer of security to keep containers isolated from each other and from the host. SELinux allows administrators to enforce mandatory access controls (MAC) for every user, application, process, and file. Think of it as a backstop that restricts the operations that can be performed against to specific resources based on a set of labels. On EKS, SELinux can be used to prevent containers from accessing each other's resources. Container SELinux policies are defined in the container-selinux package. Docker CE requires this package (along with its dependencies) so that the processes and files created by Docker (or other container runtimes) run with limited system access. Containers leverage the container_t label which is an alias to svirt_lxc_net_t . These policies effectively prevent containers from accessing certain features of the host. When you configure SELinux for Docker, Docker automatically labels workloads container_t as a type and gives each container a unique MCS level. This will isolate containers from one another. If you need looser restrictions, you can create your own profile in SElinux which grants a container permissions to specific areas of the file system. This is similiar to PSPs in that you can create different profiles for different containers/pods. For example, you can have a profile for general workloads with a set of restrictive controls and another for things that require privileged access. SELinux for Containers has a set of options that can be configured to modify the default restrictions. The following SELinux Booleans can be enabled or disabled based on your needs: Boolean Default Description container_connect_any off Allow containers to access privileged ports on the host. For example, if you have a container that needs to map ports to 443 or 80 on the host. container_manage_cgroup off Allow containers to manage cgroup configuration. For example, a container running systemd will need this to be enabled. container_use_cephfs off Allow containers to use a ceph file system. By default, containers are allowed to read/execute under /usr and read most content from /etc . The files under /var/lib/docker and /var/lib/containers have the label container_var_lib_t . To view a full list of default, labels see the container.fc file. docker container run -it \\ -v /var/lib/docker/image/overlay2/repositories.json:/host/repositories.json \\ centos:7 cat /host/repositories.json # cat: /host/repositories.json: Permission denied docker container run -it \\ -v /etc/passwd:/host/etc/passwd \\ centos:7 cat /host/etc/passwd # cat: /host/etc/passwd: Permission denied Files labeled with container_file_t are the only files that are writable by containers. If you want a volume mount to be writeable, you will needed to specify :z or :Z at the end. :z will re-label the files so that the container can read/write :Z will re-label the files so that only the container can read/write ls -Z /var/lib/misc # -rw-r--r--. root root system_u:object_r:var_lib_t:s0 postfix.aliasesdb-stamp docker container run -it \\ -v /var/lib/misc:/host/var/lib/misc:z \\ centos:7 echo \"Relabeled!\" ls -Z /var/lib/misc #-rw-r--r--. root root system_u:object_r:container_file_t:s0 postfix.aliasesdb-stamp docker container run -it \\ -v /var/log:/host/var/log:Z \\ fluentbit:latest In Kubernetes, relabeling is slightly different. Rather than having Docker automatically relabel the files, you can specify a custom MCS label to run the pod. Volumes that support relabeling will automatically be relabeled so that they are accessible. Pods with a matching MCS label will be able to access the volume. If you need strict isolation, set a different MCS label for each pod. securityContext : seLinuxOptions : # Provide a unique MCS label per container # You can specify user, role, and type also # enforcement based on type and level (svert) level : s0:c144:c154 In this example s0:c144:c154 corresponds to an MCS label assigned to a file that the container is allowed to access. On EKS you could create policies that allow for privileged containers to run, like FluentD and create an SELinux policy to allow it to read from /var/log on the host without needing to relabel the host directory. Pods with the same label will be able to access the same host volumes. We have implemented sample AMIs for Amazon EKS that have SELinux configured on CentOS 7 and RHEL 7. These AMIs were developed to demonstrate sample implementations that meet requirements of highly regulated customers, such as STIG, CJIS, and C2S. Caution SELinux will ignore containers where the type is unconfined. Additional resources \u00b6 SELinux Kubernetes RBAC and Shipping Security Policies for On-prem Applications Iterative Hardening of Kubernetes Audit2Allow SEAlert Generate SELinux policies for containers with Udica describes a tool that looks at container spec files for Linux capabilities, ports, and mount points, and generates a set of SELinux rules that allow the container to run properly AMI Hardening playbooks for hardening the OS to meet different regulatory requirements Tools \u00b6 Keiko Sysdig Secure eksctl","title":"Infrastructure Security"},{"location":"hosts/#protecting-the-infrastructure-hosts","text":"Inasmuch as it's important to secure your container images, it's equally important to safeguard the infrastructure that runs them. This section explores different ways to mitigate risks from attacks launched directly against the host. These guidelines should be used in conjunction with those outlined in the Runtime Security section.","title":"Protecting the infrastructure (hosts)"},{"location":"hosts/#recommendations","text":"","title":"Recommendations"},{"location":"hosts/#use-an-os-optimized-for-running-containers","text":"Consider using Flatcar Linux, Project Atomic, RancherOS, and Bottlerocket , a special purpose OS from AWS designed for running Linux containers. It includes a reduced attack surface, a disk image that is verified on boot, and enforced permission boundaries using SELinux.","title":"Use an OS optimized for running containers"},{"location":"hosts/#treat-your-infrastructure-as-immutable-and-automate-the-replacement-of-your-worker-nodes","text":"Rather than performing in-place upgrades, replace your workers when a new patch or update becomes available. This can be approached a couple of ways. You can either add instances to an existing autoscaling group using the latest AMI as you sequentially cordon and drain nodes until all of the nodes in the group have been replaced with the latest AMI. Alternatively, you can add instances to a new node group while you sequentially cordon and drain nodes from the old node group until all of the nodes have been replaced. EKS managed node groups uses the second approach and will present an option to upgrade workers when a new AMI becomes available. eksctl also has a mechanism for creating node groups with the latest AMI and for gracefully cordoning and draining pods from nodes groups before the instances are terminated. If you decide to use a different method for replacing your worker nodes, it is strongly recommended that you automate the process to minimize human oversight as you will likely need to replace workers regularly as new updates/patches are released and when the control plane is upgraded. With EKS Fargate, AWS will automatically update the underlying infrastructure as updates become available. Oftentimes this can be done seamlessly, but there may be times when an update will cause your pod to be rescheduled. Hence, we recommend that you create deployments with multiple replicas when running your application as a Fargate pod.","title":"Treat your infrastructure as immutable and automate the replacement of your worker nodes"},{"location":"hosts/#periodically-run-kube-bench-to-verify-compliance-with-cis-benchmarks-for-kubernetes","text":"When running kube-bench against an EKS cluster, follow these instructions from Aqua Security, https://github.com/aquasecurity/kube-bench#running-in-an-eks-cluster . Caution False positives may appear in the report because of the way the EKS optimized AMI configures the kubelet. The issue is currently being tracked on GitHub .","title":"Periodically run kube-bench to verify compliance with CIS benchmarks for Kubernetes"},{"location":"hosts/#minimize-access-to-worker-nodes","text":"Instead of enabling SSH access, use SSM Session Manager when you need to remote into a host. Unlike SSH keys which can be lost, copied, or shared, Session Manager allows you to control access to EC2 instances using IAM. Moreover, it provides an audit trail and log of the commands that were run on the instance. As of August 19th, 2020 Managed Node Groups support custom AMIs and EC2 Launch Templates. This allows you to embed the SSM agent into the AMI or install it as the worker node is being bootstrapped.","title":"Minimize access to worker nodes"},{"location":"hosts/#deploy-workers-onto-private-subnets","text":"By deploying workers onto private subnets, you minimize their exposure to the Internet where attacks often originate. Beginning April 22, 2020, the assignment of public IP addresses to nodes in a managed node groups will be controlled by the subnet they are deployed onto. Prior to this, nodes in a Managed Node Group were automatically assigned a public IP. If you choose to deploy your worker nodes on to public subnets, implement restrictive AWS security group rules to limit their exposure.","title":"Deploy workers onto private subnets"},{"location":"hosts/#run-amazon-inspector-to-assess-hosts-for-exposure-vulnerabilities-and-deviations-from-best-practices","text":"Inspector requires the deployment of an agent that continually monitors activity on the instance while using a set of rules to assess alignment with best practices. Attention Inspector cannot be run on the infrastructure used to run Fargate pods.","title":"Run Amazon Inspector to assess hosts for exposure, vulnerabilities, and deviations from best practices"},{"location":"hosts/#alternatives","text":"","title":"Alternatives"},{"location":"hosts/#run-selinux","text":"Info Available on Red Hat Enterprise Linux (RHEL), CentOS, and CoreOS SELinux provides an additional layer of security to keep containers isolated from each other and from the host. SELinux allows administrators to enforce mandatory access controls (MAC) for every user, application, process, and file. Think of it as a backstop that restricts the operations that can be performed against to specific resources based on a set of labels. On EKS, SELinux can be used to prevent containers from accessing each other's resources. Container SELinux policies are defined in the container-selinux package. Docker CE requires this package (along with its dependencies) so that the processes and files created by Docker (or other container runtimes) run with limited system access. Containers leverage the container_t label which is an alias to svirt_lxc_net_t . These policies effectively prevent containers from accessing certain features of the host. When you configure SELinux for Docker, Docker automatically labels workloads container_t as a type and gives each container a unique MCS level. This will isolate containers from one another. If you need looser restrictions, you can create your own profile in SElinux which grants a container permissions to specific areas of the file system. This is similiar to PSPs in that you can create different profiles for different containers/pods. For example, you can have a profile for general workloads with a set of restrictive controls and another for things that require privileged access. SELinux for Containers has a set of options that can be configured to modify the default restrictions. The following SELinux Booleans can be enabled or disabled based on your needs: Boolean Default Description container_connect_any off Allow containers to access privileged ports on the host. For example, if you have a container that needs to map ports to 443 or 80 on the host. container_manage_cgroup off Allow containers to manage cgroup configuration. For example, a container running systemd will need this to be enabled. container_use_cephfs off Allow containers to use a ceph file system. By default, containers are allowed to read/execute under /usr and read most content from /etc . The files under /var/lib/docker and /var/lib/containers have the label container_var_lib_t . To view a full list of default, labels see the container.fc file. docker container run -it \\ -v /var/lib/docker/image/overlay2/repositories.json:/host/repositories.json \\ centos:7 cat /host/repositories.json # cat: /host/repositories.json: Permission denied docker container run -it \\ -v /etc/passwd:/host/etc/passwd \\ centos:7 cat /host/etc/passwd # cat: /host/etc/passwd: Permission denied Files labeled with container_file_t are the only files that are writable by containers. If you want a volume mount to be writeable, you will needed to specify :z or :Z at the end. :z will re-label the files so that the container can read/write :Z will re-label the files so that only the container can read/write ls -Z /var/lib/misc # -rw-r--r--. root root system_u:object_r:var_lib_t:s0 postfix.aliasesdb-stamp docker container run -it \\ -v /var/lib/misc:/host/var/lib/misc:z \\ centos:7 echo \"Relabeled!\" ls -Z /var/lib/misc #-rw-r--r--. root root system_u:object_r:container_file_t:s0 postfix.aliasesdb-stamp docker container run -it \\ -v /var/log:/host/var/log:Z \\ fluentbit:latest In Kubernetes, relabeling is slightly different. Rather than having Docker automatically relabel the files, you can specify a custom MCS label to run the pod. Volumes that support relabeling will automatically be relabeled so that they are accessible. Pods with a matching MCS label will be able to access the volume. If you need strict isolation, set a different MCS label for each pod. securityContext : seLinuxOptions : # Provide a unique MCS label per container # You can specify user, role, and type also # enforcement based on type and level (svert) level : s0:c144:c154 In this example s0:c144:c154 corresponds to an MCS label assigned to a file that the container is allowed to access. On EKS you could create policies that allow for privileged containers to run, like FluentD and create an SELinux policy to allow it to read from /var/log on the host without needing to relabel the host directory. Pods with the same label will be able to access the same host volumes. We have implemented sample AMIs for Amazon EKS that have SELinux configured on CentOS 7 and RHEL 7. These AMIs were developed to demonstrate sample implementations that meet requirements of highly regulated customers, such as STIG, CJIS, and C2S. Caution SELinux will ignore containers where the type is unconfined.","title":"Run SELinux"},{"location":"hosts/#additional-resources","text":"SELinux Kubernetes RBAC and Shipping Security Policies for On-prem Applications Iterative Hardening of Kubernetes Audit2Allow SEAlert Generate SELinux policies for containers with Udica describes a tool that looks at container spec files for Linux capabilities, ports, and mount points, and generates a set of SELinux rules that allow the container to run properly AMI Hardening playbooks for hardening the OS to meet different regulatory requirements","title":"Additional resources"},{"location":"hosts/#tools","text":"Keiko Sysdig Secure eksctl","title":"Tools"},{"location":"iam/","text":"Identity and Access Management \u00b6 Identity and Access Management (IAM) is an AWS service that performs two essential functions: Authentication and Authorization. Authentication involves the verification of a identity whereas authorization governs the actions that can be performed by AWS resources. Within AWS, a resource can be another AWS service, e.g. EC2, or an AWS principle such as an IAM User or Role . The rules governing the actions that a resource is allowed to perform are expressed as IAM policies . Controlling Access to EKS Clusters \u00b6 The Kubernetes project supports a variety of different strategies to authenticate requests to the kube-apiserver service, e.g. Bearer Tokens, X.509 certificates, OIDC, etc. EKS currently has native support for webhook token authentication and service account tokens . The webhook authentication strategy calls a webhook that verifies bearer tokens. On EKS, these bearer tokens are generated by the AWS CLI or the aws-iam-authenticator client when you run kubectl commands. As you execute commands, the token is passed to the kube-apiserver which forwards it to the authentication webhook. If the request is well-formed, the webhook calls a pre-signed URL embedded in the token's body. This URL validates the request's signature and returns information about the user, e.g. the user's account, Arn, and UserId to the kube-apiserver. To manually generate a authentication token, type the following command in a terminal window: aws eks get-token --cluster <cluster_name> The output should resemble this: { \"kind\" : \"ExecCredential\" , \"apiVersion\" : \"client.authentication.k8s.io/v1alpha1\" , \"spec\" : {}, \"status\" : { \"expirationTimestamp\" : \"2020-02-19T16:08:27Z\" , \"token\" : \"k8s-aws-v1.aHR0cHM6Ly9zdHMuYW1hem9uYXdzLmNvbS8_QWN0aW9uPUdldENhbGxlcklkZW50aXR5JlZlcnNpb249MjAxMS0wNi0xNSZYLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFKTkdSSUxLTlNSQzJXNVFBJTJGMjAyMDAyMTklMkZ1cy1lYXN0LTElMkZzdHMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDIwMDIxOVQxNTU0MjdaJlgtQW16LUV4cGlyZXM9NjAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JTNCeC1rOHMtYXdzLWlkJlgtQW16LVNpZ25hdHVyZT0yMjBmOGYzNTg1ZTMyMGRkYjVlNjgzYTVjOWE0MDUzMDFhZDc2NTQ2ZjI0ZjI4MTExZmRhZDA5Y2Y2NDhhMzkz\" } } Each token starts with k8s-aws-v1. followed by a base64 encoded string. The string, when decoded, should resemble this: https://sts.amazonaws.com/?Action = GetCallerIdentity & Version = 2011 -06-15 & X-Amz-Algorithm = AWS4-HMAC-SHA256 & X-Amz-Credential = AKIAJPFRILKNSRC2W5QA%2F20200219%2Fus-east-1%2Fsts%2Faws4_request & X-Amz-Date = 20200219T155427Z & X-Amz-Expires = 60 & X-Amz-SignedHeaders = host%3Bx-k8s-aws-id & X-Amz-Signature = 220f8f3285e320ddb5e683a5c9a405301ad76546f24f28111fdad09cf648a393 The token consists of a pre-signed URL that includes an Amazon credential and signature. For additional details see https://docs.aws.amazon.com/STS/latest/APIReference/API_GetCallerIdentity.html . The token has a time to live (TTL) of 15 minutes after which a new token will need to be generated. This is handled automatically when you use a client like kubectl , however, if you're using the Kubernetes dashboard, you will need to generate a new token and re-authenticate each time the token expires. Once the user's identity has been authenticated by the AWS IAM service, the kube-apiserver reads the aws-auth ConfigMap in the kube-system Namespace to determine the RBAC group to associate with the user. The aws-auth ConfigMap is used to create a static mapping between IAM principles, i.e. IAM Users and Roles, and Kubernetes RBAC groups. RBAC groups can be referenced in Kubernetes RoleBindings or ClusterRoleBindings. They are similar to IAM Roles in that they define a set of actions (verbs) that can be performed against a collection of Kubernetes resources (objects). Recommendations \u00b6 Don't use a service account token for authentication \u00b6 A service account token is a long-lived, static credential. If it is compromised, lost, or stolen, an attacker may be able to perform all the actions associated with that token until the service account is deleted. At times, you may need to grant an exception for applications that have to consume the Kubernetes API from outside the cluster, e.g. a CI/CD pipeline application. If such applications run on AWS infrastructure, like EC2 instances, consider using an instance profile and mapping that to a Kubernetes RBAC role in the aws-auth ConfigMap instead. Employ least privileged access to AWS Resources \u00b6 An IAM User does not need to be assigned privileges to AWS resources to access the Kubernetes API. If you need to grant an IAM user access to an EKS cluster, create an entry in the aws-auth ConfigMap for that user that maps to a specific Kubernetes RBAC group. Use IAM Roles when multiple users need identical access to the cluster \u00b6 Rather than creating an entry for each individual IAM User in the aws-auth ConfigMap, allow those users to assume an IAM Role and map that role to a Kubernetes RBAC group. This will be easier to maintain, especially as the number of users that require access grows. Employ least privileged access when creating RoleBindings and ClusterRoleBindings \u00b6 Like the earlier point about granting access to AWS Resources, RoleBindings and ClusterRoleBindings should only include the set of permissions necessary to perform a specific function. Avoid using [\"*\"] in your Roles and ClusterRoles unless it's absolutely necessary. If you're unsure what permissions to assign, consider using a tool like audit2rbac to automatically generate Roles and binding based on the observed API calls in the Kubernetes Audit Log. Make the EKS Cluster Endpoint private \u00b6 By default when you provision an EKS cluster, the API cluster endpoint is set to public, i.e. it can be accessed from the Internet. Despite being accessible from the Internet, the endpoint is still considered secure because it requires all API requests to be authenticated by IAM and then authorized by Kubernetes RBAC. That said, if your corporate security policy mandates that you restrict access to the API from the Internet or prevents you from routing traffic outside the cluster VPC, you can: Configure the EKS cluster endpoint to be private. See Modifying Cluster Endpoint Access for further information on this topic. Leave the cluster endpoint public and specify which CIDR blocks can communicate with the cluster endpoint. The blocks are effectively a whitelisted set of public IP addresses that are allowed to access the cluster endpoint. Configure public access with a set of whitelisted CIDR blocks and set private endpoint access to enabled. This will allow public access from a specific range of public IPs while forcing all network traffic between the kubelets (workers) and the Kubernetes API through the cross-account ENIs that get provisioned into the cluster VPC when the control plane is provisioned. Create the cluster with a dedicated IAM role \u00b6 When you create an Amazon EKS cluster, the IAM entity user or role, such as a federated user that creates the cluster, is automatically granted system:masters permissions in the cluster's RBAC configuration. This access cannot be removed and is not managed through the aws-auth ConfigMap. Therefore it is a good idea to create the cluster with a dedicated IAM role and regularly audit who can assume this role. This role should not be used to perform routine actions on the cluster, and instead additional users should be granted access to the cluster through the aws-auth ConfigMap for this purpose. After the aws-auth ConfigMap is configured, the role can be deleted and only recreated in an emergency / break glass scenario where the aws-auth ConfigMap is corrupted and the cluster is otherwise inaccessible. This can be particularly useful in production clusters which do not usually have direct user access configured. Regularly audit access to the cluster \u00b6 Who requires access is likely to change over time. Plan to periodically audit the aws-auth ConfigMap to see who has been granted access and the rights they've been assigned. You can also use open source tooling like kubectl-who-can , or rbac-lookup to examine the roles bound to a particular service account, user, or group. We'll explore this topic further when we get to the section on auditing . Additional ideas can be found in this article from NCC Group. Alternative Approaches to Authentication and Access Management \u00b6 While IAM is the preferred way to authenticate users who need access to an EKS cluster, it is possible to use an OIDC identity provider such as GitHub using an authentication proxy and Kubernetes impersonation . Posts for two such solutions have been published on the AWS Open Source blog: Authenticating to EKS Using GitHub Credentials with Teleport Consistent OIDC authentication across multiple EKS clusters using kube-oidc-proxy You can also use AWS SSO to federate AWS with an external identity provider, e.g. Azure AD. If you decide to use this, the AWS CLI v2.0 includes an option to create a named profile that makes it easy to associate an SSO session with your current CLI session and assume an IAM role. Know that you must assume a role prior to running kubectl as the IAM role is used to determine the user's Kubernetes RBAC group. Additional Resources \u00b6 rbac.dev A list of additional resources, including blogs and tools, for Kubernetes RBAC Pods Identities \u00b6 Certain applications that run within a Kubernetes cluster need permission to call the Kubernetes API to function properly. For example, the ALB Ingress Controller needs to be able to list a Service's Endpoints. The controller also needs to be able to invoke AWS APIs to provision and configure an ALB. In this section we will explore the best practices for assigning rights and privileges to Pods. Kubernetes Service Accounts \u00b6 A service account is a special type of object that allows you to assign a Kubernetes RBAC role to a pod. A default service account is created automatically for each Namespace within a cluster. When you deploy a pod into a Namespace without referencing a specific service account, the default service account for that Namespace will automatically get assigned to the Pod and the Secret, i.e. the service account (JWT) token for that service account, will get mounted to the pod as a volume at /var/run/secrets/kubernetes.io/serviceaccount . Decoding the service account token in that directory will reveal the following metadata: { \"iss\" : \"kubernetes/serviceaccount\" , \"kubernetes.io/serviceaccount/namespace\" : \"default\" , \"kubernetes.io/serviceaccount/secret.name\" : \"default-token-5pv4z\" , \"kubernetes.io/serviceaccount/service-account.name\" : \"default\" , \"kubernetes.io/serviceaccount/service-account.uid\" : \"3b36ddb5-438c-11ea-9438-063a49b60fba\" , \"sub\" : \"system:serviceaccount:default:default\" } The default service account has the following permissions to the Kubernetes API. apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : annotations : rbac.authorization.kubernetes.io/autoupdate : \"true\" creationTimestamp : \"2020-01-30T18:13:25Z\" labels : kubernetes.io/bootstrapping : rbac-defaults name : system:discovery resourceVersion : \"43\" selfLink : /apis/rbac.authorization.k8s.io/v1/clusterroles/system%3Adiscovery uid : 350d2ab8-438c-11ea-9438-063a49b60fba rules : - nonResourceURLs : - /api - /api/* - /apis - /apis/* - /healthz - /openapi - /openapi/* - /version - /version/ verbs : - get This role authorizes unauthenticated and authenticated users to read API information and is deemed safe to be publicly accessible. When an application running within a Pod calls the Kubernetes APIs, the Pod needs to be assigned a service account that explicitly grants it permission to call those APIs. Similar to guidelines for user access, the Role or ClusterRole bound to a service account should be restricted to the API resources and methods that the application needs to function and nothing else. To use a non-default service account simply set the spec.serviceAccountName field of a Pod to the name of the service account you wish to use. For additional information about creating service accounts, see https://kubernetes.io/docs/reference/access-authn-authz/rbac/#service-account-permissions . IAM Roles for Service Accounts (IRSA) \u00b6 IRSA is a feature that allows you to assign an IAM role to a Kubernetes service account. It works by leveraging a Kubernetes feature known as Service Account Token Volume Projection . Pods with service accounts that reference an IAM Role call a public OIDC discovery endpoint for AWS IAM upon startup. The endpoint cryptographically signs the OIDC token issued by Kubernetes which ultimately allows the Pod to call the AWS APIs associated IAM role. When an AWS API is invoked, the AWS SDKs calls sts:AssumeRoleWithWebIdentity and automatically exchanges the Kubernetes issued token for a AWS role credential. Decoding the (JWT) token for IRSA will produce output similar to the example you see below: { \"aud\" : [ \"sts.amazonaws.com\" ], \"exp\" : 1582306514 , \"iat\" : 1582220114 , \"iss\" : \"https://oidc.eks.us-west-2.amazonaws.com/id/D43CF17C27A865933144EA99A26FB128\" , \"kubernetes.io\" : { \"namespace\" : \"default\" , \"pod\" : { \"name\" : \"alpine-57b5664646-rf966\" , \"uid\" : \"5a20f883-5407-11ea-a85c-0e62b7a4a436\" }, \"serviceaccount\" : { \"name\" : \"s3-read-only\" , \"uid\" : \"a720ba5c-5406-11ea-9438-063a49b60fba\" } }, \"nbf\" : 1582220114 , \"sub\" : \"system:serviceaccount:default:s3-read-only\" } This particular token grants the Pod view-only privileges to S3. When the application attempts to read from S3, the token is exchanged for a temporary set of IAM credentials that resembles this: { \"AssumedRoleUser\" : { \"AssumedRoleId\" : \"AROA36C6WWEJULFUYMPB6:abc\" , \"Arn\" : \"arn:aws:sts::123456789012:assumed-role/eksctl-winterfell-addon-iamserviceaccount-de-Role1-1D61LT75JH3MB/abc\" }, \"Audience\" : \"sts.amazonaws.com\" , \"Provider\" : \"arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-west-2.amazonaws.com/id/D43CF17C27A865933144EA99A26FB128\" , \"SubjectFromWebIdentityToken\" : \"system:serviceaccount:default:s3-read-only\" , \"Credentials\" : { \"SecretAccessKey\" : \"ORJ+8Adk+wW+nU8FETq7+mOqeA8Z6jlPihnV8hX1\" , \"SessionToken\" : \"FwoGZXIvYXdzEGMaDMLxAZkuLpmSwYXShiL9A1S0X87VBC1mHCrRe/pB2oes+l1eXxUYnPJyC9ayOoXMvqXQsomq0xs6OqZ3vaa5Iw1HIyA4Cv1suLaOCoU3hNvOIJ6C94H1vU0siQYk7DIq9Av5RZe+uE2FnOctNBvYLd3i0IZo1ajjc00yRK3v24VRq9nQpoPLuqyH2jzlhCEjXuPScPbi5KEVs9fNcOTtgzbVf7IG2gNiwNs5aCpN4Bv/Zv2A6zp5xGz9cWj2f0aD9v66vX4bexOs5t/YYhwuwAvkkJPSIGvxja0xRThnceHyFHKtj0H+bi/PWAtlI8YJcDX69cM30JAHDdQH+ltm/4scFptW1hlvMaP+WReCAaCrsHrAT+yka7ttw5YlUyvZ8EPog+j6fwHlxmrXM9h1BqdikomyJU00gm1++FJelfP+1zAwcyrxCnbRl3ARFrAt8hIlrT6Vyu8WvWtLxcI8KcLcJQb/LgkW+sCTGlYcY8z3zkigJMbYn07ewTL5Ss7LazTJJa758I7PZan/v3xQHd5DEc5WBneiV3iOznDFgup0VAMkIviVjVCkszaPSVEdK2NU7jtrh6Jfm7bU/3P6ZG+CkyDLIa8MBn9KPXeJd/y+jTk5Ii+fIwO/+mDpGNUribg6TPxhzZ8b/XdZO1kS1gVgqjXyVC+M+BRBh6C4H21w/eMzjCtDIpoxt5rGKL6Nu/IFMipoC4fgx6LIIHwtGYMG7SWQi7OsMAkiwZRg0n68/RqWgLzBt/4pfjSRYuk=\" , \"Expiration\" : \"2020-02-20T18:49:50Z\" , \"AccessKeyId\" : \"ASIA36C6WWEJUMHA3L7Z\" } } A mutating webhook that runs as part of the EKS control plane injects the AWS Role ARN and the path to a web identity token file into the Pod as environment variables. These values can also be supplied manually. AWS_ROLE_ARN=arn:aws:iam::AWS_ACCOUNT_ID:role/IAM_ROLE_NAME AWS_WEB_IDENTITY_TOKEN_FILE=/var/run/secrets/eks.amazonaws.com/serviceaccount/token The kubelet will automatically rotate the projected token when it is older than 80% of its total TTL, or after 24 hours. The AWS SDKs are responsible for reloading the token when it rotates. For further information about IRSA, see https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts-technical-overview.html . Recommendations \u00b6 Update the aws-node daemonset to use IRSA \u00b6 At present, the aws-node daemonset is configured to use a role assigned to the EC2 instances to assign IPs to pods. This role includes several AWS managed policies, e.g. AmazonEKS_CNI_Policy and EC2ContainerRegistryReadOnly that effectly allow all pods running on a node to attach/detach ENIs, assign/unassign IP addresses, or pull images from ECR. Since this presents a risk to your cluster, it is recommended that you update the aws-node daemonset to use IRSA. A script for doing this can be found in the repository for this guide. Restrict access to the instance profile assigned to the worker node \u00b6 When you use IRSA, it updates the credential chain of the pod to use the IRSA token, however, the pod can still inherit the rights of the instance profile assigned to the worker node . When using IRSA, it is strongly recommended that you block access instance metadata to minimize the blast radius of a breach. Caution Blocking access to instance metadata will prevent pods that do not use IRSA from inheriting the role assigned to the worker node. You can block access to instance metadata by requiring the instance to use IMDSv2 only and updating the hop count to 1 as in the example below. You can also include these settings in the node group's launch template. Do not disable instance metadata as this will prevent components like the node termination handler and other things that rely on instance metadata from working properly. aws ec2 modify-instance-metadata-options --instance-id <value> --http-tokens required --http-put-response-hop-limit 1 You can also block a pod's access to EC2 metadata by manipulating iptables on the node. For further information about this method, see https://docs.aws.amazon.com/eks/latest/userguide/restrict-ec2-credential-access.html . Scope the IAM Role trust policy for IRSA to the service account name \u00b6 The trust policy can be scoped to a Namespace or a specific service account within a Namespace. When using IRSA it's best to make the role trust policy as explicit as possible by including the service account name. This will effectively prevent other Pods within the same Namespace from assuming the role. The CLI eksctl will do this automatically when you use it to create service accounts/IAM roles. See https://eksctl.io/usage/iamserviceaccounts/ for further information. When your application needs access to IDMS, use IMDSv2 and increase the hop limit on EC2 instances to 2 \u00b6 IMDSv2 requires you use a PUT request to get a session token. The initial PUT request has to include a TTL for the session token. Newer versions of the AWS SDKs will handle this and the renewal of said token automatically. It's also important to be aware that the default hop limit on EC2 instances is intentionally set to 1 to prevent IP forwarding. As a consequence, Pods that request a session token that are run on EC2 instances may eventually time out and fallback to using the IMDSv1 data flow. EKS adds support IMDSv2 by enabling both v1 and v2 and changing the hop limit to 2 on nodes provisioned by eksctl or with the official CloudFormation templates. Disable auto-mounting of service account tokens \u00b6 If your application doesn't need to call the Kubernetes API set the automountServiceAccountToken attribute to false in the PodSpec for your application or patch the default service account in each namespace so that it's no longer mounted to pods automatically. For example: kubectl patch serviceaccount default -p $'automountServiceAccountToken: false' Use dedicated service accounts for each application \u00b6 Each application should have its own dedicated service account. This applies to service accounts for the Kubernetes API as well as IRSA. Attention If you employ a blue/green approach to cluster upgrades instead of performing an in-place cluster upgrade, you will need to update the trust policy of each of the IRSA IAM roles with the OIDC endpoint of the new cluster. A blue/green cluster upgrade is where you create a cluster running a newer version of Kubernetes alongside the old cluster and use a load balancer or a service mesh to seamlessly shift traffic from services running on the old cluster to the new cluster. Run the application as a non-root user \u00b6 Containers run as root by default. While this allows them to read the web identity token file, running a container as root is not considered a best practice. As an alternative, consider adding the spec.securityContext.runAsUser attribute to the PodSpec. The value of runAsUser is abritrary value. In the following example, all processes within the Pod will run under the user ID specified in the runAsUser field. apiVersion : v1 kind : Pod metadata : name : security-context-demo spec : securityContext : runAsUser : 1000 runAsGroup : 3000 containers : - name : sec-ctx-demo image : busybox command : [ \"sh\" , \"-c\" , \"sleep 1h\" ] When you run a container as a non-root user, it prevents the container from reading the IRSA service account token because the token is assigned 0600 [root] permissions by default. If you update the securityContext for your container to include fsgroup=65534 [Nobody] it will allow the container to read the token. spec : securityContext : fsGroup : 65534 This is supposed to get fixed in an future release of k8s, kubernetes/enhancements#1598 . Grant least privileged access to applications \u00b6 Action Hero is a utility that you can run alongside your application to identify the AWS API calls and corresponding IAM permissions your application needs to function properly. It is similar to IAM Access Advisor in that it helps you gradually limit the scope of IAM roles assigned to applications. Consult the documentation on granting least privileged access to AWS resources for further information. Alternative approaches \u00b6 While IRSA is the preferred way to assign an AWS \"identity\" to a pod, it requires that you include recent version of the AWS SDKs in your application. For a complete listing of the SDKs that currently support IRSA, see https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts-minimum-sdk.html . If you have an application that you can't immediately update with a IRSA-compatible SDK, there are several community-built solutions available for assigning IAM roles to Kubernetes pods, including kube2iam and kiam . Although AWS doesn't endorse or condone the use of these solutions, they are frequently used by the community at large to achieve similar results as IRSA.","title":"Identity and Access Management"},{"location":"iam/#identity-and-access-management","text":"Identity and Access Management (IAM) is an AWS service that performs two essential functions: Authentication and Authorization. Authentication involves the verification of a identity whereas authorization governs the actions that can be performed by AWS resources. Within AWS, a resource can be another AWS service, e.g. EC2, or an AWS principle such as an IAM User or Role . The rules governing the actions that a resource is allowed to perform are expressed as IAM policies .","title":"Identity and Access Management"},{"location":"iam/#controlling-access-to-eks-clusters","text":"The Kubernetes project supports a variety of different strategies to authenticate requests to the kube-apiserver service, e.g. Bearer Tokens, X.509 certificates, OIDC, etc. EKS currently has native support for webhook token authentication and service account tokens . The webhook authentication strategy calls a webhook that verifies bearer tokens. On EKS, these bearer tokens are generated by the AWS CLI or the aws-iam-authenticator client when you run kubectl commands. As you execute commands, the token is passed to the kube-apiserver which forwards it to the authentication webhook. If the request is well-formed, the webhook calls a pre-signed URL embedded in the token's body. This URL validates the request's signature and returns information about the user, e.g. the user's account, Arn, and UserId to the kube-apiserver. To manually generate a authentication token, type the following command in a terminal window: aws eks get-token --cluster <cluster_name> The output should resemble this: { \"kind\" : \"ExecCredential\" , \"apiVersion\" : \"client.authentication.k8s.io/v1alpha1\" , \"spec\" : {}, \"status\" : { \"expirationTimestamp\" : \"2020-02-19T16:08:27Z\" , \"token\" : \"k8s-aws-v1.aHR0cHM6Ly9zdHMuYW1hem9uYXdzLmNvbS8_QWN0aW9uPUdldENhbGxlcklkZW50aXR5JlZlcnNpb249MjAxMS0wNi0xNSZYLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFKTkdSSUxLTlNSQzJXNVFBJTJGMjAyMDAyMTklMkZ1cy1lYXN0LTElMkZzdHMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDIwMDIxOVQxNTU0MjdaJlgtQW16LUV4cGlyZXM9NjAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JTNCeC1rOHMtYXdzLWlkJlgtQW16LVNpZ25hdHVyZT0yMjBmOGYzNTg1ZTMyMGRkYjVlNjgzYTVjOWE0MDUzMDFhZDc2NTQ2ZjI0ZjI4MTExZmRhZDA5Y2Y2NDhhMzkz\" } } Each token starts with k8s-aws-v1. followed by a base64 encoded string. The string, when decoded, should resemble this: https://sts.amazonaws.com/?Action = GetCallerIdentity & Version = 2011 -06-15 & X-Amz-Algorithm = AWS4-HMAC-SHA256 & X-Amz-Credential = AKIAJPFRILKNSRC2W5QA%2F20200219%2Fus-east-1%2Fsts%2Faws4_request & X-Amz-Date = 20200219T155427Z & X-Amz-Expires = 60 & X-Amz-SignedHeaders = host%3Bx-k8s-aws-id & X-Amz-Signature = 220f8f3285e320ddb5e683a5c9a405301ad76546f24f28111fdad09cf648a393 The token consists of a pre-signed URL that includes an Amazon credential and signature. For additional details see https://docs.aws.amazon.com/STS/latest/APIReference/API_GetCallerIdentity.html . The token has a time to live (TTL) of 15 minutes after which a new token will need to be generated. This is handled automatically when you use a client like kubectl , however, if you're using the Kubernetes dashboard, you will need to generate a new token and re-authenticate each time the token expires. Once the user's identity has been authenticated by the AWS IAM service, the kube-apiserver reads the aws-auth ConfigMap in the kube-system Namespace to determine the RBAC group to associate with the user. The aws-auth ConfigMap is used to create a static mapping between IAM principles, i.e. IAM Users and Roles, and Kubernetes RBAC groups. RBAC groups can be referenced in Kubernetes RoleBindings or ClusterRoleBindings. They are similar to IAM Roles in that they define a set of actions (verbs) that can be performed against a collection of Kubernetes resources (objects).","title":"Controlling Access to EKS Clusters"},{"location":"iam/#recommendations","text":"","title":"Recommendations"},{"location":"iam/#dont-use-a-service-account-token-for-authentication","text":"A service account token is a long-lived, static credential. If it is compromised, lost, or stolen, an attacker may be able to perform all the actions associated with that token until the service account is deleted. At times, you may need to grant an exception for applications that have to consume the Kubernetes API from outside the cluster, e.g. a CI/CD pipeline application. If such applications run on AWS infrastructure, like EC2 instances, consider using an instance profile and mapping that to a Kubernetes RBAC role in the aws-auth ConfigMap instead.","title":"Don't use a service account token for authentication"},{"location":"iam/#employ-least-privileged-access-to-aws-resources","text":"An IAM User does not need to be assigned privileges to AWS resources to access the Kubernetes API. If you need to grant an IAM user access to an EKS cluster, create an entry in the aws-auth ConfigMap for that user that maps to a specific Kubernetes RBAC group.","title":"Employ least privileged access to AWS Resources"},{"location":"iam/#use-iam-roles-when-multiple-users-need-identical-access-to-the-cluster","text":"Rather than creating an entry for each individual IAM User in the aws-auth ConfigMap, allow those users to assume an IAM Role and map that role to a Kubernetes RBAC group. This will be easier to maintain, especially as the number of users that require access grows.","title":"Use IAM Roles when multiple users need identical access to the cluster"},{"location":"iam/#employ-least-privileged-access-when-creating-rolebindings-and-clusterrolebindings","text":"Like the earlier point about granting access to AWS Resources, RoleBindings and ClusterRoleBindings should only include the set of permissions necessary to perform a specific function. Avoid using [\"*\"] in your Roles and ClusterRoles unless it's absolutely necessary. If you're unsure what permissions to assign, consider using a tool like audit2rbac to automatically generate Roles and binding based on the observed API calls in the Kubernetes Audit Log.","title":"Employ least privileged access when creating RoleBindings and ClusterRoleBindings"},{"location":"iam/#make-the-eks-cluster-endpoint-private","text":"By default when you provision an EKS cluster, the API cluster endpoint is set to public, i.e. it can be accessed from the Internet. Despite being accessible from the Internet, the endpoint is still considered secure because it requires all API requests to be authenticated by IAM and then authorized by Kubernetes RBAC. That said, if your corporate security policy mandates that you restrict access to the API from the Internet or prevents you from routing traffic outside the cluster VPC, you can: Configure the EKS cluster endpoint to be private. See Modifying Cluster Endpoint Access for further information on this topic. Leave the cluster endpoint public and specify which CIDR blocks can communicate with the cluster endpoint. The blocks are effectively a whitelisted set of public IP addresses that are allowed to access the cluster endpoint. Configure public access with a set of whitelisted CIDR blocks and set private endpoint access to enabled. This will allow public access from a specific range of public IPs while forcing all network traffic between the kubelets (workers) and the Kubernetes API through the cross-account ENIs that get provisioned into the cluster VPC when the control plane is provisioned.","title":"Make the EKS Cluster Endpoint private"},{"location":"iam/#create-the-cluster-with-a-dedicated-iam-role","text":"When you create an Amazon EKS cluster, the IAM entity user or role, such as a federated user that creates the cluster, is automatically granted system:masters permissions in the cluster's RBAC configuration. This access cannot be removed and is not managed through the aws-auth ConfigMap. Therefore it is a good idea to create the cluster with a dedicated IAM role and regularly audit who can assume this role. This role should not be used to perform routine actions on the cluster, and instead additional users should be granted access to the cluster through the aws-auth ConfigMap for this purpose. After the aws-auth ConfigMap is configured, the role can be deleted and only recreated in an emergency / break glass scenario where the aws-auth ConfigMap is corrupted and the cluster is otherwise inaccessible. This can be particularly useful in production clusters which do not usually have direct user access configured.","title":"Create the cluster with a dedicated IAM role"},{"location":"iam/#regularly-audit-access-to-the-cluster","text":"Who requires access is likely to change over time. Plan to periodically audit the aws-auth ConfigMap to see who has been granted access and the rights they've been assigned. You can also use open source tooling like kubectl-who-can , or rbac-lookup to examine the roles bound to a particular service account, user, or group. We'll explore this topic further when we get to the section on auditing . Additional ideas can be found in this article from NCC Group.","title":"Regularly audit access to the cluster"},{"location":"iam/#alternative-approaches-to-authentication-and-access-management","text":"While IAM is the preferred way to authenticate users who need access to an EKS cluster, it is possible to use an OIDC identity provider such as GitHub using an authentication proxy and Kubernetes impersonation . Posts for two such solutions have been published on the AWS Open Source blog: Authenticating to EKS Using GitHub Credentials with Teleport Consistent OIDC authentication across multiple EKS clusters using kube-oidc-proxy You can also use AWS SSO to federate AWS with an external identity provider, e.g. Azure AD. If you decide to use this, the AWS CLI v2.0 includes an option to create a named profile that makes it easy to associate an SSO session with your current CLI session and assume an IAM role. Know that you must assume a role prior to running kubectl as the IAM role is used to determine the user's Kubernetes RBAC group.","title":"Alternative Approaches to Authentication and Access Management"},{"location":"iam/#additional-resources","text":"rbac.dev A list of additional resources, including blogs and tools, for Kubernetes RBAC","title":"Additional Resources"},{"location":"iam/#pods-identities","text":"Certain applications that run within a Kubernetes cluster need permission to call the Kubernetes API to function properly. For example, the ALB Ingress Controller needs to be able to list a Service's Endpoints. The controller also needs to be able to invoke AWS APIs to provision and configure an ALB. In this section we will explore the best practices for assigning rights and privileges to Pods.","title":"Pods Identities"},{"location":"iam/#kubernetes-service-accounts","text":"A service account is a special type of object that allows you to assign a Kubernetes RBAC role to a pod. A default service account is created automatically for each Namespace within a cluster. When you deploy a pod into a Namespace without referencing a specific service account, the default service account for that Namespace will automatically get assigned to the Pod and the Secret, i.e. the service account (JWT) token for that service account, will get mounted to the pod as a volume at /var/run/secrets/kubernetes.io/serviceaccount . Decoding the service account token in that directory will reveal the following metadata: { \"iss\" : \"kubernetes/serviceaccount\" , \"kubernetes.io/serviceaccount/namespace\" : \"default\" , \"kubernetes.io/serviceaccount/secret.name\" : \"default-token-5pv4z\" , \"kubernetes.io/serviceaccount/service-account.name\" : \"default\" , \"kubernetes.io/serviceaccount/service-account.uid\" : \"3b36ddb5-438c-11ea-9438-063a49b60fba\" , \"sub\" : \"system:serviceaccount:default:default\" } The default service account has the following permissions to the Kubernetes API. apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : annotations : rbac.authorization.kubernetes.io/autoupdate : \"true\" creationTimestamp : \"2020-01-30T18:13:25Z\" labels : kubernetes.io/bootstrapping : rbac-defaults name : system:discovery resourceVersion : \"43\" selfLink : /apis/rbac.authorization.k8s.io/v1/clusterroles/system%3Adiscovery uid : 350d2ab8-438c-11ea-9438-063a49b60fba rules : - nonResourceURLs : - /api - /api/* - /apis - /apis/* - /healthz - /openapi - /openapi/* - /version - /version/ verbs : - get This role authorizes unauthenticated and authenticated users to read API information and is deemed safe to be publicly accessible. When an application running within a Pod calls the Kubernetes APIs, the Pod needs to be assigned a service account that explicitly grants it permission to call those APIs. Similar to guidelines for user access, the Role or ClusterRole bound to a service account should be restricted to the API resources and methods that the application needs to function and nothing else. To use a non-default service account simply set the spec.serviceAccountName field of a Pod to the name of the service account you wish to use. For additional information about creating service accounts, see https://kubernetes.io/docs/reference/access-authn-authz/rbac/#service-account-permissions .","title":"Kubernetes Service Accounts"},{"location":"iam/#iam-roles-for-service-accounts-irsa","text":"IRSA is a feature that allows you to assign an IAM role to a Kubernetes service account. It works by leveraging a Kubernetes feature known as Service Account Token Volume Projection . Pods with service accounts that reference an IAM Role call a public OIDC discovery endpoint for AWS IAM upon startup. The endpoint cryptographically signs the OIDC token issued by Kubernetes which ultimately allows the Pod to call the AWS APIs associated IAM role. When an AWS API is invoked, the AWS SDKs calls sts:AssumeRoleWithWebIdentity and automatically exchanges the Kubernetes issued token for a AWS role credential. Decoding the (JWT) token for IRSA will produce output similar to the example you see below: { \"aud\" : [ \"sts.amazonaws.com\" ], \"exp\" : 1582306514 , \"iat\" : 1582220114 , \"iss\" : \"https://oidc.eks.us-west-2.amazonaws.com/id/D43CF17C27A865933144EA99A26FB128\" , \"kubernetes.io\" : { \"namespace\" : \"default\" , \"pod\" : { \"name\" : \"alpine-57b5664646-rf966\" , \"uid\" : \"5a20f883-5407-11ea-a85c-0e62b7a4a436\" }, \"serviceaccount\" : { \"name\" : \"s3-read-only\" , \"uid\" : \"a720ba5c-5406-11ea-9438-063a49b60fba\" } }, \"nbf\" : 1582220114 , \"sub\" : \"system:serviceaccount:default:s3-read-only\" } This particular token grants the Pod view-only privileges to S3. When the application attempts to read from S3, the token is exchanged for a temporary set of IAM credentials that resembles this: { \"AssumedRoleUser\" : { \"AssumedRoleId\" : \"AROA36C6WWEJULFUYMPB6:abc\" , \"Arn\" : \"arn:aws:sts::123456789012:assumed-role/eksctl-winterfell-addon-iamserviceaccount-de-Role1-1D61LT75JH3MB/abc\" }, \"Audience\" : \"sts.amazonaws.com\" , \"Provider\" : \"arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-west-2.amazonaws.com/id/D43CF17C27A865933144EA99A26FB128\" , \"SubjectFromWebIdentityToken\" : \"system:serviceaccount:default:s3-read-only\" , \"Credentials\" : { \"SecretAccessKey\" : \"ORJ+8Adk+wW+nU8FETq7+mOqeA8Z6jlPihnV8hX1\" , \"SessionToken\" : \"FwoGZXIvYXdzEGMaDMLxAZkuLpmSwYXShiL9A1S0X87VBC1mHCrRe/pB2oes+l1eXxUYnPJyC9ayOoXMvqXQsomq0xs6OqZ3vaa5Iw1HIyA4Cv1suLaOCoU3hNvOIJ6C94H1vU0siQYk7DIq9Av5RZe+uE2FnOctNBvYLd3i0IZo1ajjc00yRK3v24VRq9nQpoPLuqyH2jzlhCEjXuPScPbi5KEVs9fNcOTtgzbVf7IG2gNiwNs5aCpN4Bv/Zv2A6zp5xGz9cWj2f0aD9v66vX4bexOs5t/YYhwuwAvkkJPSIGvxja0xRThnceHyFHKtj0H+bi/PWAtlI8YJcDX69cM30JAHDdQH+ltm/4scFptW1hlvMaP+WReCAaCrsHrAT+yka7ttw5YlUyvZ8EPog+j6fwHlxmrXM9h1BqdikomyJU00gm1++FJelfP+1zAwcyrxCnbRl3ARFrAt8hIlrT6Vyu8WvWtLxcI8KcLcJQb/LgkW+sCTGlYcY8z3zkigJMbYn07ewTL5Ss7LazTJJa758I7PZan/v3xQHd5DEc5WBneiV3iOznDFgup0VAMkIviVjVCkszaPSVEdK2NU7jtrh6Jfm7bU/3P6ZG+CkyDLIa8MBn9KPXeJd/y+jTk5Ii+fIwO/+mDpGNUribg6TPxhzZ8b/XdZO1kS1gVgqjXyVC+M+BRBh6C4H21w/eMzjCtDIpoxt5rGKL6Nu/IFMipoC4fgx6LIIHwtGYMG7SWQi7OsMAkiwZRg0n68/RqWgLzBt/4pfjSRYuk=\" , \"Expiration\" : \"2020-02-20T18:49:50Z\" , \"AccessKeyId\" : \"ASIA36C6WWEJUMHA3L7Z\" } } A mutating webhook that runs as part of the EKS control plane injects the AWS Role ARN and the path to a web identity token file into the Pod as environment variables. These values can also be supplied manually. AWS_ROLE_ARN=arn:aws:iam::AWS_ACCOUNT_ID:role/IAM_ROLE_NAME AWS_WEB_IDENTITY_TOKEN_FILE=/var/run/secrets/eks.amazonaws.com/serviceaccount/token The kubelet will automatically rotate the projected token when it is older than 80% of its total TTL, or after 24 hours. The AWS SDKs are responsible for reloading the token when it rotates. For further information about IRSA, see https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts-technical-overview.html .","title":"IAM Roles for Service Accounts (IRSA)"},{"location":"iam/#recommendations_1","text":"","title":"Recommendations"},{"location":"iam/#update-the-aws-node-daemonset-to-use-irsa","text":"At present, the aws-node daemonset is configured to use a role assigned to the EC2 instances to assign IPs to pods. This role includes several AWS managed policies, e.g. AmazonEKS_CNI_Policy and EC2ContainerRegistryReadOnly that effectly allow all pods running on a node to attach/detach ENIs, assign/unassign IP addresses, or pull images from ECR. Since this presents a risk to your cluster, it is recommended that you update the aws-node daemonset to use IRSA. A script for doing this can be found in the repository for this guide.","title":"Update the aws-node daemonset to use IRSA"},{"location":"iam/#restrict-access-to-the-instance-profile-assigned-to-the-worker-node","text":"When you use IRSA, it updates the credential chain of the pod to use the IRSA token, however, the pod can still inherit the rights of the instance profile assigned to the worker node . When using IRSA, it is strongly recommended that you block access instance metadata to minimize the blast radius of a breach. Caution Blocking access to instance metadata will prevent pods that do not use IRSA from inheriting the role assigned to the worker node. You can block access to instance metadata by requiring the instance to use IMDSv2 only and updating the hop count to 1 as in the example below. You can also include these settings in the node group's launch template. Do not disable instance metadata as this will prevent components like the node termination handler and other things that rely on instance metadata from working properly. aws ec2 modify-instance-metadata-options --instance-id <value> --http-tokens required --http-put-response-hop-limit 1 You can also block a pod's access to EC2 metadata by manipulating iptables on the node. For further information about this method, see https://docs.aws.amazon.com/eks/latest/userguide/restrict-ec2-credential-access.html .","title":"Restrict access to the instance profile assigned to the worker node"},{"location":"iam/#scope-the-iam-role-trust-policy-for-irsa-to-the-service-account-name","text":"The trust policy can be scoped to a Namespace or a specific service account within a Namespace. When using IRSA it's best to make the role trust policy as explicit as possible by including the service account name. This will effectively prevent other Pods within the same Namespace from assuming the role. The CLI eksctl will do this automatically when you use it to create service accounts/IAM roles. See https://eksctl.io/usage/iamserviceaccounts/ for further information.","title":"Scope the IAM Role trust policy for IRSA to the service account name"},{"location":"iam/#when-your-application-needs-access-to-idms-use-imdsv2-and-increase-the-hop-limit-on-ec2-instances-to-2","text":"IMDSv2 requires you use a PUT request to get a session token. The initial PUT request has to include a TTL for the session token. Newer versions of the AWS SDKs will handle this and the renewal of said token automatically. It's also important to be aware that the default hop limit on EC2 instances is intentionally set to 1 to prevent IP forwarding. As a consequence, Pods that request a session token that are run on EC2 instances may eventually time out and fallback to using the IMDSv1 data flow. EKS adds support IMDSv2 by enabling both v1 and v2 and changing the hop limit to 2 on nodes provisioned by eksctl or with the official CloudFormation templates.","title":"When your application needs access to IDMS, use IMDSv2 and increase the hop limit on EC2 instances to 2"},{"location":"iam/#disable-auto-mounting-of-service-account-tokens","text":"If your application doesn't need to call the Kubernetes API set the automountServiceAccountToken attribute to false in the PodSpec for your application or patch the default service account in each namespace so that it's no longer mounted to pods automatically. For example: kubectl patch serviceaccount default -p $'automountServiceAccountToken: false'","title":"Disable auto-mounting of service account tokens"},{"location":"iam/#use-dedicated-service-accounts-for-each-application","text":"Each application should have its own dedicated service account. This applies to service accounts for the Kubernetes API as well as IRSA. Attention If you employ a blue/green approach to cluster upgrades instead of performing an in-place cluster upgrade, you will need to update the trust policy of each of the IRSA IAM roles with the OIDC endpoint of the new cluster. A blue/green cluster upgrade is where you create a cluster running a newer version of Kubernetes alongside the old cluster and use a load balancer or a service mesh to seamlessly shift traffic from services running on the old cluster to the new cluster.","title":"Use dedicated service accounts for each application"},{"location":"iam/#run-the-application-as-a-non-root-user","text":"Containers run as root by default. While this allows them to read the web identity token file, running a container as root is not considered a best practice. As an alternative, consider adding the spec.securityContext.runAsUser attribute to the PodSpec. The value of runAsUser is abritrary value. In the following example, all processes within the Pod will run under the user ID specified in the runAsUser field. apiVersion : v1 kind : Pod metadata : name : security-context-demo spec : securityContext : runAsUser : 1000 runAsGroup : 3000 containers : - name : sec-ctx-demo image : busybox command : [ \"sh\" , \"-c\" , \"sleep 1h\" ] When you run a container as a non-root user, it prevents the container from reading the IRSA service account token because the token is assigned 0600 [root] permissions by default. If you update the securityContext for your container to include fsgroup=65534 [Nobody] it will allow the container to read the token. spec : securityContext : fsGroup : 65534 This is supposed to get fixed in an future release of k8s, kubernetes/enhancements#1598 .","title":"Run the application as a non-root user"},{"location":"iam/#grant-least-privileged-access-to-applications","text":"Action Hero is a utility that you can run alongside your application to identify the AWS API calls and corresponding IAM permissions your application needs to function properly. It is similar to IAM Access Advisor in that it helps you gradually limit the scope of IAM roles assigned to applications. Consult the documentation on granting least privileged access to AWS resources for further information.","title":"Grant least privileged access to applications"},{"location":"iam/#alternative-approaches","text":"While IRSA is the preferred way to assign an AWS \"identity\" to a pod, it requires that you include recent version of the AWS SDKs in your application. For a complete listing of the SDKs that currently support IRSA, see https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts-minimum-sdk.html . If you have an application that you can't immediately update with a IRSA-compatible SDK, there are several community-built solutions available for assigning IAM roles to Kubernetes pods, including kube2iam and kiam . Although AWS doesn't endorse or condone the use of these solutions, they are frequently used by the community at large to achieve similar results as IRSA.","title":"Alternative approaches"},{"location":"image/","text":"Image security \u00b6 You should consider the container image as your first line of defense against an attack. An insecure, poorly constructed image can allow an attacker to escape the bounds of the container and gain access to the host. Once on the host, an attacker can gain access to sensitive information or move laterally within the cluster or with your AWS account. The following best practices will help mitigate risk of this happening. Recommendations \u00b6 Create minimal images \u00b6 Start by removing all extraneous binaries from the container image. If you\u2019re using an unfamiliar image from Dockerhub, inspect the image using an application like Dive which can show you the contents of each of the container\u2019s layers. Remove all binaries with the SETUID and SETGID bits as they can be used to escalate privilege and consider removing all shells and utilities like nc and curl that can be used for nefarious purposes. You can find the files with SETUID and SETGID bits with the following command: find / -perm /6000 -type f -exec ls -ld {} \\; To remove the special permissions from these files, add the following directive to your container image: RUN find / -xdev -perm /6000 -type f -exec chmod a-s {} \\; || true Colloquially, this is known as de-fanging your image. Use multi-stage builds \u00b6 Using multi-stage builds is a way to create minimal images. Oftentimes, multi-stage builds are used to automate parts of the Continuous Integration cycle. For example, multi-stage builds can be used to lint your source code or perform static code analysis. This affords developers an opportunity to get near immediate feedback instead of waiting for a pipeline to execute. Multi-stage builds are attractive from a security standpoint because they allow you to minimize the size of the final image pushed to your container registry. Container images devoid of build tools and other extraneous binaries improves your security posture by reducing the attack surface of the image. For additional information about multi-stage builds, see https://docs.docker.com/develop/develop-images/multistage-build/ . Scan images for vulnerabilities regularly \u00b6 Like their virtual machine counterparts, container images can contain binaries and application libraries with vulnerabilities or develop vulnerabilities over time. The best way to safeguard against exploits is by regularly scanning your images with an image scanner. Images that are stored in Amazon ECR can be scanned on push or on-demand (once during a 24 hour period). ECR currently leverages Clair an open source image scanning solution. After an image is scanned, the results are logged to the event stream for ECR in EventBridge. You can also see the results of a scan from within the ECR console. Images with a HIGH or CRITICAL vulnerability should be deleted or rebuilt. If an image that has been deployed develops a vulnerability, it should be replaced as soon as possible. Knowing where images with vulnerabilities have been deployed is essential to keeping your environment secure. While you could conceivably build an image tracking solution yourself, there are already several commercial offerings that provide this and other advanced capabilities out of the box, including: Anchore Twistlock Aqua Kubei Trivy A Kubernetes validation webhook could also be used to validate that images are free of critical vulnerabilities. Validation webhooks are invoked prior to the Kubernetes API. They are typically used to reject requests that don't comply with the validation criteria defined in the webhook. This is an example of a serverless webhook that calls the ECR describeImageScanFindings API to deteremine whether a pod is pulling an image with critical vulnerabilities. If vulnerabilities are found, the pod is rejected and a message with list of CVEs is returned as an Event. Create IAM policies for ECR repositories \u00b6 Nowadays, it is not uncommon for an organization to have multiple development teams operating independently within a shared AWS account. If these teams don't need to share assets, you may want to create a set of IAM policies that restrict access to the repositories each team can interact with. A good way to implement this is by using ECR namespaces . Namespaces are a way to group similar repositories together. For example, all of the registries for team A can be prefaced with the team-a/ while those for team B can use the team-b/ prefix. The policy to restrict access might look like the following: { \"Version\" : \"2012-10-17\" , \"Statement\" : [{ \"Sid\" : \"AllowPushPull\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"arn:aws:iam::123456789012:role/<team_a_role_name>\" }, \"Action\" : [ \"ecr:GetDownloadUrlForLayer\" , \"ecr:BatchGetImage\" , \"ecr:BatchCheckLayerAvailability\" , \"ecr:PutImage\" , \"ecr:InitiateLayerUpload\" , \"ecr:UploadLayerPart\" , \"ecr:CompleteLayerUpload\" ], \"Resource\" : [ \"arn:aws:ecr:region:123456789012:repository/team-a/*\" ] }] } Consider using ECR private endpoints \u00b6 The ECR API has a public endpoint. Consequently, ECR registries can be accessed from the Internet so long as the request has been authenticated and authorized by IAM. For those who need to operate in a sandboxed environment where the cluster VPC lacks an Internet Gateway (IGW), you can configure a private endpoint for ECR. Creating a private endpoint enables you to privately access the ECR API through a private IP address instead of routing traffic across the Internet. For additional information on this topic, see https://docs.aws.amazon.com/AmazonECR/latest/userguide/vpc-endpoints.html. Implement endpoint policies for ECR \u00b6 The default endpoint policy for allows access to all ECR repositories within a region. This might allow an attacker/insider to exfiltrate data by packaging it as a container image and pushing it to a registry in another AWS account. Mitigating this risk involves creating an endpoint policy that limits API access to ECR respositories. For example, the following policy allows all AWS principles in your account to perform all actions against your and only your ECR repositories: { \"Statement\" : [{ \"Sid\" : \"LimitECRAccess\" , \"Principal\" : \"*\" , \"Action\" : \"*\" , \"Effect\" : \"Allow\" , \"Resource\" : \"arn:aws:ecr:region:<your_account_id>:repository/*\" }, ] } You can enhance this further by setting a condition that uses the new PrincipalOrgID attribute which will prevent pushing/pulling of images by an IAM principle that is not part of your AWS Organization. See, aws:PrincipalOrgID for additional details. We recommended applying the same policy to both the com.amazonaws.<region>.ecr.dkr and the com.amazonaws.<region>.ecr.api endpoints. Since EKS pulls images for kube-proxy, coredns, and aws-node from ECR, you will need to add the account ID of the registry, e.g. 602401143452.dkr.ecr.us-west-2.amazonaws.com/* to the list of resources in the endpoint policy or alter the policy to allow pulls from \"*\" and restrict pushes to your account ID. The table below reveals the mapping between the AWS accounts where EKS images are vended from and cluster region. Account Number Region 602401143452 All commercial regions except for those listed below 800184023465 HKG 558608220178 BAH 918309763551 BJS 961992271922 ZHY For further information about using endpoint policies, see Using VPC endpoint policies to control Amazon ECR access . Create a set of curated images \u00b6 Rather than allowing developers to create their own images, consider creating a set of vetted images for the different application stacks in your organization. By doing so, developers can forego learning how to compose Dockerfiles and concentrate on writing code. As changes are merged into Master, a CI/CD pipeline can automatically compile the asset, store it in an artifact repository and copy the artifact into the appropriate image before pushing it to a Docker registry like ECR. At the very least you should create a set of base images from which developers to create their own Dockerfiles. Ideally, you want to avoid pulling images from Dockerhub because a) you don't always know what is in the image and b) about a fifth of the top 1000 images have vulnerabilties. A list of those images and their vulnerabilities can be found at https://vulnerablecontainers.org/. Add the USER directive to your Dockerfiles to run as a non-root user \u00b6 As was mentioned in the pod security section, you should avoid running container as root. While you can configure this as part of the podSpec, it is a good habit to use the USER directive to your Dockerfiles. The USER directive sets the UID to use when running RUN , ENTRYPOINT , or CMD instruction that appears after the USER directive. Lint your Dockerfiles \u00b6 Linting can be used to verify that your Dockerfiles are adhering to a set of predefined guidelines, e.g. the inclusion of the USER directive or the requirement that all images be tagged. dockerfile_lint is an open source project from RedHat that verifies common best practices and includes a rule engine that you can use to build your own rules for linting Dockerfiles. It can be incorporated into a CI pipeline, in that builds with Dockerfiles that violate a rule will automatically fail. Build images from Scratch \u00b6 Reducing the attack surface of your container images should be primary aim when building images. The ideal way to do this is by creating minimal images that are devoid of binaries that can be used to exploit vulnerabilities. Fortunately, Docker has a mechanism to create images from scratch . With langages like Go, you can create a static linked binary and reference it in your Dockerfile as in this example: ############################ # STEP 1 build executable binary ############################ FROM golang:alpine AS builder # Install git. # Git is required for fetching the dependencies. RUN apk update && apk add --no-cache git WORKDIR $GOPATH/src/mypackage/myapp/ COPY . . # Fetch dependencies. # Using go get. RUN go get -d -v # Build the binary. RUN go build -o /go/bin/hello ############################ # STEP 2 build a small image ############################ FROM scratch # Copy our static executable. COPY --from = builder /go/bin/hello /go/bin/hello # Run the hello binary. ENTRYPOINT [ \"/go/bin/hello\" ] This creates a container image that consists of your application and nothing else, making it extremely secure. Sign your images \u00b6 When Docker was first introduced, there was no cryptographic model for verifying container images. With v2, Docker added digests to the image manifest. This allowed an image\u2019s configuration to be hashed and for the hash to be used to generate an ID for the image. When image signing is enabled, the [Docker] engine verifies the manifest\u2019s signature, ensuring that the content was produced from a trusted source and no tampering has occurred. After each layer is downloaded, the engine verifies the digest of the layer, ensuring that the content matches the content specified in the manifest. Image signing effectively allows you to create a secure supply chain, through the verification of digital signatures associated with the image. In a Kubernetes environment, you can use an dynamic admission controller to verify that an image has been signed, as in these examples: https://github.com/IBM/portieris and https://github.com/kelseyhightower/grafeas-tutorial. By signing your images, you're verifying the publisher (source) ensuring that the image hasn't been tampered with (integrity). Tools \u00b6 Bane An AppArmor profile generator for Docker containers docker-slim Build secure minimal images dockerfile-lint Rule based linter for Dockerfiles Gatekeeper and OPA A policy based admission controller in-toto Allows the user to verify if a step in the supply chain was intended to be performed, and if the step was performed by the right actor Notary A project for signing container images Grafeas An open artifact metadata API to audit and govern your software supply chain","title":"Image Security"},{"location":"image/#image-security","text":"You should consider the container image as your first line of defense against an attack. An insecure, poorly constructed image can allow an attacker to escape the bounds of the container and gain access to the host. Once on the host, an attacker can gain access to sensitive information or move laterally within the cluster or with your AWS account. The following best practices will help mitigate risk of this happening.","title":"Image security"},{"location":"image/#recommendations","text":"","title":"Recommendations"},{"location":"image/#create-minimal-images","text":"Start by removing all extraneous binaries from the container image. If you\u2019re using an unfamiliar image from Dockerhub, inspect the image using an application like Dive which can show you the contents of each of the container\u2019s layers. Remove all binaries with the SETUID and SETGID bits as they can be used to escalate privilege and consider removing all shells and utilities like nc and curl that can be used for nefarious purposes. You can find the files with SETUID and SETGID bits with the following command: find / -perm /6000 -type f -exec ls -ld {} \\; To remove the special permissions from these files, add the following directive to your container image: RUN find / -xdev -perm /6000 -type f -exec chmod a-s {} \\; || true Colloquially, this is known as de-fanging your image.","title":"Create minimal images"},{"location":"image/#use-multi-stage-builds","text":"Using multi-stage builds is a way to create minimal images. Oftentimes, multi-stage builds are used to automate parts of the Continuous Integration cycle. For example, multi-stage builds can be used to lint your source code or perform static code analysis. This affords developers an opportunity to get near immediate feedback instead of waiting for a pipeline to execute. Multi-stage builds are attractive from a security standpoint because they allow you to minimize the size of the final image pushed to your container registry. Container images devoid of build tools and other extraneous binaries improves your security posture by reducing the attack surface of the image. For additional information about multi-stage builds, see https://docs.docker.com/develop/develop-images/multistage-build/ .","title":"Use multi-stage builds"},{"location":"image/#scan-images-for-vulnerabilities-regularly","text":"Like their virtual machine counterparts, container images can contain binaries and application libraries with vulnerabilities or develop vulnerabilities over time. The best way to safeguard against exploits is by regularly scanning your images with an image scanner. Images that are stored in Amazon ECR can be scanned on push or on-demand (once during a 24 hour period). ECR currently leverages Clair an open source image scanning solution. After an image is scanned, the results are logged to the event stream for ECR in EventBridge. You can also see the results of a scan from within the ECR console. Images with a HIGH or CRITICAL vulnerability should be deleted or rebuilt. If an image that has been deployed develops a vulnerability, it should be replaced as soon as possible. Knowing where images with vulnerabilities have been deployed is essential to keeping your environment secure. While you could conceivably build an image tracking solution yourself, there are already several commercial offerings that provide this and other advanced capabilities out of the box, including: Anchore Twistlock Aqua Kubei Trivy A Kubernetes validation webhook could also be used to validate that images are free of critical vulnerabilities. Validation webhooks are invoked prior to the Kubernetes API. They are typically used to reject requests that don't comply with the validation criteria defined in the webhook. This is an example of a serverless webhook that calls the ECR describeImageScanFindings API to deteremine whether a pod is pulling an image with critical vulnerabilities. If vulnerabilities are found, the pod is rejected and a message with list of CVEs is returned as an Event.","title":"Scan images for vulnerabilities regularly"},{"location":"image/#create-iam-policies-for-ecr-repositories","text":"Nowadays, it is not uncommon for an organization to have multiple development teams operating independently within a shared AWS account. If these teams don't need to share assets, you may want to create a set of IAM policies that restrict access to the repositories each team can interact with. A good way to implement this is by using ECR namespaces . Namespaces are a way to group similar repositories together. For example, all of the registries for team A can be prefaced with the team-a/ while those for team B can use the team-b/ prefix. The policy to restrict access might look like the following: { \"Version\" : \"2012-10-17\" , \"Statement\" : [{ \"Sid\" : \"AllowPushPull\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"arn:aws:iam::123456789012:role/<team_a_role_name>\" }, \"Action\" : [ \"ecr:GetDownloadUrlForLayer\" , \"ecr:BatchGetImage\" , \"ecr:BatchCheckLayerAvailability\" , \"ecr:PutImage\" , \"ecr:InitiateLayerUpload\" , \"ecr:UploadLayerPart\" , \"ecr:CompleteLayerUpload\" ], \"Resource\" : [ \"arn:aws:ecr:region:123456789012:repository/team-a/*\" ] }] }","title":"Create IAM policies for ECR repositories"},{"location":"image/#consider-using-ecr-private-endpoints","text":"The ECR API has a public endpoint. Consequently, ECR registries can be accessed from the Internet so long as the request has been authenticated and authorized by IAM. For those who need to operate in a sandboxed environment where the cluster VPC lacks an Internet Gateway (IGW), you can configure a private endpoint for ECR. Creating a private endpoint enables you to privately access the ECR API through a private IP address instead of routing traffic across the Internet. For additional information on this topic, see https://docs.aws.amazon.com/AmazonECR/latest/userguide/vpc-endpoints.html.","title":"Consider using ECR private endpoints"},{"location":"image/#implement-endpoint-policies-for-ecr","text":"The default endpoint policy for allows access to all ECR repositories within a region. This might allow an attacker/insider to exfiltrate data by packaging it as a container image and pushing it to a registry in another AWS account. Mitigating this risk involves creating an endpoint policy that limits API access to ECR respositories. For example, the following policy allows all AWS principles in your account to perform all actions against your and only your ECR repositories: { \"Statement\" : [{ \"Sid\" : \"LimitECRAccess\" , \"Principal\" : \"*\" , \"Action\" : \"*\" , \"Effect\" : \"Allow\" , \"Resource\" : \"arn:aws:ecr:region:<your_account_id>:repository/*\" }, ] } You can enhance this further by setting a condition that uses the new PrincipalOrgID attribute which will prevent pushing/pulling of images by an IAM principle that is not part of your AWS Organization. See, aws:PrincipalOrgID for additional details. We recommended applying the same policy to both the com.amazonaws.<region>.ecr.dkr and the com.amazonaws.<region>.ecr.api endpoints. Since EKS pulls images for kube-proxy, coredns, and aws-node from ECR, you will need to add the account ID of the registry, e.g. 602401143452.dkr.ecr.us-west-2.amazonaws.com/* to the list of resources in the endpoint policy or alter the policy to allow pulls from \"*\" and restrict pushes to your account ID. The table below reveals the mapping between the AWS accounts where EKS images are vended from and cluster region. Account Number Region 602401143452 All commercial regions except for those listed below 800184023465 HKG 558608220178 BAH 918309763551 BJS 961992271922 ZHY For further information about using endpoint policies, see Using VPC endpoint policies to control Amazon ECR access .","title":"Implement endpoint policies for ECR"},{"location":"image/#create-a-set-of-curated-images","text":"Rather than allowing developers to create their own images, consider creating a set of vetted images for the different application stacks in your organization. By doing so, developers can forego learning how to compose Dockerfiles and concentrate on writing code. As changes are merged into Master, a CI/CD pipeline can automatically compile the asset, store it in an artifact repository and copy the artifact into the appropriate image before pushing it to a Docker registry like ECR. At the very least you should create a set of base images from which developers to create their own Dockerfiles. Ideally, you want to avoid pulling images from Dockerhub because a) you don't always know what is in the image and b) about a fifth of the top 1000 images have vulnerabilties. A list of those images and their vulnerabilities can be found at https://vulnerablecontainers.org/.","title":"Create a set of curated images"},{"location":"image/#add-the-user-directive-to-your-dockerfiles-to-run-as-a-non-root-user","text":"As was mentioned in the pod security section, you should avoid running container as root. While you can configure this as part of the podSpec, it is a good habit to use the USER directive to your Dockerfiles. The USER directive sets the UID to use when running RUN , ENTRYPOINT , or CMD instruction that appears after the USER directive.","title":"Add the USER directive to your Dockerfiles to run as a non-root user"},{"location":"image/#lint-your-dockerfiles","text":"Linting can be used to verify that your Dockerfiles are adhering to a set of predefined guidelines, e.g. the inclusion of the USER directive or the requirement that all images be tagged. dockerfile_lint is an open source project from RedHat that verifies common best practices and includes a rule engine that you can use to build your own rules for linting Dockerfiles. It can be incorporated into a CI pipeline, in that builds with Dockerfiles that violate a rule will automatically fail.","title":"Lint your Dockerfiles"},{"location":"image/#build-images-from-scratch","text":"Reducing the attack surface of your container images should be primary aim when building images. The ideal way to do this is by creating minimal images that are devoid of binaries that can be used to exploit vulnerabilities. Fortunately, Docker has a mechanism to create images from scratch . With langages like Go, you can create a static linked binary and reference it in your Dockerfile as in this example: ############################ # STEP 1 build executable binary ############################ FROM golang:alpine AS builder # Install git. # Git is required for fetching the dependencies. RUN apk update && apk add --no-cache git WORKDIR $GOPATH/src/mypackage/myapp/ COPY . . # Fetch dependencies. # Using go get. RUN go get -d -v # Build the binary. RUN go build -o /go/bin/hello ############################ # STEP 2 build a small image ############################ FROM scratch # Copy our static executable. COPY --from = builder /go/bin/hello /go/bin/hello # Run the hello binary. ENTRYPOINT [ \"/go/bin/hello\" ] This creates a container image that consists of your application and nothing else, making it extremely secure.","title":"Build images from Scratch"},{"location":"image/#sign-your-images","text":"When Docker was first introduced, there was no cryptographic model for verifying container images. With v2, Docker added digests to the image manifest. This allowed an image\u2019s configuration to be hashed and for the hash to be used to generate an ID for the image. When image signing is enabled, the [Docker] engine verifies the manifest\u2019s signature, ensuring that the content was produced from a trusted source and no tampering has occurred. After each layer is downloaded, the engine verifies the digest of the layer, ensuring that the content matches the content specified in the manifest. Image signing effectively allows you to create a secure supply chain, through the verification of digital signatures associated with the image. In a Kubernetes environment, you can use an dynamic admission controller to verify that an image has been signed, as in these examples: https://github.com/IBM/portieris and https://github.com/kelseyhightower/grafeas-tutorial. By signing your images, you're verifying the publisher (source) ensuring that the image hasn't been tampered with (integrity).","title":"Sign your images"},{"location":"image/#tools","text":"Bane An AppArmor profile generator for Docker containers docker-slim Build secure minimal images dockerfile-lint Rule based linter for Dockerfiles Gatekeeper and OPA A policy based admission controller in-toto Allows the user to verify if a step in the supply chain was intended to be performed, and if the step was performed by the right actor Notary A project for signing container images Grafeas An open artifact metadata API to audit and govern your software supply chain","title":"Tools"},{"location":"incidents/","text":"Incident response and forensics \u00b6 Your ability to react quickly to an incident can help minimize damage caused from a breach. Having a reliable alerting system that can warn you of suspicious behavior is the first step in a good incident response plan. When an incident does arise, you have to quickly decide whether to destroy and replace the container, or isolate and inspect the container. If you choose to isolate the container for forensic investigation and root cause analysis, then the following set of activities should be followed: Sample incident response plan \u00b6 Identify the offending Pod and worker node \u00b6 Your first course of action should be to isolate the damage. Start by identifying where the breach occurred and isolate that Pod and its node from the rest of the infrastructure. Isolate the Pod by creating a Network Policy that denies all ingress and egress traffic to the pod \u00b6 A deny all traffic rule may help stop an attack that is already underway by severing all connections to the pod. The following Network Policy will apply to a pod with the label app=web . apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : default-deny spec : podSelector : matchLabels : app : web policyTypes : - Ingress - Egress Attention A Network Policy may prove ineffective if an attacker has gained access to underlying host. If you suspect that has happened, you can use AWS Security Groups to isolate a compromised host from other hosts. When changing a host's security group, be aware that it will impact all containers running on that host. Revoke temporary security credentials assigned to the pod or worker node if necessary \u00b6 If the worker node has been assigned an IAM role that allows Pods to gain access to other AWS resources, remove those roles from the instance to prevent further damage from the attack. Similarly, if the Pod has been assigned an IAM role, evaluate whether you can safely remove the IAM policies from the role without impacting other workloads. Cordon the worker node \u00b6 By cordoning the impacted worker node, you're informing the scheduler to avoid scheduling pods onto the affected node. This will allow you to remove the node for forensic study without disrupting other workloads. Info This guidance is not applicable to Fargate where each Fargate pod run in its own sandboxed environment. Instead of cordoning, sequester the affected Fargate pods by applying a network policy that denies all ingress and egress traffic. Enable termination protection on impacted worker node \u00b6 An attacker may attempt to erase their misdeeds by terminating an affected node. Enabling termination protection can prevent this from happening. Instance scale-in protection will protect the node from a scale-in event. Warning You cannot enable termination protection on a Spot instance. Label the offending Pod/Node with a label indicating that it is part of an active investigation \u00b6 This will serve as a warning to cluster administrators not to tamper with the affected Pods/Nodes until the investigation is complete. Capture volatile artifacts on the worker node \u00b6 Capture the operating system memory . This will capture the Docker daemon and its subprocess per container. MargaritaShotgun , a remote memory acquisition tool, can aid in this effort. Perform a netstat tree dump of the processes running and the open ports . This will capture the docker daemon and its subprocess per container. Run docker commands before evidence is altered on the worker node . docker container top CONTAINER for processes running. docker container logs CONTAINER for daemon level held logs. docker container port CONTAINER for list of open ports. docker container diff CONTAINER to capture changes to files and directories to container's filesystem since its initial launch. Pause the container for forensic capture . Snapshot the instance's EBS volumes . Recommendations \u00b6 Review the AWS Security Incident Response Whitepaper \u00b6 While this section gives a brief overview along with a few recommendations for handling suspected security breaches, the topic is exhaustively covered in the white paper, AWS Security Incident Response . Practice security game days \u00b6 Divide your security practitioners into 2 teams: red and blue. The red team will be focused on probing different systems for vulnerabilities while the blue team will be responsible for defending against them. If you don't have enough security practitioners to create separate teams, consider hiring an outside entity that has knowledge of Kubernetes exploits. Run penetration tests against your cluster \u00b6 Periodically attacking your own cluster can help you discover vulnerabilities and misconfigurations. Before getting started, follow the penetration test guidelines before conducting a test against your cluster. Tools \u00b6 kube-hunter , a penetration testing tool for Kubernetes. Gremlin , a chaos engineering toolkit that you can use to simulate attacks against your applications and infrastructure. kube-forensics , a Kubernetes controller that triggers a job that collects the state of a running pod and dumps it in an S3 bucket. Attacking and Defending Kubernetes Installations","title":"Incident Response and Forensics"},{"location":"incidents/#incident-response-and-forensics","text":"Your ability to react quickly to an incident can help minimize damage caused from a breach. Having a reliable alerting system that can warn you of suspicious behavior is the first step in a good incident response plan. When an incident does arise, you have to quickly decide whether to destroy and replace the container, or isolate and inspect the container. If you choose to isolate the container for forensic investigation and root cause analysis, then the following set of activities should be followed:","title":"Incident response and forensics"},{"location":"incidents/#sample-incident-response-plan","text":"","title":"Sample incident response plan"},{"location":"incidents/#identify-the-offending-pod-and-worker-node","text":"Your first course of action should be to isolate the damage. Start by identifying where the breach occurred and isolate that Pod and its node from the rest of the infrastructure.","title":"Identify the offending Pod and worker node"},{"location":"incidents/#isolate-the-pod-by-creating-a-network-policy-that-denies-all-ingress-and-egress-traffic-to-the-pod","text":"A deny all traffic rule may help stop an attack that is already underway by severing all connections to the pod. The following Network Policy will apply to a pod with the label app=web . apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : default-deny spec : podSelector : matchLabels : app : web policyTypes : - Ingress - Egress Attention A Network Policy may prove ineffective if an attacker has gained access to underlying host. If you suspect that has happened, you can use AWS Security Groups to isolate a compromised host from other hosts. When changing a host's security group, be aware that it will impact all containers running on that host.","title":"Isolate the Pod by creating a Network Policy that denies all ingress and egress traffic to the pod"},{"location":"incidents/#revoke-temporary-security-credentials-assigned-to-the-pod-or-worker-node-if-necessary","text":"If the worker node has been assigned an IAM role that allows Pods to gain access to other AWS resources, remove those roles from the instance to prevent further damage from the attack. Similarly, if the Pod has been assigned an IAM role, evaluate whether you can safely remove the IAM policies from the role without impacting other workloads.","title":"Revoke temporary security credentials assigned to the pod or worker node if necessary"},{"location":"incidents/#cordon-the-worker-node","text":"By cordoning the impacted worker node, you're informing the scheduler to avoid scheduling pods onto the affected node. This will allow you to remove the node for forensic study without disrupting other workloads. Info This guidance is not applicable to Fargate where each Fargate pod run in its own sandboxed environment. Instead of cordoning, sequester the affected Fargate pods by applying a network policy that denies all ingress and egress traffic.","title":"Cordon the worker node"},{"location":"incidents/#enable-termination-protection-on-impacted-worker-node","text":"An attacker may attempt to erase their misdeeds by terminating an affected node. Enabling termination protection can prevent this from happening. Instance scale-in protection will protect the node from a scale-in event. Warning You cannot enable termination protection on a Spot instance.","title":"Enable termination protection on impacted worker node"},{"location":"incidents/#label-the-offending-podnode-with-a-label-indicating-that-it-is-part-of-an-active-investigation","text":"This will serve as a warning to cluster administrators not to tamper with the affected Pods/Nodes until the investigation is complete.","title":"Label the offending Pod/Node with a label indicating that it is part of an active investigation"},{"location":"incidents/#capture-volatile-artifacts-on-the-worker-node","text":"Capture the operating system memory . This will capture the Docker daemon and its subprocess per container. MargaritaShotgun , a remote memory acquisition tool, can aid in this effort. Perform a netstat tree dump of the processes running and the open ports . This will capture the docker daemon and its subprocess per container. Run docker commands before evidence is altered on the worker node . docker container top CONTAINER for processes running. docker container logs CONTAINER for daemon level held logs. docker container port CONTAINER for list of open ports. docker container diff CONTAINER to capture changes to files and directories to container's filesystem since its initial launch. Pause the container for forensic capture . Snapshot the instance's EBS volumes .","title":"Capture volatile artifacts on the worker node"},{"location":"incidents/#recommendations","text":"","title":"Recommendations"},{"location":"incidents/#review-the-aws-security-incident-response-whitepaper","text":"While this section gives a brief overview along with a few recommendations for handling suspected security breaches, the topic is exhaustively covered in the white paper, AWS Security Incident Response .","title":"Review the AWS Security Incident Response Whitepaper"},{"location":"incidents/#practice-security-game-days","text":"Divide your security practitioners into 2 teams: red and blue. The red team will be focused on probing different systems for vulnerabilities while the blue team will be responsible for defending against them. If you don't have enough security practitioners to create separate teams, consider hiring an outside entity that has knowledge of Kubernetes exploits.","title":"Practice security game days"},{"location":"incidents/#run-penetration-tests-against-your-cluster","text":"Periodically attacking your own cluster can help you discover vulnerabilities and misconfigurations. Before getting started, follow the penetration test guidelines before conducting a test against your cluster.","title":"Run penetration tests against your cluster"},{"location":"incidents/#tools","text":"kube-hunter , a penetration testing tool for Kubernetes. Gremlin , a chaos engineering toolkit that you can use to simulate attacks against your applications and infrastructure. kube-forensics , a Kubernetes controller that triggers a job that collects the state of a running pod and dumps it in an S3 bucket. Attacking and Defending Kubernetes Installations","title":"Tools"},{"location":"multitenancy/","text":"Tenant Isolation \u00b6 When we think of multi-tenancy, we often want to isolate a user or application from other users or applications running on a shared infrastructure. Kubernetes is a single tenant orchestrator , i.e. a single instance of the control plane is shared among all the tenants within a cluster. There are, however, various Kubernetes objects that you can use to create the semblance of multi-tenancy. For example, Namespaces and Role-based access controls (RBAC) can be implemented to logically isolate tenants from each other. Similarly, Quotas and Limit Ranges can be used to control the amount of cluster resources each tenant can consume. Nevertheless, the cluster is the only construct that provides a strong security boundary. This is because an attacker that manages to gain access to a host within the cluster can retrieve all Secrets, ConfigMaps, and Volumes, mounted on that host. They could also impersonate the Kubelet which would allow them to manipulate the attributes of the node and/or move laterally within the cluster. The following sections will explain how to implement tenant isolation while mitigating the risks of using a single tenant orchestrator like Kubernetes. Soft multi-tenancy \u00b6 With soft multi-tenancy, you use native Kubernetes constructs, e.g. namespaces, roles and role bindings, and network policies, to create logical separation between tenants. RBAC, for example, can prevent tenants from accessing or manipulate each other's resources. Quotas and limit ranges control the amount of cluster resources each tenant can consume while network policies can help prevent applications deployed into different namespaces from communicating with each other. None of these controls, however, prevent pods from different tenants from sharing a node. If stronger isolation is required, you can use a node selector, anti-affinity rules, and/or taints and tolerations to force pods from different tenants to be scheduled onto separate nodes; often referred to as sole tenant nodes . This could get rather complicated, and cost prohibitive, in an environment with many tenants. Attention Soft multi-tenancy implemented with Namespaces does not allow you to provide tenants with a filtered list of Namespaces because Namespaces are a globaly scoped Type. If a tenant has the ability to view a particular Namespace, it can view all Namespaces within the cluster. Warning With soft-multi-tenancy, tenants retain the ability to query CoreDNS for all services that run within the cluster by default. An attacker could exploit this by running dig SRV . .svc.cluster.local from any pod in the cluster. If you need to restrict access to DNS records of services that run within your clusters, consider using the Firewall or Policy plugins for CoreDNS. For additional information, see https://github.com/coredns/policy#kubernetes-metadata-multi-tenancy-policy . Kiosk is an open source project that can aid in the implementation of soft multi-tenancy. It is implemented as a series of CRDs and controllers that provide the following capabilities: Accounts & Account Users to separate tenants in a shared Kubernetes cluster Self-Service Namespace Provisioning for account users Account Limits to ensure quality of service and fairness when sharing a cluster Namespace Templates for secure tenant isolation and self-service namespace initialization Loft is a commerical offering from the maintainers of Kiosk and DevSpace that adds the following capabilities: Mutli-cluster access for granting access to spaces in different clusters Sleep mode scales down deployments in a space during periods of inactivity Single sign-on with OIDC authentication providers like GitHub There are three primary use cases that can be addressed by soft multi-tenancy. Enterprise Setting \u00b6 The first is in an Enterprise setting where the \"tenants\" are semi-trusted in that they are employees, contractors, or are otherwise authorized by the organization. Each tenant will typically align to an administrative division such as a department or team. In this type of setting, a cluster administrator will usually be responsible for creating namespaces and managing policies. They may also implement a delegated adminstration model where certain individuals are given oversight of a namespace, allowing them to perform CRUD operations for non-policy related objects like deployments, services, pods, jobs, etc. The isolation provided by Docker may be acceptable within this setting or it may need to be augmented with additional controls such as Pod Security Policies (PSPs). It may also be necessary to restrict communication between services in different namespaces if stricter isolation is required. Kubernetes as a Service \u00b6 By constrast, soft multi-tenancy can be used in settings where you want to offer Kubernetes as a service (KaaS). With KaaS, your application is hosted in a shared cluster along with a collection of controllers and CRDs that provide a set of PaaS services. Tenants interact directly with the Kubernetes API server and are permitted to perform CRUD operations on non-policy objects. There is also an element of self-service in that tenants may be allowed to create and manage their own namespaces. In this type of environment, tenants are assumed to be running untrusted code. To isolate tenants in this type of environment, you will likely need to implement strict network policies as well as pod sandboxing . Sandboxing is where you run the containers of a pod inside a micro VM like Firecracker or in a user-space kernel. Today, you can create sandboxed pods with EKS Fargate. Software as a Service (SaaS) \u00b6 The final use case for soft multi-tenancy is in a Software-as-a-Service (SaaS) setting. In this environment, each tenant is associated with a particular instance of an application that's running within the cluster. Each instance often has its own data and uses separate access controls that are usually independent of Kubernetes RBAC. Unlike the other use cases, the tenant in a SaaS setting does not directly interface with the Kubernetes API. Instead, the SaaS application is responsible for interfacing with the Kubernetes API to create the necessary objects to support each tenant. Kubernetes Constructs \u00b6 In each of these instances the following constructs are used to isolate tenants from each other: Namespaces \u00b6 Namespaces are fundamental to implementing soft multi-tenancy. They allow you to divide the cluster into logical partitions. Quotas, network policies, service accounts, and other objects needed to implement multi-tenancy are scoped to a namespace. Network policies \u00b6 By default, all pods in a Kubernetes cluster are allowed to communicate with each other. This behavior can be altered using network policies. Network policies restrict communication between pods using labels or IP address ranges. In a multi-tenant environment where strict network isolation between tenants is required, we recommend starting with a default rule that denies communication between pods, and another rule that allows all pods to query the DNS server for name resolution. With that in place, you can begin adding more permissive rules that allow for communication within a namespace. This can be further refined as required. Attention Network policies are necessary but not sufficient. The enforcement of network policies requires a policy engine such as Calico or Cilium. Role-based access control (RBAC) \u00b6 Roles and role bindings are the Kubernetes objects used to enforce role-based access control (RBAC) in Kubernetes. Roles contain lists of actions that can be performed against objects in your cluster. Role bindings specify the individuals or groups to whom the roles apply. In the enterprise and KaaS settings, RBAC can be used to permit administration of objects by selected groups or individuals. Quotas \u00b6 Quotas are used to define limits on workloads hosted in your cluster. With quotas, you can specify the maximum amount of CPU and memory that a pod can consume, or you can limit the number of resources that can be allocated in a cluster or namespace. Limit ranges allow you to declare minimum, maximum, and default values for each limit. Overcommitting resources in a shared cluster is often beneficial because it allows you maximize your resources. However, unbounded access to a cluster can cause resource starvation, which can lead to performance degradation and loss of application availability. If a pod's requests are set too low and the actual resource utilization exceeds the capacity of the node, the node will begin to experience CPU or memory pressure. When this happens, pods may be restarted and/or evicted from the node. To prevent this from happening, you should plan to impose quotas on namespaces in a multi-tenant environment to force tenants to specify requests and limits when scheduling their pods on the cluster. It will also mitigate a potential denial of service by constraining the amount of resources a pod can consume. You can also use quotas to apportion the cluster's resources to align with a tenant's spend. This is particularly useful in the KaaS scenario. Pod priority and pre-emption \u00b6 Pod priority and pre-emption can be useful when you want to provide different qualities of services (QoS) for different customers. For example, with pod priority you can configure pods from customer A to run at a higher priority than customer B. When there's insufficient capacity available, the Kubelet will evict the lower-priority pods from customer B to accommodate the higher-priority pods from customer A. This can be especially handy in a SaaS environment where customers willing to pay a premium receive a higher quality of service. Mitigating controls \u00b6 Your chief concern as an administrator of a multi-tenant environment is preventing an attacker from gaining access to the underlying host. The following controls should be considered to mitigate this risk: Pod Security Policies (PSPs) \u00b6 PSPs should be used to curtail the actions that can be performed by a container and to reduce a container's privileges, e.g. running as a non-root user. Sandboxed execution environments for containers \u00b6 Sandboxing is a technique by which each container is run in its own isolated virtual machine. Technologies that perform pod sandboxing include Firecracker and Weave's Firekube . If you are building your own self-managed Kubernetes cluster on AWS, you may be able to configure alternate container runtimes such as Kata Containers . For additional information about the effort to make Firecracker a supported runtime for EKS, see https://threadreaderapp.com/thread/1238496944684597248.html . Open Policy Agent (OPA) & Gatekeeper \u00b6 Gatekeeper is a Kubernetes admission controller that enforces policies created with OPA . With OPA you can create a policy that runs pods from tenants on separate instances or at a higher priority than other tenants. A collection of common OPA policies can be found in the GitHub repository for this project. There is also an experimental OPA plugin for CoreDNS that allows you to use OPA to filter/control the records returned by CoreDNS. Kyverno \u00b6 Kyverno is a Kubernetes native policy engine that can validate, mutate, and generate configurations with policies as Kubernetes resources. Kyverno uses Kustomize-style overlays for validation, supports JSON Patch and strategic merge patch for mutation, and can clone resources across namespaces based on flexible triggers. You can use Kyverno to isolate namespaces, enforce pod security and other best practices, and generate default configurations such as network policies. Several examples are included in the GitHub respository for this project. Tools \u00b6 k-rail Designed to help you secure a multi-tenant environment through the enforcement of certain policies. Hard multi-tenancy \u00b6 Hard multi-tenancy can be implemented by provisioning separate clusters for each tenant. While this provides very strong isolation between tenants, it has several drawbacks. First, when you have many tenants, this approach can quickly become expensive. Not only will you have to pay for the control plane costs for each cluster, you will not be able to share compute resources between clusters. This will eventually cause fragmentation where a subset of your clusters are underutilized while others are overutilized. Second, you will likely need to buy or build special tooling to manage all of these clusters. In time, managing hundreds or thousands of clusters may simply become too unweildy. Finally, creating a cluster per tenant will be slow relative to a creating a namespace. Nevertheless, a hard-tenancy approach may be necessary in highly-regulated industries or in SaaS environments where strong isolation is required. Future directions \u00b6 The Kubernetes community has recognized the current shortcomings of soft multi-tenancy and the challenges with hard multi-tenancy. The Multi-Tenancy Special Interest Group (SIG) is attempting to address these shortcomings through several incubation projects, including Hierarchical Namespace Controller (HNC) and Virtual Cluster. The HNC proposal (KEP) describes a way to create parent-child relationships between namespaces with [policy] object inheritance along with an ability for tenant administrators to create subnamespaces. The Virtual Cluster proposal describes a mechanism for creating separate instances of the control plane services, including the API server, the controller manager, and scheduler, for each tenant within the cluster (also known as \"Kubernetes on Kubernetes\"). The Multi-Tenancy Benchmarks proposal provides guidelines for sharing clusters using namespaces for isolation and segmentation, and a command line tool kubectl-mtb to validate conformance to the guidelines. Multi-cluster management resources \u00b6 Banzai Cloud Kommander Lens Nirmata Rafay Rancher Weave Flux","title":"Multi-tenancy"},{"location":"multitenancy/#tenant-isolation","text":"When we think of multi-tenancy, we often want to isolate a user or application from other users or applications running on a shared infrastructure. Kubernetes is a single tenant orchestrator , i.e. a single instance of the control plane is shared among all the tenants within a cluster. There are, however, various Kubernetes objects that you can use to create the semblance of multi-tenancy. For example, Namespaces and Role-based access controls (RBAC) can be implemented to logically isolate tenants from each other. Similarly, Quotas and Limit Ranges can be used to control the amount of cluster resources each tenant can consume. Nevertheless, the cluster is the only construct that provides a strong security boundary. This is because an attacker that manages to gain access to a host within the cluster can retrieve all Secrets, ConfigMaps, and Volumes, mounted on that host. They could also impersonate the Kubelet which would allow them to manipulate the attributes of the node and/or move laterally within the cluster. The following sections will explain how to implement tenant isolation while mitigating the risks of using a single tenant orchestrator like Kubernetes.","title":"Tenant Isolation"},{"location":"multitenancy/#soft-multi-tenancy","text":"With soft multi-tenancy, you use native Kubernetes constructs, e.g. namespaces, roles and role bindings, and network policies, to create logical separation between tenants. RBAC, for example, can prevent tenants from accessing or manipulate each other's resources. Quotas and limit ranges control the amount of cluster resources each tenant can consume while network policies can help prevent applications deployed into different namespaces from communicating with each other. None of these controls, however, prevent pods from different tenants from sharing a node. If stronger isolation is required, you can use a node selector, anti-affinity rules, and/or taints and tolerations to force pods from different tenants to be scheduled onto separate nodes; often referred to as sole tenant nodes . This could get rather complicated, and cost prohibitive, in an environment with many tenants. Attention Soft multi-tenancy implemented with Namespaces does not allow you to provide tenants with a filtered list of Namespaces because Namespaces are a globaly scoped Type. If a tenant has the ability to view a particular Namespace, it can view all Namespaces within the cluster. Warning With soft-multi-tenancy, tenants retain the ability to query CoreDNS for all services that run within the cluster by default. An attacker could exploit this by running dig SRV . .svc.cluster.local from any pod in the cluster. If you need to restrict access to DNS records of services that run within your clusters, consider using the Firewall or Policy plugins for CoreDNS. For additional information, see https://github.com/coredns/policy#kubernetes-metadata-multi-tenancy-policy . Kiosk is an open source project that can aid in the implementation of soft multi-tenancy. It is implemented as a series of CRDs and controllers that provide the following capabilities: Accounts & Account Users to separate tenants in a shared Kubernetes cluster Self-Service Namespace Provisioning for account users Account Limits to ensure quality of service and fairness when sharing a cluster Namespace Templates for secure tenant isolation and self-service namespace initialization Loft is a commerical offering from the maintainers of Kiosk and DevSpace that adds the following capabilities: Mutli-cluster access for granting access to spaces in different clusters Sleep mode scales down deployments in a space during periods of inactivity Single sign-on with OIDC authentication providers like GitHub There are three primary use cases that can be addressed by soft multi-tenancy.","title":"Soft multi-tenancy"},{"location":"multitenancy/#enterprise-setting","text":"The first is in an Enterprise setting where the \"tenants\" are semi-trusted in that they are employees, contractors, or are otherwise authorized by the organization. Each tenant will typically align to an administrative division such as a department or team. In this type of setting, a cluster administrator will usually be responsible for creating namespaces and managing policies. They may also implement a delegated adminstration model where certain individuals are given oversight of a namespace, allowing them to perform CRUD operations for non-policy related objects like deployments, services, pods, jobs, etc. The isolation provided by Docker may be acceptable within this setting or it may need to be augmented with additional controls such as Pod Security Policies (PSPs). It may also be necessary to restrict communication between services in different namespaces if stricter isolation is required.","title":"Enterprise Setting"},{"location":"multitenancy/#kubernetes-as-a-service","text":"By constrast, soft multi-tenancy can be used in settings where you want to offer Kubernetes as a service (KaaS). With KaaS, your application is hosted in a shared cluster along with a collection of controllers and CRDs that provide a set of PaaS services. Tenants interact directly with the Kubernetes API server and are permitted to perform CRUD operations on non-policy objects. There is also an element of self-service in that tenants may be allowed to create and manage their own namespaces. In this type of environment, tenants are assumed to be running untrusted code. To isolate tenants in this type of environment, you will likely need to implement strict network policies as well as pod sandboxing . Sandboxing is where you run the containers of a pod inside a micro VM like Firecracker or in a user-space kernel. Today, you can create sandboxed pods with EKS Fargate.","title":"Kubernetes as a Service"},{"location":"multitenancy/#software-as-a-service-saas","text":"The final use case for soft multi-tenancy is in a Software-as-a-Service (SaaS) setting. In this environment, each tenant is associated with a particular instance of an application that's running within the cluster. Each instance often has its own data and uses separate access controls that are usually independent of Kubernetes RBAC. Unlike the other use cases, the tenant in a SaaS setting does not directly interface with the Kubernetes API. Instead, the SaaS application is responsible for interfacing with the Kubernetes API to create the necessary objects to support each tenant.","title":"Software as a Service (SaaS)"},{"location":"multitenancy/#kubernetes-constructs","text":"In each of these instances the following constructs are used to isolate tenants from each other:","title":"Kubernetes Constructs"},{"location":"multitenancy/#namespaces","text":"Namespaces are fundamental to implementing soft multi-tenancy. They allow you to divide the cluster into logical partitions. Quotas, network policies, service accounts, and other objects needed to implement multi-tenancy are scoped to a namespace.","title":"Namespaces"},{"location":"multitenancy/#network-policies","text":"By default, all pods in a Kubernetes cluster are allowed to communicate with each other. This behavior can be altered using network policies. Network policies restrict communication between pods using labels or IP address ranges. In a multi-tenant environment where strict network isolation between tenants is required, we recommend starting with a default rule that denies communication between pods, and another rule that allows all pods to query the DNS server for name resolution. With that in place, you can begin adding more permissive rules that allow for communication within a namespace. This can be further refined as required. Attention Network policies are necessary but not sufficient. The enforcement of network policies requires a policy engine such as Calico or Cilium.","title":"Network policies"},{"location":"multitenancy/#role-based-access-control-rbac","text":"Roles and role bindings are the Kubernetes objects used to enforce role-based access control (RBAC) in Kubernetes. Roles contain lists of actions that can be performed against objects in your cluster. Role bindings specify the individuals or groups to whom the roles apply. In the enterprise and KaaS settings, RBAC can be used to permit administration of objects by selected groups or individuals.","title":"Role-based access control (RBAC)"},{"location":"multitenancy/#quotas","text":"Quotas are used to define limits on workloads hosted in your cluster. With quotas, you can specify the maximum amount of CPU and memory that a pod can consume, or you can limit the number of resources that can be allocated in a cluster or namespace. Limit ranges allow you to declare minimum, maximum, and default values for each limit. Overcommitting resources in a shared cluster is often beneficial because it allows you maximize your resources. However, unbounded access to a cluster can cause resource starvation, which can lead to performance degradation and loss of application availability. If a pod's requests are set too low and the actual resource utilization exceeds the capacity of the node, the node will begin to experience CPU or memory pressure. When this happens, pods may be restarted and/or evicted from the node. To prevent this from happening, you should plan to impose quotas on namespaces in a multi-tenant environment to force tenants to specify requests and limits when scheduling their pods on the cluster. It will also mitigate a potential denial of service by constraining the amount of resources a pod can consume. You can also use quotas to apportion the cluster's resources to align with a tenant's spend. This is particularly useful in the KaaS scenario.","title":"Quotas"},{"location":"multitenancy/#pod-priority-and-pre-emption","text":"Pod priority and pre-emption can be useful when you want to provide different qualities of services (QoS) for different customers. For example, with pod priority you can configure pods from customer A to run at a higher priority than customer B. When there's insufficient capacity available, the Kubelet will evict the lower-priority pods from customer B to accommodate the higher-priority pods from customer A. This can be especially handy in a SaaS environment where customers willing to pay a premium receive a higher quality of service.","title":"Pod priority and pre-emption"},{"location":"multitenancy/#mitigating-controls","text":"Your chief concern as an administrator of a multi-tenant environment is preventing an attacker from gaining access to the underlying host. The following controls should be considered to mitigate this risk:","title":"Mitigating controls"},{"location":"multitenancy/#pod-security-policies-psps","text":"PSPs should be used to curtail the actions that can be performed by a container and to reduce a container's privileges, e.g. running as a non-root user.","title":"Pod Security Policies (PSPs)"},{"location":"multitenancy/#sandboxed-execution-environments-for-containers","text":"Sandboxing is a technique by which each container is run in its own isolated virtual machine. Technologies that perform pod sandboxing include Firecracker and Weave's Firekube . If you are building your own self-managed Kubernetes cluster on AWS, you may be able to configure alternate container runtimes such as Kata Containers . For additional information about the effort to make Firecracker a supported runtime for EKS, see https://threadreaderapp.com/thread/1238496944684597248.html .","title":"Sandboxed execution environments for containers"},{"location":"multitenancy/#open-policy-agent-opa-gatekeeper","text":"Gatekeeper is a Kubernetes admission controller that enforces policies created with OPA . With OPA you can create a policy that runs pods from tenants on separate instances or at a higher priority than other tenants. A collection of common OPA policies can be found in the GitHub repository for this project. There is also an experimental OPA plugin for CoreDNS that allows you to use OPA to filter/control the records returned by CoreDNS.","title":"Open Policy Agent (OPA) &amp; Gatekeeper"},{"location":"multitenancy/#kyverno","text":"Kyverno is a Kubernetes native policy engine that can validate, mutate, and generate configurations with policies as Kubernetes resources. Kyverno uses Kustomize-style overlays for validation, supports JSON Patch and strategic merge patch for mutation, and can clone resources across namespaces based on flexible triggers. You can use Kyverno to isolate namespaces, enforce pod security and other best practices, and generate default configurations such as network policies. Several examples are included in the GitHub respository for this project.","title":"Kyverno"},{"location":"multitenancy/#tools","text":"k-rail Designed to help you secure a multi-tenant environment through the enforcement of certain policies.","title":"Tools"},{"location":"multitenancy/#hard-multi-tenancy","text":"Hard multi-tenancy can be implemented by provisioning separate clusters for each tenant. While this provides very strong isolation between tenants, it has several drawbacks. First, when you have many tenants, this approach can quickly become expensive. Not only will you have to pay for the control plane costs for each cluster, you will not be able to share compute resources between clusters. This will eventually cause fragmentation where a subset of your clusters are underutilized while others are overutilized. Second, you will likely need to buy or build special tooling to manage all of these clusters. In time, managing hundreds or thousands of clusters may simply become too unweildy. Finally, creating a cluster per tenant will be slow relative to a creating a namespace. Nevertheless, a hard-tenancy approach may be necessary in highly-regulated industries or in SaaS environments where strong isolation is required.","title":"Hard multi-tenancy"},{"location":"multitenancy/#future-directions","text":"The Kubernetes community has recognized the current shortcomings of soft multi-tenancy and the challenges with hard multi-tenancy. The Multi-Tenancy Special Interest Group (SIG) is attempting to address these shortcomings through several incubation projects, including Hierarchical Namespace Controller (HNC) and Virtual Cluster. The HNC proposal (KEP) describes a way to create parent-child relationships between namespaces with [policy] object inheritance along with an ability for tenant administrators to create subnamespaces. The Virtual Cluster proposal describes a mechanism for creating separate instances of the control plane services, including the API server, the controller manager, and scheduler, for each tenant within the cluster (also known as \"Kubernetes on Kubernetes\"). The Multi-Tenancy Benchmarks proposal provides guidelines for sharing clusters using namespaces for isolation and segmentation, and a command line tool kubectl-mtb to validate conformance to the guidelines.","title":"Future directions"},{"location":"multitenancy/#multi-cluster-management-resources","text":"Banzai Cloud Kommander Lens Nirmata Rafay Rancher Weave Flux","title":"Multi-cluster management resources"},{"location":"network/","text":"Network security \u00b6 Network security has several facets. The first involves the application of rules which restrict the flow of network traffic between services. The second involves the encryption of traffic while it is in transit. The mechanisms to implement these security measures on EKS are varied but often include the following items: Traffic control \u00b6 Network Policies Security Groups Encryption in transit \u00b6 Service Mesh Container Network Interfaces (CNIs) Nitro Instances Network policy \u00b6 Within a Kubernetes cluster, all Pod to Pod communication is allowed by default. While this flexibility may help promote experimentation, it is not considered secure. Kubernetes network policies give you a mechanism to restrict network traffic between Pods (often referred to as East/West traffic) and between Pods and external services. Kubernetes network policies operate at layers 3 and 4 of the OSI model. Network policies use pod selectors and labels to identify source and destination pods, but can also include IP addresses, port numbers, protocol number, or a combination of these. Calico , is an open source policy engine from Tigera that works well with EKS. In addition to implementing the full set of Kubernetes network policy features, Calico supports extended network polices with a richer set of features, including support for layer 7 rules, e.g. HTTP, when integrated with Istio. Isovalent, the maintainers of Cilium , have also extended the network policies to include partial support for layer 7 rules, e.g. HTTP. Cilium also has support for DNS hostnames which can be useful for restricting traffic between Kubernetes Services/Pods and resources that run within or outside of your VPC. By contrast, Calico Enterprise includes a feature that allows you to map a Kubernetes network policy to an AWS security group, as well as DNS hostnames. Attention When you first provision an EKS cluster, the Calico policy engine is not installed by default. The manifests for installing Calico can be found in the VPC CNI repository at https://github.com/aws/amazon-vpc-cni-k8s/tree/master/config . Calico policies can be scoped to Namespaces, Pods, service accounts, or globally. When policies are scoped to a service account, it associates a set of ingress/egress rules with that service account. With the proper RBAC rules in place, you can prevent teams from overriding these rules, allowing IT security professionals to safely delegate administration of namespaces. You can find a list of common Kubernetes network policies at https://github.com/ahmetb/kubernetes-network-policy-recipes . A similar set of rules for Calico are available at https://docs.projectcalico.org/security/calico-network-policy . Recommendations \u00b6 Create a default deny policy \u00b6 As with RBAC policies, network policies should adhere to the policy of least privileged access. Start by creating a deny all policy that restricts all inbound and outbound traffic from a namespace or create a global policy using Calico. Kubernetes network policy apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : default-deny namespace : default spec : podSelector : {} policyTypes : - Ingress - Egress Tip The image above was created by the network policy viewer from Tufin . Calico global network policy apiVersion : crd.projectcalico.org/v1 kind : GlobalNetworkPolicy metadata : name : default-deny spec : selector : all() types : - Ingress - Egress Create a rule to allow DNS queries \u00b6 Once you have the default deny all rule in place, you can begin layering on additional rules, such as a global rule that allows pods to query CoreDNS for name resolution. You begin by labeling the namespace: kubectl label namespace kube-system name=kube-system Then add the network policy: apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : allow-dns-access namespace : default spec : podSelector : matchLabels : {} policyTypes : - Egress egress : - to : - namespaceSelector : matchLabels : name : kube-system ports : - protocol : UDP port : 53 Calico global policy equivalent apiVersion : crd.projectcalico.org/v1 kind : GlobalNetworkPolicy metadata : name : allow-dns-egress spec : selector : all() types : - Egress egress : - action : Allow protocol : UDP destination : namespaceSelector : name == \"kube-system\" ports : - 53 The following is an example of how to associate a network policy with a service account while preventing users associated with the readonly-sa-group from editing the service account my-sa in the default namespace: apiVersion : v1 kind : ServiceAccount metadata : name : my-sa namespace : default labels : name : my-sa --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : namespace : default name : readonly-sa-role rules : # Allows the subject to read a service account called my-sa - apiGroups : [ \"\" ] resources : [ \"serviceaccounts\" ] resourceNames : [ \"my-sa\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : namespace : default name : readonly-sa-rolebinding # Binds the readonly-sa-role to the RBAC group called readonly-sa-group. subjects : - kind : Group name : readonly-sa-group apiGroup : rbac.authorization.k8s.io roleRef : kind : Role name : readonly-sa-role apiGroup : rbac.authorization.k8s.io --- apiVersion : crd.projectcalico.org/v1 kind : NetworkPolicy metadata : name : netpol-sa-demo namespace : default # Allows all ingress traffic to services in the default namespace that reference # the service account called my-sa spec : ingress : - action : Allow source : serviceAccounts : selector : 'name == \"my-sa\"' selector : all() Incrementally add rules to selectively allow the flow of traffic between namespaces/pods \u00b6 Start by allowing Pods within a Namespace to communicate with each other and then add custom rules that further restrict Pod to Pod communication within that Namespace. Log network traffic metadata \u00b6 AWS VPC Flow Logs captures metadata about the traffic flowing through a VPC, such as source and destination IP address and port along with accepted/dropped packets. This information could be analyzed to look for suspicous or unusual activity between resources within the VPC, including Pods. However, since the IP addresses of pods frequently change as they are replaced, Flow Logs may not be sufficient on its own. Calico Enterprise extends the Flow Logs with pod labels and other metadata, making it easier to decipher the traffic flows between pods. Use encryption with AWS load balancers \u00b6 The AWS Application Load Balancer (ALB) and Network Load Balancer (NLB) both have support for transport encryption (SSL and TLS). The alb.ingress.kubernetes.io/certificate-arn annotation for the ALB lets you to specify which certificates to add to the ALB. If you omit the annotation the controller will attempt to add certificates to listeners that require it by matching the available AWS Certificate Manager (ACM) certificates using the host field. Starting with EKS v1.15 you can use the service.beta.kubernetes.io/aws-load-balancer-ssl-cert annotation with the NLB as shown in the example below. apiVersion : v1 kind : Service metadata : name : demo-app namespace : default labels : app : demo-app annotations : service.beta.kubernetes.io/aws-load-balancer-type : \"nlb\" service.beta.kubernetes.io/aws-load-balancer-ssl-cert : \"<certificate ARN>\" service.beta.kubernetes.io/aws-load-balancer-ssl-ports : \"443\" service.beta.kubernetes.io/aws-load-balancer-backend-protocol : \"http\" spec : type : LoadBalancer ports : - port : 443 targetPort : 80 protocol : TCP selector : app : demo-app --- kind : Deployment apiVersion : apps/v1 metadata : name : nginx namespace : default labels : app : demo-app spec : replicas : 1 selector : matchLabels : app : demo-app template : metadata : labels : app : demo-app spec : containers : - name : nginx image : nginx ports : - containerPort : 443 protocol : TCP - containerPort : 80 protocol : TCP Additional Resources \u00b6 Kubernetes & Tigera: Network Policies, Security, and Audit Calico Enterprise Cilium Kinvolk's Network Policy Advisor Suggests network policies based on an analysis of network traffic Security groups \u00b6 EKS uses AWS VPC Security Groups (SGs) to control the traffic between the Kubernetes control plane and the cluster's worker nodes. Security groups are also used to control the traffic between worker nodes, and other VPC resources, and external IP addresses. When you provision an EKS cluster (with Kubernetes version 1.14-eks.3 or greater), a cluster security group is automatically created for you. This security group allows unfettered communication between the EKS control plane and the nodes from managed node groups. For simplicity, it is recommended that you add the cluster SG to all node groups, including unmanaged node groups. Prior to Kubernetes version 1.14 and EKS version eks.3, there were separate security groups configured for the EKS control plane and node groups. The minimum and suggested rules for the control plane and node group security groups can be found at https://docs.aws.amazon.com/eks/latest/userguide/sec-group-reqs.html . The minimum rules for the control plane security group allows port 443 inbound from the worker node SG. This rule is what allows the kubelets to communicate with the Kubernetes API server. It also includes port 10250 for outbound traffic to the worker node SG; 10250 is the port that the kubelets listen on. Similarly, the minimum node group rules allow port 10250 inbound from the control plane SG and 443 outbound to the control plane SG. Finally there is a rule that allows unfettered communication between nodes within a node group. If you need to control communication between services that run within the cluster and service the run outside the cluster such as an RDS database, consider security groups for pods . With security groups for pods, you can assign an existing security group to a collection of pods. Warning If you reference a security group that does not exist prior to the creation of the pods, the pods will not get scheduled. You can control which pods are assigned to a security group by creating a SecurityGroupPolicy object and specifying a PodSelector or a ServiceAccountSelector . Setting the selectors to {} will assign the SGs referenced in the SecurityGroupPolicy to all pods in a namespace or all Service Accounts in a namespace. Be sure you've familiarized youself with all the considerations before implementing security groups for pods. Important If you use SGs for pods you must create SGs that allow port 53 outbound to the cluster security group. Similarly, you must update the cluster security group to accept port 53 inbound traffic from the pod security group. Important The limits for security groups still apply when using security groups for pods so use them judiciously. Important You must create rules for inbound traffic from the cluster security group (kubelet) for all of the probes configured for pod. Warning There is a bug that currently prevents the kublet from communicating with pods that are assigned to SGs. The current workaround involves running sudo sysctl net.ipv4.tcp_early_demux=0 on the affected worker nodes. This is fixed in CNI v1.7.3, https://github.com/aws/amazon-vpc-cni-k8s/releases/tag/v1.7.3 . Important Security groups for pods relies on a feature known as ENI trunking which was created to increase the ENI density of an EC2 instance. When a pod is assigned to an SG, a VPC controller associates a branch ENI from the node group with the pod. If there aren't enough branch ENIs available in a node group at the time the pod is scheduled, the pod will stay in pending state. The number of branch ENIs an instance can support varies by instance type/family. See https://docs.aws.amazon.com/eks/latest/userguide/security-groups-for-pods.html#supported-instance-types for further details. While security groups for pods offers an AWS-native way to control network traffic within and outside of your cluster without the overhead of a policy daemon, other options are available. For example, the Cilium policy engine allows you to reference a DNS name in a network policy. Calico Enterprise includes an option for mapping network policies to AWS security groups. If you've implemented a service mesh like Istio, you can use an egress gateway to restrict network egress to specific, fully qualified domains or IP addresses. For further infomration about this option, read the three part series on egress traffic control in Istio . Encryption in transit \u00b6 Applications that need to conform to PCI, HIPAA, or other regulations may need to encrypt data while it is in transit. Nowadays TLS is the de facto choice for encrypting traffic on the wire. TLS, like it's predecessor SSL, provides secure communications over a network using cryptographic protocols. TLS uses symmetric encryption where the keys to encrypt the data are generated based on a shared secret that is negotiated at the beginning of the session. The following are a few ways that you can encrypt data in a Kubernetes environment. Nitro Instances \u00b6 Traffic exchanged between the following Nitro instance types C5n, G4, I3en, M5dn, M5n, P3dn, R5dn, and R5n, is automatically encrypted by default. When there's an intermediate hop, like a transit gateway or a load balancer, the traffic is not encrypted. See Encryption in transit and the following What's new announcement for further details. Container Network Interfaces (CNIs) \u00b6 WeaveNet can be configured to automatically encrypt all traffic using NaCl encryption for sleeve traffic, and IPsec ESP for fast datapath traffic. Service Mesh \u00b6 Encryption in transit can also be implemented with a service mesh like App Mesh, Linkerd v2, and Istio. Currently, App Mesh supports TLS encryption with a private certificate issued by AWS Certificate Manager (ACM) or a certificate stored on the local file system of the virtual node. Linkerd and Istio both have support for mTLS which adds another layer of security through mutual exchange and validation of certificates. The aws-app-mesh-examples GitHub repository provides walkthroughs for configuring TLS using certificates issued by ACM and certificates that are packaged with your Envoy container: + Configuring TLS with File Provided TLS Certificates + Configuring TLS with AWS Certificate Manager Ingress Controllers and Load Balancers \u00b6 Ingress controllers are a way for you to intelligently route HTTP/S traffic that emanates from outside the cluster to services running inside the cluster. Oftentimes, these Ingresses are fronted by a layer 4 load balancer, like the Classic Load Balancer or the Network Load Balancer (NLB). Encrypted traffic can be terminated at different places within the network, e.g. at the load balancer, at the ingress resource, or the Pod. How and where you terminate your SSL connection will ultimately be dictated by your organization's network security policy. For instance, if you have a policy that requires end-to-end encryption, you will have to decrypt the traffic at the Pod. This will place additional burden on your Pod as it will have to spend cycles establishing the initial handshake. Overall SSL/TLS processing is very CPU intensive. Consequently, if you have the flexibility, try performing the SSL offload at the Ingress or the load balancer. An ingress controller can be configured to terminate SSL/TLS connections. An example for how to terminate SSL/TLS connections at the NLB appears above . Additional examples for SSL/TLS termination appear below. Securing EKS Ingress With Contour And Let\u2019s Encrypt The GitOps Way How do I terminate HTTPS traffic on Amazon EKS workloads with ACM? Attention Some Ingresses, like the ALB ingress controller, implement the SSL/TLS using Annotations instead of as part of the Ingress Spec. Tooling \u00b6 Verifying Service Mesh TLS in Kubernetes, Using ksniff and Wireshark ksniff egress-operator An operator and DNS plugin to control egress traffic from your cluster without protocol inspection","title":"Network Security"},{"location":"network/#network-security","text":"Network security has several facets. The first involves the application of rules which restrict the flow of network traffic between services. The second involves the encryption of traffic while it is in transit. The mechanisms to implement these security measures on EKS are varied but often include the following items:","title":"Network security"},{"location":"network/#traffic-control","text":"Network Policies Security Groups","title":"Traffic control"},{"location":"network/#encryption-in-transit","text":"Service Mesh Container Network Interfaces (CNIs) Nitro Instances","title":"Encryption in transit"},{"location":"network/#network-policy","text":"Within a Kubernetes cluster, all Pod to Pod communication is allowed by default. While this flexibility may help promote experimentation, it is not considered secure. Kubernetes network policies give you a mechanism to restrict network traffic between Pods (often referred to as East/West traffic) and between Pods and external services. Kubernetes network policies operate at layers 3 and 4 of the OSI model. Network policies use pod selectors and labels to identify source and destination pods, but can also include IP addresses, port numbers, protocol number, or a combination of these. Calico , is an open source policy engine from Tigera that works well with EKS. In addition to implementing the full set of Kubernetes network policy features, Calico supports extended network polices with a richer set of features, including support for layer 7 rules, e.g. HTTP, when integrated with Istio. Isovalent, the maintainers of Cilium , have also extended the network policies to include partial support for layer 7 rules, e.g. HTTP. Cilium also has support for DNS hostnames which can be useful for restricting traffic between Kubernetes Services/Pods and resources that run within or outside of your VPC. By contrast, Calico Enterprise includes a feature that allows you to map a Kubernetes network policy to an AWS security group, as well as DNS hostnames. Attention When you first provision an EKS cluster, the Calico policy engine is not installed by default. The manifests for installing Calico can be found in the VPC CNI repository at https://github.com/aws/amazon-vpc-cni-k8s/tree/master/config . Calico policies can be scoped to Namespaces, Pods, service accounts, or globally. When policies are scoped to a service account, it associates a set of ingress/egress rules with that service account. With the proper RBAC rules in place, you can prevent teams from overriding these rules, allowing IT security professionals to safely delegate administration of namespaces. You can find a list of common Kubernetes network policies at https://github.com/ahmetb/kubernetes-network-policy-recipes . A similar set of rules for Calico are available at https://docs.projectcalico.org/security/calico-network-policy .","title":"Network policy"},{"location":"network/#recommendations","text":"","title":"Recommendations"},{"location":"network/#create-a-default-deny-policy","text":"As with RBAC policies, network policies should adhere to the policy of least privileged access. Start by creating a deny all policy that restricts all inbound and outbound traffic from a namespace or create a global policy using Calico. Kubernetes network policy apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : default-deny namespace : default spec : podSelector : {} policyTypes : - Ingress - Egress Tip The image above was created by the network policy viewer from Tufin . Calico global network policy apiVersion : crd.projectcalico.org/v1 kind : GlobalNetworkPolicy metadata : name : default-deny spec : selector : all() types : - Ingress - Egress","title":"Create a default deny policy"},{"location":"network/#create-a-rule-to-allow-dns-queries","text":"Once you have the default deny all rule in place, you can begin layering on additional rules, such as a global rule that allows pods to query CoreDNS for name resolution. You begin by labeling the namespace: kubectl label namespace kube-system name=kube-system Then add the network policy: apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : allow-dns-access namespace : default spec : podSelector : matchLabels : {} policyTypes : - Egress egress : - to : - namespaceSelector : matchLabels : name : kube-system ports : - protocol : UDP port : 53 Calico global policy equivalent apiVersion : crd.projectcalico.org/v1 kind : GlobalNetworkPolicy metadata : name : allow-dns-egress spec : selector : all() types : - Egress egress : - action : Allow protocol : UDP destination : namespaceSelector : name == \"kube-system\" ports : - 53 The following is an example of how to associate a network policy with a service account while preventing users associated with the readonly-sa-group from editing the service account my-sa in the default namespace: apiVersion : v1 kind : ServiceAccount metadata : name : my-sa namespace : default labels : name : my-sa --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : namespace : default name : readonly-sa-role rules : # Allows the subject to read a service account called my-sa - apiGroups : [ \"\" ] resources : [ \"serviceaccounts\" ] resourceNames : [ \"my-sa\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : namespace : default name : readonly-sa-rolebinding # Binds the readonly-sa-role to the RBAC group called readonly-sa-group. subjects : - kind : Group name : readonly-sa-group apiGroup : rbac.authorization.k8s.io roleRef : kind : Role name : readonly-sa-role apiGroup : rbac.authorization.k8s.io --- apiVersion : crd.projectcalico.org/v1 kind : NetworkPolicy metadata : name : netpol-sa-demo namespace : default # Allows all ingress traffic to services in the default namespace that reference # the service account called my-sa spec : ingress : - action : Allow source : serviceAccounts : selector : 'name == \"my-sa\"' selector : all()","title":"Create a rule to allow DNS queries"},{"location":"network/#incrementally-add-rules-to-selectively-allow-the-flow-of-traffic-between-namespacespods","text":"Start by allowing Pods within a Namespace to communicate with each other and then add custom rules that further restrict Pod to Pod communication within that Namespace.","title":"Incrementally add rules to selectively allow the flow of traffic between namespaces/pods"},{"location":"network/#log-network-traffic-metadata","text":"AWS VPC Flow Logs captures metadata about the traffic flowing through a VPC, such as source and destination IP address and port along with accepted/dropped packets. This information could be analyzed to look for suspicous or unusual activity between resources within the VPC, including Pods. However, since the IP addresses of pods frequently change as they are replaced, Flow Logs may not be sufficient on its own. Calico Enterprise extends the Flow Logs with pod labels and other metadata, making it easier to decipher the traffic flows between pods.","title":"Log network traffic metadata"},{"location":"network/#use-encryption-with-aws-load-balancers","text":"The AWS Application Load Balancer (ALB) and Network Load Balancer (NLB) both have support for transport encryption (SSL and TLS). The alb.ingress.kubernetes.io/certificate-arn annotation for the ALB lets you to specify which certificates to add to the ALB. If you omit the annotation the controller will attempt to add certificates to listeners that require it by matching the available AWS Certificate Manager (ACM) certificates using the host field. Starting with EKS v1.15 you can use the service.beta.kubernetes.io/aws-load-balancer-ssl-cert annotation with the NLB as shown in the example below. apiVersion : v1 kind : Service metadata : name : demo-app namespace : default labels : app : demo-app annotations : service.beta.kubernetes.io/aws-load-balancer-type : \"nlb\" service.beta.kubernetes.io/aws-load-balancer-ssl-cert : \"<certificate ARN>\" service.beta.kubernetes.io/aws-load-balancer-ssl-ports : \"443\" service.beta.kubernetes.io/aws-load-balancer-backend-protocol : \"http\" spec : type : LoadBalancer ports : - port : 443 targetPort : 80 protocol : TCP selector : app : demo-app --- kind : Deployment apiVersion : apps/v1 metadata : name : nginx namespace : default labels : app : demo-app spec : replicas : 1 selector : matchLabels : app : demo-app template : metadata : labels : app : demo-app spec : containers : - name : nginx image : nginx ports : - containerPort : 443 protocol : TCP - containerPort : 80 protocol : TCP","title":"Use encryption with AWS load balancers"},{"location":"network/#additional-resources","text":"Kubernetes & Tigera: Network Policies, Security, and Audit Calico Enterprise Cilium Kinvolk's Network Policy Advisor Suggests network policies based on an analysis of network traffic","title":"Additional Resources"},{"location":"network/#security-groups","text":"EKS uses AWS VPC Security Groups (SGs) to control the traffic between the Kubernetes control plane and the cluster's worker nodes. Security groups are also used to control the traffic between worker nodes, and other VPC resources, and external IP addresses. When you provision an EKS cluster (with Kubernetes version 1.14-eks.3 or greater), a cluster security group is automatically created for you. This security group allows unfettered communication between the EKS control plane and the nodes from managed node groups. For simplicity, it is recommended that you add the cluster SG to all node groups, including unmanaged node groups. Prior to Kubernetes version 1.14 and EKS version eks.3, there were separate security groups configured for the EKS control plane and node groups. The minimum and suggested rules for the control plane and node group security groups can be found at https://docs.aws.amazon.com/eks/latest/userguide/sec-group-reqs.html . The minimum rules for the control plane security group allows port 443 inbound from the worker node SG. This rule is what allows the kubelets to communicate with the Kubernetes API server. It also includes port 10250 for outbound traffic to the worker node SG; 10250 is the port that the kubelets listen on. Similarly, the minimum node group rules allow port 10250 inbound from the control plane SG and 443 outbound to the control plane SG. Finally there is a rule that allows unfettered communication between nodes within a node group. If you need to control communication between services that run within the cluster and service the run outside the cluster such as an RDS database, consider security groups for pods . With security groups for pods, you can assign an existing security group to a collection of pods. Warning If you reference a security group that does not exist prior to the creation of the pods, the pods will not get scheduled. You can control which pods are assigned to a security group by creating a SecurityGroupPolicy object and specifying a PodSelector or a ServiceAccountSelector . Setting the selectors to {} will assign the SGs referenced in the SecurityGroupPolicy to all pods in a namespace or all Service Accounts in a namespace. Be sure you've familiarized youself with all the considerations before implementing security groups for pods. Important If you use SGs for pods you must create SGs that allow port 53 outbound to the cluster security group. Similarly, you must update the cluster security group to accept port 53 inbound traffic from the pod security group. Important The limits for security groups still apply when using security groups for pods so use them judiciously. Important You must create rules for inbound traffic from the cluster security group (kubelet) for all of the probes configured for pod. Warning There is a bug that currently prevents the kublet from communicating with pods that are assigned to SGs. The current workaround involves running sudo sysctl net.ipv4.tcp_early_demux=0 on the affected worker nodes. This is fixed in CNI v1.7.3, https://github.com/aws/amazon-vpc-cni-k8s/releases/tag/v1.7.3 . Important Security groups for pods relies on a feature known as ENI trunking which was created to increase the ENI density of an EC2 instance. When a pod is assigned to an SG, a VPC controller associates a branch ENI from the node group with the pod. If there aren't enough branch ENIs available in a node group at the time the pod is scheduled, the pod will stay in pending state. The number of branch ENIs an instance can support varies by instance type/family. See https://docs.aws.amazon.com/eks/latest/userguide/security-groups-for-pods.html#supported-instance-types for further details. While security groups for pods offers an AWS-native way to control network traffic within and outside of your cluster without the overhead of a policy daemon, other options are available. For example, the Cilium policy engine allows you to reference a DNS name in a network policy. Calico Enterprise includes an option for mapping network policies to AWS security groups. If you've implemented a service mesh like Istio, you can use an egress gateway to restrict network egress to specific, fully qualified domains or IP addresses. For further infomration about this option, read the three part series on egress traffic control in Istio .","title":"Security groups"},{"location":"network/#encryption-in-transit_1","text":"Applications that need to conform to PCI, HIPAA, or other regulations may need to encrypt data while it is in transit. Nowadays TLS is the de facto choice for encrypting traffic on the wire. TLS, like it's predecessor SSL, provides secure communications over a network using cryptographic protocols. TLS uses symmetric encryption where the keys to encrypt the data are generated based on a shared secret that is negotiated at the beginning of the session. The following are a few ways that you can encrypt data in a Kubernetes environment.","title":"Encryption in transit"},{"location":"network/#nitro-instances","text":"Traffic exchanged between the following Nitro instance types C5n, G4, I3en, M5dn, M5n, P3dn, R5dn, and R5n, is automatically encrypted by default. When there's an intermediate hop, like a transit gateway or a load balancer, the traffic is not encrypted. See Encryption in transit and the following What's new announcement for further details.","title":"Nitro Instances"},{"location":"network/#container-network-interfaces-cnis","text":"WeaveNet can be configured to automatically encrypt all traffic using NaCl encryption for sleeve traffic, and IPsec ESP for fast datapath traffic.","title":"Container Network Interfaces (CNIs)"},{"location":"network/#service-mesh","text":"Encryption in transit can also be implemented with a service mesh like App Mesh, Linkerd v2, and Istio. Currently, App Mesh supports TLS encryption with a private certificate issued by AWS Certificate Manager (ACM) or a certificate stored on the local file system of the virtual node. Linkerd and Istio both have support for mTLS which adds another layer of security through mutual exchange and validation of certificates. The aws-app-mesh-examples GitHub repository provides walkthroughs for configuring TLS using certificates issued by ACM and certificates that are packaged with your Envoy container: + Configuring TLS with File Provided TLS Certificates + Configuring TLS with AWS Certificate Manager","title":"Service Mesh"},{"location":"network/#ingress-controllers-and-load-balancers","text":"Ingress controllers are a way for you to intelligently route HTTP/S traffic that emanates from outside the cluster to services running inside the cluster. Oftentimes, these Ingresses are fronted by a layer 4 load balancer, like the Classic Load Balancer or the Network Load Balancer (NLB). Encrypted traffic can be terminated at different places within the network, e.g. at the load balancer, at the ingress resource, or the Pod. How and where you terminate your SSL connection will ultimately be dictated by your organization's network security policy. For instance, if you have a policy that requires end-to-end encryption, you will have to decrypt the traffic at the Pod. This will place additional burden on your Pod as it will have to spend cycles establishing the initial handshake. Overall SSL/TLS processing is very CPU intensive. Consequently, if you have the flexibility, try performing the SSL offload at the Ingress or the load balancer. An ingress controller can be configured to terminate SSL/TLS connections. An example for how to terminate SSL/TLS connections at the NLB appears above . Additional examples for SSL/TLS termination appear below. Securing EKS Ingress With Contour And Let\u2019s Encrypt The GitOps Way How do I terminate HTTPS traffic on Amazon EKS workloads with ACM? Attention Some Ingresses, like the ALB ingress controller, implement the SSL/TLS using Annotations instead of as part of the Ingress Spec.","title":"Ingress Controllers and Load Balancers"},{"location":"network/#tooling","text":"Verifying Service Mesh TLS in Kubernetes, Using ksniff and Wireshark ksniff egress-operator An operator and DNS plugin to control egress traffic from your cluster without protocol inspection","title":"Tooling"},{"location":"pods/","text":"Pod Security \u00b6 Pods have a variety of different settings that can strengthen or weaken your overall security posture. As a Kubernetes practitioner your chief concern should be preventing a process that\u2019s running in a container from escaping the isolation boundaries of Docker and gaining access to the underlying host. The reason for this is twofold. First, the processes that run within a container run under the context of the [Linux] root user by default. Although the actions of root within a container are partially constrained by the set of Linux capabilities that Docker assigns to the containers, these default privileges could allow an attacker to escalate their privileges and/or gain access to sensitive information bound to the host, including Secrets and ConfigMaps. Below is a list of the default capabilities assigned to Docker containers. For additional information about each capability, see http://man7.org/linux/man-pages/man7/capabilities.7.html . CAP_CHOWN, CAP_DAC_OVERERIDE, CAP_FOWNER, CAP_FSETID, CAP_KILL, CAP_SETGID, CAP_SETUID, CAP_SETPCAP, CAP_NET_BIND_SERVICE, CAP_NET_RAW, CAP_SYS_CHROOT, CAP_MKNOD, CAP_AUDIT_WRITE, CAP_SETFCAP Info EC2 and Fargate pods are assigned the aforementioned capabilites by default. Additionally, Linux capabilities can only be dropped from Fargate pods. Pods that are run as privileged, inherit all of the Linux capabilities associated with root on the host and should be avoided if possible. Second, all Kubernetes worker nodes use an authorization mode called the node authorizer. The node authorizer authorizes all API requests that originate from the kubelet and allows nodes to perform the following actions: Read operations: services endpoints nodes pods secrets, configmaps, persistent volume claims and persistent volumes related to pods bound to the kubelet\u2019s node Write operations: nodes and node status (enable the NodeRestriction admission plugin to limit a kubelet to modify its own node) pods and pod status (enable the NodeRestriction admission plugin to limit a kubelet to modify pods bound to itself) events Auth-related operations: Read/write access to the CertificateSigningRequest (CSR) API for TLS bootstrapping the ability to create TokenReview and SubjectAccessReview for delegated authentication/authorization checks EKS uses the node restriction admission controller which only allows the node to modify a limited set of node attributes and pod objects that are bound to the node. Nevertheless, an attacker who manages to get access to the host will still be able to glean sensitive information about the environment from the Kubernetes API that could allow them to move laterally within the cluster. Recommendations \u00b6 Restrict the containers that can run as privileged \u00b6 As mentioned, containers that run as privileged inherit all of the Linux capabilities assigned to root on the host. Seldom do containers need these types of privileges to function properly. You can reject pods with containers configured to run as privileged by creating a pod security policy . You can think of a pod security policy as a set of requirements that pods have to meet before they can be created. If you elect to use pod security policies, you will need to create a role binding that allows service accounts to read your pod security policies. When you provision an EKS cluster, a pod security policy called eks.privileged is automatically created. The manifest for that policy appears below: apiVersion : policy/v1beta1 kind : PodSecurityPolicy metadata : annotations : kubernetes.io/description : privileged allows full unrestricted access to pod features, as if the PodSecurityPolicy controller was not enabled. seccomp.security.alpha.kubernetes.io/allowedProfileNames : '*' labels : eks.amazonaws.com/component : pod-security-policy kubernetes.io/cluster-service : \"true\" name : eks.privileged spec : allowPrivilegeEscalation : true allowedCapabilities : - '*' fsGroup : rule : RunAsAny hostIPC : true hostNetwork : true hostPID : true hostPorts : - max : 65535 min : 0 privileged : true runAsUser : rule : RunAsAny seLinux : rule : RunAsAny supplementalGroups : rule : RunAsAny volumes : - '*' This PSP allows an authenticated user to run privileged containers across all namespaces within the cluster. While this may seem overly permissive at first, there are certain applications/plug-ins such as the AWS VPC CNI and kube-proxy that have to run as privileged because they are responsible for configuring the host\u2019s network settings. Furthermore, this policy provides backward compatibility with earlier versions of Kubernetes that lacked support for pod security policies. The binding shown below is what binds the ClusterRole eks:podsecuritypolicy:privileged to the system:authenticated RBAC group. apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : annotations : kubernetes.io/description : Allow all authenticated users to create privileged labels : eks.amazonaws.com/component : pod-security-policy kubernetes.io/cluster-service : \"true\" name : eks:podsecuritypolicy:authenticated roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : eks:podsecuritypolicy:privileged subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : system:authenticated Lastly, the ClusterRole below allow all bindings that reference it to use the eks.privileged PodSecurityPolicy. apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : labels : eks.amazonaws.com/component : pod-security-policy kubernetes.io/cluster-service : \"true\" name : eks:podsecuritypolicy:privileged rules : - apiGroups : - policy resourceNames : - eks.privileged resources : - podsecuritypolicies verbs : - use As a best practice we recommend that you scope the binding for privileged pods to service accounts within a particular namespace, e.g. kube-system, and limiting access to that namespace. For all other serviceaccounts/namespaces, we recommend implementing a more restrictive policy such as this: apiVersion : policy/v1beta1 kind : PodSecurityPolicy metadata : name : restricted annotations : seccomp.security.alpha.kubernetes.io/allowedProfileNames : 'docker/default,runtime/default' apparmor.security.beta.kubernetes.io/allowedProfileNames : 'runtime/default' seccomp.security.alpha.kubernetes.io/defaultProfileName : 'runtime/default' apparmor.security.beta.kubernetes.io/defaultProfileName : 'runtime/default' spec : privileged : false # Required to prevent escalations to root. allowPrivilegeEscalation : false # This is redundant with non-root + disallow privilege escalation, # but we can provide it for defense in depth. requiredDropCapabilities : - ALL # Allow core volume types. volumes : - 'configMap' - 'emptyDir' - 'projected' - 'secret' - 'downwardAPI' # Assume that persistentVolumes set up by the cluster admin are safe to use. - 'persistentVolumeClaim' hostNetwork : false hostIPC : false hostPID : false runAsUser : # Require the container to run without root privileges. rule : 'MustRunAsNonRoot' seLinux : # This policy assumes the nodes are using AppArmor rather than SELinux. rule : 'RunAsAny' supplementalGroups : rule : 'MustRunAs' ranges : # Forbid adding the root group. - min : 1 max : 65535 fsGroup : rule : 'MustRunAs' ranges : # Forbid adding the root group. - min : 1 max : 65535 readOnlyRootFilesystem : false This policy prevents pods from running as privileged or escalating privileges. It also restricts the types of volumes that can be mounted and the root supplemental groups that can be added. Another, albeit similar, approach is to start with policy that locks everything down and incrementally add exceptions for applications that need looser restrictions such as logging agents which need the ability to mount a host path. You can learn more about this in a recent post on the Square engineering blog . Attention Fargate is a launch type that enables you to run \"serverless\" container(s) where the containers of a pod are run on infrastructure that AWS manages. With Fargate, you cannot run a privileged container or configure your pod to use hostNetwork or hostPort. Do not run processes in containers as root \u00b6 All containers run as root by default. This could be problematic if an attacker is able to exploit a vulnerability in the application and get shell access to the running container. You can mitigate this risk a variety of ways. First, by removing the shell from the container image. Second, adding the USER directive to your Dockerfile or running the containers in the pod as a non-root user. The Kubernetes podSpec includes a set of fields under spec.securityContext , that allow to let you specify the user and/or group to run your application as. These fields are runAsUser and runAsGroup respectively. You can mandate the use of these fields by creating a pod security policy. See https://kubernetes.io/docs/concepts/policy/pod-security-policy/#users-and-groups for further information on this topic. Never run Docker in Docker or mount the socket in the container \u00b6 While this conveniently lets you to build/run images in Docker containers, you're basically relinquishing complete control of the node to the process running in the container. If you need to build container images on Kubernetes use Kaniko , buildah , img , or a build service like CodeBuild instead. Restrict the use of hostPath or if hostPath is necessary restrict which prefixes can be used and configure the volume as read-only \u00b6 hostPath is a volume that mounts a directory from the host directly to the container. Rarely will pods need this type of access, but if they do, you need to be aware of the risks. By default pods that run as root will have write access to the file system exposed by hostPath. This could allow an attacker to modify the kubelet settings, create symbolic links to directories or files not directly exposed by the hostPath, e.g. /etc/shadow, install ssh keys, read secrets mounted to the host, and other malicious things. To mitigate the risks from hostPath, configure the spec.containers.volumeMounts as readOnly , for example: volumeMounts : - name : hostPath-volume readOnly : true mountPath : /host-path You should also use a pod security policy to restrict the directories that can be used by hostPath volumes. For example the following PSP excerpt only allows paths that begin with /foo . It will prevent containers from traversing the host file system from outside the prefix: allowedHostPaths : # This allows \"/foo\", \"/foo/\", \"/foo/bar\" etc., but # disallows \"/fool\", \"/etc/foo\" etc. # \"/foo/../\" is never valid. - pathPrefix : \"/foo\" readOnly : true # only allow read-only mounts Set requests and limits for each container to avoid resource contention and DoS attacks \u00b6 A pod without requests or limits can theoretically consume all of the resources available on a host. As additional pods are scheduled onto a node, the node may experience CPU or memory pressure which can cause the Kubelet to terminate or evict pods from the node. While you can\u2019t prevent this from happening all together, setting requests and limits will help minimize resource contention and mitigate the risk from poorly written applications that consume an excessive amount of resources. The podSpec allows you to specify requests and limits for CPU and memory. CPU is considered a compressible resource because it can be oversubscribed. Memory is incompressible, i.e. it cannot be shared among multiple containers. When you specify requests for CPU or memory, you\u2019re essentially designating the amount of memory that containers are guaranteed to get. Kubernetes aggregates the requests of all the containers in a pod to determine which node to schedule the pod onto. If a container exceeds the requested amount of memory it may be subject to termination if there\u2019s memory pressure on the node. Limits are the maximum amount of CPU and memory resources that a container is allowed to consume and directly corresponds to the memory.limit_in_bytes value of the cgroup created for the container. A container that exceeds the memory limit will be OOM killed. If a container exceeds its CPU limit, it will be throttled. Kubernetes uses three Quality of Service (QoS) classes to prioritize the workloads running on a node. These include: guaranteed, burstable, and best-effort. If limits and requests are not set, the pod is configured as best-effort (lowest priority). Best-effort pods are the first to get killed when there is insufficient memory. If limits are set on all containers within the pod, or if the requests and limits are set to the same values and not equal to 0, the pod is configured as guaranteed (highest priority). Guaranteed pods will not be killed unless they exceed their configured memory limits. If the limits and requests are configured with different values and not equal to 0, or one container within the pod sets limits and the others don\u2019t or have limits set for different resources, the pods are configured as burstable (medium priority). These pods have some resource guarantees, but can be killed once they exceed their requested memory. Attention Requests don't affect the memory_limit_in_bytes value of the container's cgroup; the cgroup limit is set to the amount of memory available on the host. Nevertheless, setting the requests value too low could cause the pod to be targeted for termination by the kubelet if the node undergoes memory pressure. Class Priority Condition Kill Condition Guaranteed highest limit = request != 0 Only exceed memory limits Burstable medium limit != request != 0 Can be killed if exceed request memory Best-Effort lowest limit & request Not Set First to get killed when there's insufficient menory For additional information about resource QoS, please refer to the Kubernetes documentation . You can force the use of requests and limits by setting a resource quota on a namespace or by creating a limit range . A resource quota allows you to specify the total amount of resources, e.g. CPU and RAM, allocated to a namespace. When it\u2019s applied to a namespace, it forces you to specify requests and limits for all containers deployed into that namespace. By contrast, limit ranges give you more granular control of the allocation of resources. With limit ranges you can min/max for CPU and memory resources per pod or per container within a namespace. You can also use them to set default request/limit values if none are provided. Do not allow privileged escalation \u00b6 Privileged escalation allows a process to change the security context under which its running. Sudo is a good example of this as are binaries with the SUID or SGID bit. Privileged escalation is basically a way for users to execute a file with the permissions of another user or group. You can prevent a container from using privileged escalation by implementing a pod security policy that sets allowPriviledgedEscalation to false or by setting securityContext.allowPrivilegedEscalation in the podSpec . Tools \u00b6 kube-psp-advisor is a tool that makes it easier to create K8s Pod Security Policies (PSPs) from either a live K8s environment or from a single .yaml file containing a pod specification (Deployment, DaemonSet, Pod, etc).","title":"Pod Security"},{"location":"pods/#pod-security","text":"Pods have a variety of different settings that can strengthen or weaken your overall security posture. As a Kubernetes practitioner your chief concern should be preventing a process that\u2019s running in a container from escaping the isolation boundaries of Docker and gaining access to the underlying host. The reason for this is twofold. First, the processes that run within a container run under the context of the [Linux] root user by default. Although the actions of root within a container are partially constrained by the set of Linux capabilities that Docker assigns to the containers, these default privileges could allow an attacker to escalate their privileges and/or gain access to sensitive information bound to the host, including Secrets and ConfigMaps. Below is a list of the default capabilities assigned to Docker containers. For additional information about each capability, see http://man7.org/linux/man-pages/man7/capabilities.7.html . CAP_CHOWN, CAP_DAC_OVERERIDE, CAP_FOWNER, CAP_FSETID, CAP_KILL, CAP_SETGID, CAP_SETUID, CAP_SETPCAP, CAP_NET_BIND_SERVICE, CAP_NET_RAW, CAP_SYS_CHROOT, CAP_MKNOD, CAP_AUDIT_WRITE, CAP_SETFCAP Info EC2 and Fargate pods are assigned the aforementioned capabilites by default. Additionally, Linux capabilities can only be dropped from Fargate pods. Pods that are run as privileged, inherit all of the Linux capabilities associated with root on the host and should be avoided if possible. Second, all Kubernetes worker nodes use an authorization mode called the node authorizer. The node authorizer authorizes all API requests that originate from the kubelet and allows nodes to perform the following actions: Read operations: services endpoints nodes pods secrets, configmaps, persistent volume claims and persistent volumes related to pods bound to the kubelet\u2019s node Write operations: nodes and node status (enable the NodeRestriction admission plugin to limit a kubelet to modify its own node) pods and pod status (enable the NodeRestriction admission plugin to limit a kubelet to modify pods bound to itself) events Auth-related operations: Read/write access to the CertificateSigningRequest (CSR) API for TLS bootstrapping the ability to create TokenReview and SubjectAccessReview for delegated authentication/authorization checks EKS uses the node restriction admission controller which only allows the node to modify a limited set of node attributes and pod objects that are bound to the node. Nevertheless, an attacker who manages to get access to the host will still be able to glean sensitive information about the environment from the Kubernetes API that could allow them to move laterally within the cluster.","title":"Pod Security"},{"location":"pods/#recommendations","text":"","title":"Recommendations"},{"location":"pods/#restrict-the-containers-that-can-run-as-privileged","text":"As mentioned, containers that run as privileged inherit all of the Linux capabilities assigned to root on the host. Seldom do containers need these types of privileges to function properly. You can reject pods with containers configured to run as privileged by creating a pod security policy . You can think of a pod security policy as a set of requirements that pods have to meet before they can be created. If you elect to use pod security policies, you will need to create a role binding that allows service accounts to read your pod security policies. When you provision an EKS cluster, a pod security policy called eks.privileged is automatically created. The manifest for that policy appears below: apiVersion : policy/v1beta1 kind : PodSecurityPolicy metadata : annotations : kubernetes.io/description : privileged allows full unrestricted access to pod features, as if the PodSecurityPolicy controller was not enabled. seccomp.security.alpha.kubernetes.io/allowedProfileNames : '*' labels : eks.amazonaws.com/component : pod-security-policy kubernetes.io/cluster-service : \"true\" name : eks.privileged spec : allowPrivilegeEscalation : true allowedCapabilities : - '*' fsGroup : rule : RunAsAny hostIPC : true hostNetwork : true hostPID : true hostPorts : - max : 65535 min : 0 privileged : true runAsUser : rule : RunAsAny seLinux : rule : RunAsAny supplementalGroups : rule : RunAsAny volumes : - '*' This PSP allows an authenticated user to run privileged containers across all namespaces within the cluster. While this may seem overly permissive at first, there are certain applications/plug-ins such as the AWS VPC CNI and kube-proxy that have to run as privileged because they are responsible for configuring the host\u2019s network settings. Furthermore, this policy provides backward compatibility with earlier versions of Kubernetes that lacked support for pod security policies. The binding shown below is what binds the ClusterRole eks:podsecuritypolicy:privileged to the system:authenticated RBAC group. apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : annotations : kubernetes.io/description : Allow all authenticated users to create privileged labels : eks.amazonaws.com/component : pod-security-policy kubernetes.io/cluster-service : \"true\" name : eks:podsecuritypolicy:authenticated roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : eks:podsecuritypolicy:privileged subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : system:authenticated Lastly, the ClusterRole below allow all bindings that reference it to use the eks.privileged PodSecurityPolicy. apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : labels : eks.amazonaws.com/component : pod-security-policy kubernetes.io/cluster-service : \"true\" name : eks:podsecuritypolicy:privileged rules : - apiGroups : - policy resourceNames : - eks.privileged resources : - podsecuritypolicies verbs : - use As a best practice we recommend that you scope the binding for privileged pods to service accounts within a particular namespace, e.g. kube-system, and limiting access to that namespace. For all other serviceaccounts/namespaces, we recommend implementing a more restrictive policy such as this: apiVersion : policy/v1beta1 kind : PodSecurityPolicy metadata : name : restricted annotations : seccomp.security.alpha.kubernetes.io/allowedProfileNames : 'docker/default,runtime/default' apparmor.security.beta.kubernetes.io/allowedProfileNames : 'runtime/default' seccomp.security.alpha.kubernetes.io/defaultProfileName : 'runtime/default' apparmor.security.beta.kubernetes.io/defaultProfileName : 'runtime/default' spec : privileged : false # Required to prevent escalations to root. allowPrivilegeEscalation : false # This is redundant with non-root + disallow privilege escalation, # but we can provide it for defense in depth. requiredDropCapabilities : - ALL # Allow core volume types. volumes : - 'configMap' - 'emptyDir' - 'projected' - 'secret' - 'downwardAPI' # Assume that persistentVolumes set up by the cluster admin are safe to use. - 'persistentVolumeClaim' hostNetwork : false hostIPC : false hostPID : false runAsUser : # Require the container to run without root privileges. rule : 'MustRunAsNonRoot' seLinux : # This policy assumes the nodes are using AppArmor rather than SELinux. rule : 'RunAsAny' supplementalGroups : rule : 'MustRunAs' ranges : # Forbid adding the root group. - min : 1 max : 65535 fsGroup : rule : 'MustRunAs' ranges : # Forbid adding the root group. - min : 1 max : 65535 readOnlyRootFilesystem : false This policy prevents pods from running as privileged or escalating privileges. It also restricts the types of volumes that can be mounted and the root supplemental groups that can be added. Another, albeit similar, approach is to start with policy that locks everything down and incrementally add exceptions for applications that need looser restrictions such as logging agents which need the ability to mount a host path. You can learn more about this in a recent post on the Square engineering blog . Attention Fargate is a launch type that enables you to run \"serverless\" container(s) where the containers of a pod are run on infrastructure that AWS manages. With Fargate, you cannot run a privileged container or configure your pod to use hostNetwork or hostPort.","title":"Restrict the containers that can run as privileged"},{"location":"pods/#do-not-run-processes-in-containers-as-root","text":"All containers run as root by default. This could be problematic if an attacker is able to exploit a vulnerability in the application and get shell access to the running container. You can mitigate this risk a variety of ways. First, by removing the shell from the container image. Second, adding the USER directive to your Dockerfile or running the containers in the pod as a non-root user. The Kubernetes podSpec includes a set of fields under spec.securityContext , that allow to let you specify the user and/or group to run your application as. These fields are runAsUser and runAsGroup respectively. You can mandate the use of these fields by creating a pod security policy. See https://kubernetes.io/docs/concepts/policy/pod-security-policy/#users-and-groups for further information on this topic.","title":"Do not run processes in containers as root"},{"location":"pods/#never-run-docker-in-docker-or-mount-the-socket-in-the-container","text":"While this conveniently lets you to build/run images in Docker containers, you're basically relinquishing complete control of the node to the process running in the container. If you need to build container images on Kubernetes use Kaniko , buildah , img , or a build service like CodeBuild instead.","title":"Never run Docker in Docker or mount the socket in the container"},{"location":"pods/#restrict-the-use-of-hostpath-or-if-hostpath-is-necessary-restrict-which-prefixes-can-be-used-and-configure-the-volume-as-read-only","text":"hostPath is a volume that mounts a directory from the host directly to the container. Rarely will pods need this type of access, but if they do, you need to be aware of the risks. By default pods that run as root will have write access to the file system exposed by hostPath. This could allow an attacker to modify the kubelet settings, create symbolic links to directories or files not directly exposed by the hostPath, e.g. /etc/shadow, install ssh keys, read secrets mounted to the host, and other malicious things. To mitigate the risks from hostPath, configure the spec.containers.volumeMounts as readOnly , for example: volumeMounts : - name : hostPath-volume readOnly : true mountPath : /host-path You should also use a pod security policy to restrict the directories that can be used by hostPath volumes. For example the following PSP excerpt only allows paths that begin with /foo . It will prevent containers from traversing the host file system from outside the prefix: allowedHostPaths : # This allows \"/foo\", \"/foo/\", \"/foo/bar\" etc., but # disallows \"/fool\", \"/etc/foo\" etc. # \"/foo/../\" is never valid. - pathPrefix : \"/foo\" readOnly : true # only allow read-only mounts","title":"Restrict the use of hostPath or if hostPath is necessary restrict which prefixes can be used and configure the volume as read-only"},{"location":"pods/#set-requests-and-limits-for-each-container-to-avoid-resource-contention-and-dos-attacks","text":"A pod without requests or limits can theoretically consume all of the resources available on a host. As additional pods are scheduled onto a node, the node may experience CPU or memory pressure which can cause the Kubelet to terminate or evict pods from the node. While you can\u2019t prevent this from happening all together, setting requests and limits will help minimize resource contention and mitigate the risk from poorly written applications that consume an excessive amount of resources. The podSpec allows you to specify requests and limits for CPU and memory. CPU is considered a compressible resource because it can be oversubscribed. Memory is incompressible, i.e. it cannot be shared among multiple containers. When you specify requests for CPU or memory, you\u2019re essentially designating the amount of memory that containers are guaranteed to get. Kubernetes aggregates the requests of all the containers in a pod to determine which node to schedule the pod onto. If a container exceeds the requested amount of memory it may be subject to termination if there\u2019s memory pressure on the node. Limits are the maximum amount of CPU and memory resources that a container is allowed to consume and directly corresponds to the memory.limit_in_bytes value of the cgroup created for the container. A container that exceeds the memory limit will be OOM killed. If a container exceeds its CPU limit, it will be throttled. Kubernetes uses three Quality of Service (QoS) classes to prioritize the workloads running on a node. These include: guaranteed, burstable, and best-effort. If limits and requests are not set, the pod is configured as best-effort (lowest priority). Best-effort pods are the first to get killed when there is insufficient memory. If limits are set on all containers within the pod, or if the requests and limits are set to the same values and not equal to 0, the pod is configured as guaranteed (highest priority). Guaranteed pods will not be killed unless they exceed their configured memory limits. If the limits and requests are configured with different values and not equal to 0, or one container within the pod sets limits and the others don\u2019t or have limits set for different resources, the pods are configured as burstable (medium priority). These pods have some resource guarantees, but can be killed once they exceed their requested memory. Attention Requests don't affect the memory_limit_in_bytes value of the container's cgroup; the cgroup limit is set to the amount of memory available on the host. Nevertheless, setting the requests value too low could cause the pod to be targeted for termination by the kubelet if the node undergoes memory pressure. Class Priority Condition Kill Condition Guaranteed highest limit = request != 0 Only exceed memory limits Burstable medium limit != request != 0 Can be killed if exceed request memory Best-Effort lowest limit & request Not Set First to get killed when there's insufficient menory For additional information about resource QoS, please refer to the Kubernetes documentation . You can force the use of requests and limits by setting a resource quota on a namespace or by creating a limit range . A resource quota allows you to specify the total amount of resources, e.g. CPU and RAM, allocated to a namespace. When it\u2019s applied to a namespace, it forces you to specify requests and limits for all containers deployed into that namespace. By contrast, limit ranges give you more granular control of the allocation of resources. With limit ranges you can min/max for CPU and memory resources per pod or per container within a namespace. You can also use them to set default request/limit values if none are provided.","title":"Set requests and limits for each container to avoid resource contention and DoS attacks"},{"location":"pods/#do-not-allow-privileged-escalation","text":"Privileged escalation allows a process to change the security context under which its running. Sudo is a good example of this as are binaries with the SUID or SGID bit. Privileged escalation is basically a way for users to execute a file with the permissions of another user or group. You can prevent a container from using privileged escalation by implementing a pod security policy that sets allowPriviledgedEscalation to false or by setting securityContext.allowPrivilegedEscalation in the podSpec .","title":"Do not allow privileged escalation"},{"location":"pods/#tools","text":"kube-psp-advisor is a tool that makes it easier to create K8s Pod Security Policies (PSPs) from either a live K8s environment or from a single .yaml file containing a pod specification (Deployment, DaemonSet, Pod, etc).","title":"Tools"},{"location":"runtime/","text":"Runtime security \u00b6 Runtime security provides active protection for your containers while they're running. The idea is to detect and/or prevent malicious activity from occuring inside the container. With secure computing (seccomp) you can prevent a containerized application from making certain syscalls to the underlying host operating system's kernel. While the Linux operating system has a few hundred system calls, the lion's share of them are not necessary for running containers. By restricting what syscalls can be made by a container, you can effectively decrease your application's attack surface. To get started with seccomp, use strace to generate a stack trace to see which system calls your application is making, then use a tool such as syscall2seccomp to create a seccomp profile from the data gathered from the trace. Unlike SELinux, seccomp was not designed to isolate containers from each other, however, it will protect the host kernel from unauthorized syscalls. It works by intercepting syscalls and only allowing those that have been whitelisted to pass through. Docker has a default seccomp profile which is suitable for a majority of general purpose workloads. You can configure your container or Pod to use this profile by adding the following annotation to your container's or Pod's spec (pre-1.19): annotations : seccomp . security . alpha . kubernetes . io /pod: \"runtime/ default \" 1.19 and later: securityContext : seccompProfile : type : RuntimeDefault It's also possible to create your own profiles for things that require additional privileges. Caution seccomp profiles are a Kubelet alpha feature. You'll need to add the --seccomp-profile-root flag to the Kubelet arguments to make use of this feature. AppArmor is similar to seccomp, only it restricts an container's capabilities including accessing parts of the file system. It can be run in either enforcement or complain mode. Since building Apparmor profiles can be challenging, it is recommended you use a tool like bane instead. Attention Apparmor is only available Ubuntu/Debian distributions of Linux. Attention Kubernetes does not currently provide any native mechanisms for loading AppArmor or seccomp profiles onto Nodes. They either have to be loaded manually or installed onto Nodes when they are bootstrapped. This has to be done prior to referencing them in your Pods because the scheduler is unaware of which nodes have profiles. Recommendations \u00b6 Use a 3rd party solution for runtime defense \u00b6 Creating and managing seccomp and Apparmor profiles can be difficult if you're not familiar with Linux security. If you don't have the time to become proficient, consider using a commercial solution. A lot of them have moved beyond static profiles like Apparmor and seccomp and have begun using machine learning to block or alert on suspicious activity. A handful of these solutions can be found below in the tools section. Additional options can be found on the AWS Marketplace for Containers . Consider add/dropping Linux capabilities before writing seccomp policies \u00b6 Capabilities involve various checks in kernel functions reachable by syscalls. If the check fails, the syscall typically returns an error. The check can be done either right at the beginning of a specific syscall, or deeper in the kernel in areas that might be reachable through multiple different syscalls (such as writing to a specific privileged file). Seccomp, on the other hand, is a syscall filter which is applied to all syscalls before they are run. A process can set up a filter which allows them to revoke their right to run certain syscalls, or specific arguments for certain syscalls. Before using seccomp, consider whether adding/removing Linux capabilities gives you the control you need. See https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container for further information. See whether you can accomplish your aims by using Pod Security Policies (PSPs) \u00b6 Pod Security Policies offer a lot of different ways to improve your security posture without introducing undue complexity. Explore the options available in PSPs before venturing into building seccomp and Apparmor profiles. Warning With the future propects of PSPs in doubt, you may want to look at implementing these controls using Pod security contexts or OPA/Gatekeeper. A collection of Gatekeeper constraints and constraint templates for implementing policies commonly found in PSPs can be pulled from the Gatekeeper repository on GitHub. Additional Resources \u00b6 7 things you should know before you start AppArmor Loader Setting up nodes with profiles zaz A command-line tool to assist on assessing container security requirements and generating seccomp profiles seccomp-operator Is similar to the AppArmor Loader, only instead of AppArmor profiles, it creates a seccomp profiles on each host Tools \u00b6 Aqua Qualys Stackrox Sysdig Secure Twistlock","title":"Runtime Security"},{"location":"runtime/#runtime-security","text":"Runtime security provides active protection for your containers while they're running. The idea is to detect and/or prevent malicious activity from occuring inside the container. With secure computing (seccomp) you can prevent a containerized application from making certain syscalls to the underlying host operating system's kernel. While the Linux operating system has a few hundred system calls, the lion's share of them are not necessary for running containers. By restricting what syscalls can be made by a container, you can effectively decrease your application's attack surface. To get started with seccomp, use strace to generate a stack trace to see which system calls your application is making, then use a tool such as syscall2seccomp to create a seccomp profile from the data gathered from the trace. Unlike SELinux, seccomp was not designed to isolate containers from each other, however, it will protect the host kernel from unauthorized syscalls. It works by intercepting syscalls and only allowing those that have been whitelisted to pass through. Docker has a default seccomp profile which is suitable for a majority of general purpose workloads. You can configure your container or Pod to use this profile by adding the following annotation to your container's or Pod's spec (pre-1.19): annotations : seccomp . security . alpha . kubernetes . io /pod: \"runtime/ default \" 1.19 and later: securityContext : seccompProfile : type : RuntimeDefault It's also possible to create your own profiles for things that require additional privileges. Caution seccomp profiles are a Kubelet alpha feature. You'll need to add the --seccomp-profile-root flag to the Kubelet arguments to make use of this feature. AppArmor is similar to seccomp, only it restricts an container's capabilities including accessing parts of the file system. It can be run in either enforcement or complain mode. Since building Apparmor profiles can be challenging, it is recommended you use a tool like bane instead. Attention Apparmor is only available Ubuntu/Debian distributions of Linux. Attention Kubernetes does not currently provide any native mechanisms for loading AppArmor or seccomp profiles onto Nodes. They either have to be loaded manually or installed onto Nodes when they are bootstrapped. This has to be done prior to referencing them in your Pods because the scheduler is unaware of which nodes have profiles.","title":"Runtime security"},{"location":"runtime/#recommendations","text":"","title":"Recommendations"},{"location":"runtime/#use-a-3rd-party-solution-for-runtime-defense","text":"Creating and managing seccomp and Apparmor profiles can be difficult if you're not familiar with Linux security. If you don't have the time to become proficient, consider using a commercial solution. A lot of them have moved beyond static profiles like Apparmor and seccomp and have begun using machine learning to block or alert on suspicious activity. A handful of these solutions can be found below in the tools section. Additional options can be found on the AWS Marketplace for Containers .","title":"Use a 3rd party solution for runtime defense"},{"location":"runtime/#consider-adddropping-linux-capabilities-before-writing-seccomp-policies","text":"Capabilities involve various checks in kernel functions reachable by syscalls. If the check fails, the syscall typically returns an error. The check can be done either right at the beginning of a specific syscall, or deeper in the kernel in areas that might be reachable through multiple different syscalls (such as writing to a specific privileged file). Seccomp, on the other hand, is a syscall filter which is applied to all syscalls before they are run. A process can set up a filter which allows them to revoke their right to run certain syscalls, or specific arguments for certain syscalls. Before using seccomp, consider whether adding/removing Linux capabilities gives you the control you need. See https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container for further information.","title":"Consider add/dropping Linux capabilities before writing seccomp policies"},{"location":"runtime/#see-whether-you-can-accomplish-your-aims-by-using-pod-security-policies-psps","text":"Pod Security Policies offer a lot of different ways to improve your security posture without introducing undue complexity. Explore the options available in PSPs before venturing into building seccomp and Apparmor profiles. Warning With the future propects of PSPs in doubt, you may want to look at implementing these controls using Pod security contexts or OPA/Gatekeeper. A collection of Gatekeeper constraints and constraint templates for implementing policies commonly found in PSPs can be pulled from the Gatekeeper repository on GitHub.","title":"See whether you can accomplish your aims by using Pod Security Policies (PSPs)"},{"location":"runtime/#additional-resources","text":"7 things you should know before you start AppArmor Loader Setting up nodes with profiles zaz A command-line tool to assist on assessing container security requirements and generating seccomp profiles seccomp-operator Is similar to the AppArmor Loader, only instead of AppArmor profiles, it creates a seccomp profiles on each host","title":"Additional Resources"},{"location":"runtime/#tools","text":"Aqua Qualys Stackrox Sysdig Secure Twistlock","title":"Tools"}]}